2022-03-18 08:10:42 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': '/workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/tensorboard', 'wandb_project': None, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': True}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'broadcast_buffers': False, 'distributed_wrapper': 'DDP', 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4400, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': 128, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-06], 'min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2', 'restore_file': '/workspace/data/users/zanchangtong1/mbart.cc25/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': False, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='mbart_large', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_prev_output_tokens=True, all_gather_list_size=16384, arch='mbart_large', attention_dropout=0.1, batch_size=128, batch_size_valid=128, best_checkpoint_metric='accuracy', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', classification_head_name='sentence_classification_head', clip_norm=0.0, cpu=False, criterion='sentence_prediction', cross_self_attention=False, curriculum=0, data='/workspace/data/users/zanchangtong1/3_XIE/data_bin/prompt_2/', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=128, gen_subset='test', init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, langs='ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN', layernorm_embedding=True, localsgd_frequency=3, log_format=None, log_interval=100, lr=[5e-06], lr_scheduler='polynomial_decay', max_epoch=10, max_source_positions=1024, max_target_positions=1024, max_tokens=4400, max_tokens_valid=4400, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_shuffle=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_classes=10, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, prepend_bos=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, regression_target=False, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='/workspace/data/users/zanchangtong1/mbart.cc25/model.pt', save_dir='/workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2', save_interval=1, save_interval_updates=0, scoring='bleu', seed=222, sentence_avg=False, separator_token=None, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, src_language='zh_CN', stop_time_hours=0, task='mbart_sentence_prediction', tensorboard_logdir='/workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/tensorboard', tgt_language='en_XX', threshold_loss_scale=1.0, tokenizer=None, total_num_update='30968', tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=1858, weight_decay=0.01, zero_sharding='none'), 'task': Namespace(_name='mbart_sentence_prediction', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_prev_output_tokens=True, all_gather_list_size=16384, arch='mbart_large', attention_dropout=0.1, batch_size=128, batch_size_valid=128, best_checkpoint_metric='accuracy', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', classification_head_name='sentence_classification_head', clip_norm=0.0, cpu=False, criterion='sentence_prediction', cross_self_attention=False, curriculum=0, data='/workspace/data/users/zanchangtong1/3_XIE/data_bin/prompt_2/', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=128, gen_subset='test', init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, langs='ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN', layernorm_embedding=True, localsgd_frequency=3, log_format=None, log_interval=100, lr=[5e-06], lr_scheduler='polynomial_decay', max_epoch=10, max_source_positions=1024, max_target_positions=1024, max_tokens=4400, max_tokens_valid=4400, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_shuffle=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_classes=10, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, prepend_bos=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, regression_target=False, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='/workspace/data/users/zanchangtong1/mbart.cc25/model.pt', save_dir='/workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2', save_interval=1, save_interval_updates=0, scoring='bleu', seed=222, sentence_avg=False, separator_token=None, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, src_language='zh_CN', stop_time_hours=0, task='mbart_sentence_prediction', tensorboard_logdir='/workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/tensorboard', tgt_language='en_XX', threshold_loss_scale=1.0, tokenizer=None, total_num_update='30968', tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=1858, weight_decay=0.01, zero_sharding='none'), 'criterion': Namespace(_name='sentence_prediction', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_prev_output_tokens=True, all_gather_list_size=16384, arch='mbart_large', attention_dropout=0.1, batch_size=128, batch_size_valid=128, best_checkpoint_metric='accuracy', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', classification_head_name='sentence_classification_head', clip_norm=0.0, cpu=False, criterion='sentence_prediction', cross_self_attention=False, curriculum=0, data='/workspace/data/users/zanchangtong1/3_XIE/data_bin/prompt_2/', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=128, gen_subset='test', init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, langs='ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN', layernorm_embedding=True, localsgd_frequency=3, log_format=None, log_interval=100, lr=[5e-06], lr_scheduler='polynomial_decay', max_epoch=10, max_source_positions=1024, max_target_positions=1024, max_tokens=4400, max_tokens_valid=4400, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_shuffle=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_classes=10, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, prepend_bos=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, regression_target=False, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='/workspace/data/users/zanchangtong1/mbart.cc25/model.pt', save_dir='/workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2', save_interval=1, save_interval_updates=0, scoring='bleu', seed=222, sentence_avg=False, separator_token=None, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, src_language='zh_CN', stop_time_hours=0, task='mbart_sentence_prediction', tensorboard_logdir='/workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/tensorboard', tgt_language='en_XX', threshold_loss_scale=1.0, tokenizer=None, total_num_update='30968', tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=1858, weight_decay=0.01, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [5e-06]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 1858, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 30968.0, 'lr': [5e-06]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-03-18 08:10:43 | INFO | fairseq.tasks.mbart_sentence_prediction | [input] dictionary: 250002 types
2022-03-18 08:10:43 | INFO | fairseq.tasks.mbart_sentence_prediction | [label] dictionary: 17 types
2022-03-18 08:10:43 | INFO | fairseq.data.data_utils | loaded 15000 examples from: /workspace/data/users/zanchangtong1/3_XIE/data_bin/prompt_2/input0/valid
2022-03-18 08:10:43 | INFO | fairseq.data.data_utils | loaded 15000 examples from: /workspace/data/users/zanchangtong1/3_XIE/data_bin/prompt_2/label/valid
2022-03-18 08:10:43 | INFO | fairseq.tasks.mbart_sentence_prediction | Loaded valid with #samples: 15000
2022-03-18 08:11:24 | INFO | fairseq.models.bart.model | Registering classification head: sentence_classification_head
2022-03-18 08:11:24 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(250027, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(250027, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=250027, bias=False)
  )
  (classification_heads): ModuleDict(
    (sentence_classification_head): BARTClassificationHead(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (out_proj): Linear(in_features=1024, out_features=10, bias=True)
    )
  )
)
2022-03-18 08:11:24 | INFO | fairseq_cli.train | task: mBARTSentencePredictionTask
2022-03-18 08:11:24 | INFO | fairseq_cli.train | model: BARTModel
2022-03-18 08:11:24 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion)
2022-03-18 08:11:24 | INFO | fairseq_cli.train | num. model params: 611911690 (num. trained: 611911690)
2022-03-18 08:11:39 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-03-18 08:11:39 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-18 08:11:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-18 08:11:39 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.587 GB ; name = A100-SXM4-40GB                          
2022-03-18 08:11:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-18 08:11:39 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-18 08:11:39 | INFO | fairseq_cli.train | max tokens per GPU = 4400 and batch size per GPU = 128
2022-03-18 08:11:47 | INFO | fairseq.trainer | loaded checkpoint /workspace/data/users/zanchangtong1/mbart.cc25/model.pt (epoch 142 @ 0 updates)
2022-03-18 08:11:47 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-18 08:11:47 | INFO | fairseq.data.data_utils | loaded 289301 examples from: /workspace/data/users/zanchangtong1/3_XIE/data_bin/prompt_2/input0/train
2022-03-18 08:11:47 | INFO | fairseq.data.data_utils | loaded 289301 examples from: /workspace/data/users/zanchangtong1/3_XIE/data_bin/prompt_2/label/train
2022-03-18 08:11:47 | INFO | fairseq.tasks.mbart_sentence_prediction | Loaded train with #samples: 289301
2022-03-18 08:11:52 | INFO | fairseq.trainer | begin training epoch 1
--- Logging error ---
Traceback (most recent call last):
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 1025, in emit
    msg = self.format(record)
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 869, in format
    return fmt.format(record)
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 608, in format
    record.message = record.getMessage()
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 369, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "/opt/conda/envs/XIE/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq_cli/train.py", line 394, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/distributed_utils.py", line 334, in call_main
    main(cfg, **kwargs)
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq_cli/train.py", line 121, in main
    disable_iterator_cache=task.has_sharded_data("train"),
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/checkpoint_utils.py", line 196, in load_checkpoint
    reset_meters=reset_meters,
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/trainer.py", line 341, in load_checkpoint
    state["model"], strict=False, model_cfg=self.cfg.model
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/models/fairseq_model.py", line 113, in load_state_dict
    self.upgrade_state_dict(state_dict)
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/models/fairseq_model.py", line 119, in upgrade_state_dict
    self.upgrade_state_dict_named(state_dict, "")
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/models/bart/model.py", line 280, in upgrade_state_dict_named
    logger.info("Overwriting", prefix + "classification_heads." + k)
Message: 'Overwriting'
Arguments: ('classification_heads.sentence_classification_head.dense.weight',)
--- Logging error ---
Traceback (most recent call last):
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 1025, in emit
    msg = self.format(record)
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 869, in format
    return fmt.format(record)
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 608, in format
    record.message = record.getMessage()
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 369, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "/opt/conda/envs/XIE/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq_cli/train.py", line 394, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/distributed_utils.py", line 334, in call_main
    main(cfg, **kwargs)
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq_cli/train.py", line 121, in main
    disable_iterator_cache=task.has_sharded_data("train"),
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/checkpoint_utils.py", line 196, in load_checkpoint
    reset_meters=reset_meters,
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/trainer.py", line 341, in load_checkpoint
    state["model"], strict=False, model_cfg=self.cfg.model
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/models/fairseq_model.py", line 113, in load_state_dict
    self.upgrade_state_dict(state_dict)
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/models/fairseq_model.py", line 119, in upgrade_state_dict
    self.upgrade_state_dict_named(state_dict, "")
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/models/bart/model.py", line 280, in upgrade_state_dict_named
    logger.info("Overwriting", prefix + "classification_heads." + k)
Message: 'Overwriting'
Arguments: ('classification_heads.sentence_classification_head.dense.bias',)
--- Logging error ---
Traceback (most recent call last):
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 1025, in emit
    msg = self.format(record)
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 869, in format
    return fmt.format(record)
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 608, in format
    record.message = record.getMessage()
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 369, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "/opt/conda/envs/XIE/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq_cli/train.py", line 394, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/distributed_utils.py", line 334, in call_main
    main(cfg, **kwargs)
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq_cli/train.py", line 121, in main
    disable_iterator_cache=task.has_sharded_data("train"),
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/checkpoint_utils.py", line 196, in load_checkpoint
    reset_meters=reset_meters,
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/trainer.py", line 341, in load_checkpoint
    state["model"], strict=False, model_cfg=self.cfg.model
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/models/fairseq_model.py", line 113, in load_state_dict
    self.upgrade_state_dict(state_dict)
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/models/fairseq_model.py", line 119, in upgrade_state_dict
    self.upgrade_state_dict_named(state_dict, "")
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/models/bart/model.py", line 280, in upgrade_state_dict_named
    logger.info("Overwriting", prefix + "classification_heads." + k)
Message: 'Overwriting'
Arguments: ('classification_heads.sentence_classification_head.out_proj.weight',)
--- Logging error ---
Traceback (most recent call last):
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 1025, in emit
    msg = self.format(record)
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 869, in format
    return fmt.format(record)
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 608, in format
    record.message = record.getMessage()
  File "/opt/conda/envs/XIE/lib/python3.7/logging/__init__.py", line 369, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "/opt/conda/envs/XIE/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq_cli/train.py", line 394, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/distributed_utils.py", line 334, in call_main
    main(cfg, **kwargs)
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq_cli/train.py", line 121, in main
    disable_iterator_cache=task.has_sharded_data("train"),
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/checkpoint_utils.py", line 196, in load_checkpoint
    reset_meters=reset_meters,
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/trainer.py", line 341, in load_checkpoint
    state["model"], strict=False, model_cfg=self.cfg.model
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/models/fairseq_model.py", line 113, in load_state_dict
    self.upgrade_state_dict(state_dict)
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/models/fairseq_model.py", line 119, in upgrade_state_dict
    self.upgrade_state_dict_named(state_dict, "")
  File "/workspace/data/users/zanchangtong1/3_XIE/fairseq/fairseq/models/bart/model.py", line 280, in upgrade_state_dict_named
    logger.info("Overwriting", prefix + "classification_heads." + k)
Message: 'Overwriting'
Arguments: ('classification_heads.sentence_classification_head.out_proj.bias',)
2022-03-18 08:12:25 | INFO | train_inner | epoch 001:    100 / 7971 loss=3.509, nll_loss=0.061, accuracy=8.2, wps=7694.1, ups=3.67, wpb=2098.8, bsz=36.7, num_updates=100, lr=2.69107e-07, gnorm=32.306, loss_scale=4, train_wall=32, wall=46
2022-03-18 08:12:52 | INFO | train_inner | epoch 001:    200 / 7971 loss=3.402, nll_loss=0.059, accuracy=9.7, wps=7802, ups=3.71, wpb=2103.2, bsz=36.5, num_updates=200, lr=5.38213e-07, gnorm=33.573, loss_scale=8, train_wall=26, wall=73
2022-03-18 08:13:19 | INFO | train_inner | epoch 001:    300 / 7971 loss=3.233, nll_loss=0.057, accuracy=18.2, wps=7738.4, ups=3.71, wpb=2086.3, bsz=36.6, num_updates=300, lr=8.0732e-07, gnorm=21.96, loss_scale=16, train_wall=26, wall=100
2022-03-18 08:13:46 | INFO | train_inner | epoch 001:    400 / 7971 loss=3.012, nll_loss=0.053, accuracy=28.6, wps=7763.7, ups=3.71, wpb=2092.1, bsz=36.8, num_updates=400, lr=1.07643e-06, gnorm=17.964, loss_scale=32, train_wall=26, wall=127
2022-03-18 08:14:14 | INFO | train_inner | epoch 001:    500 / 7971 loss=2.622, nll_loss=0.046, accuracy=41.5, wps=7624.9, ups=3.67, wpb=2077.7, bsz=36.7, num_updates=500, lr=1.34553e-06, gnorm=17.098, loss_scale=32, train_wall=26, wall=154
2022-03-18 08:14:41 | INFO | train_inner | epoch 001:    600 / 7971 loss=2.19, nll_loss=0.038, accuracy=53.2, wps=7406.9, ups=3.67, wpb=2015.8, bsz=35, num_updates=600, lr=1.61464e-06, gnorm=15.435, loss_scale=64, train_wall=26, wall=181
2022-03-18 08:14:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:15:08 | INFO | train_inner | epoch 001:    701 / 7971 loss=1.784, nll_loss=0.031, accuracy=63.1, wps=7676.9, ups=3.65, wpb=2104.6, bsz=36.3, num_updates=700, lr=1.88375e-06, gnorm=14.833, loss_scale=64, train_wall=27, wall=209
2022-03-18 08:15:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:15:36 | INFO | train_inner | epoch 001:    802 / 7971 loss=1.386, nll_loss=0.024, accuracy=71.8, wps=7528.8, ups=3.65, wpb=2064.1, bsz=35.9, num_updates=800, lr=2.15285e-06, gnorm=15.209, loss_scale=64, train_wall=26, wall=236
2022-03-18 08:16:03 | INFO | train_inner | epoch 001:    902 / 7971 loss=1.093, nll_loss=0.019, accuracy=78.5, wps=7639.5, ups=3.67, wpb=2080.1, bsz=36.6, num_updates=900, lr=2.42196e-06, gnorm=14.717, loss_scale=64, train_wall=26, wall=263
2022-03-18 08:16:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:16:30 | INFO | train_inner | epoch 001:   1003 / 7971 loss=0.848, nll_loss=0.015, accuracy=83.3, wps=7602.8, ups=3.65, wpb=2083.8, bsz=36.4, num_updates=1000, lr=2.69107e-06, gnorm=14.912, loss_scale=64, train_wall=26, wall=291
2022-03-18 08:16:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:16:58 | INFO | train_inner | epoch 001:   1104 / 7971 loss=0.709, nll_loss=0.012, accuracy=84.7, wps=7652, ups=3.66, wpb=2090.9, bsz=36.5, num_updates=1100, lr=2.96017e-06, gnorm=13.523, loss_scale=64, train_wall=26, wall=318
2022-03-18 08:17:25 | INFO | train_inner | epoch 001:   1204 / 7971 loss=0.575, nll_loss=0.01, accuracy=88.2, wps=7712.1, ups=3.7, wpb=2085.6, bsz=36.2, num_updates=1200, lr=3.22928e-06, gnorm=13.618, loss_scale=64, train_wall=26, wall=345
2022-03-18 08:17:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:17:52 | INFO | train_inner | epoch 001:   1305 / 7971 loss=0.578, nll_loss=0.01, accuracy=87.9, wps=7642.4, ups=3.66, wpb=2090.1, bsz=36.5, num_updates=1300, lr=3.49839e-06, gnorm=12.595, loss_scale=64, train_wall=26, wall=372
2022-03-18 08:18:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:18:19 | INFO | train_inner | epoch 001:   1406 / 7971 loss=0.484, nll_loss=0.008, accuracy=89.9, wps=7355.9, ups=3.65, wpb=2014.6, bsz=35.4, num_updates=1400, lr=3.76749e-06, gnorm=11.847, loss_scale=64, train_wall=26, wall=400
2022-03-18 08:18:47 | INFO | train_inner | epoch 001:   1506 / 7971 loss=0.463, nll_loss=0.008, accuracy=90.3, wps=7803.1, ups=3.67, wpb=2123.4, bsz=36.7, num_updates=1500, lr=4.0366e-06, gnorm=11.413, loss_scale=64, train_wall=26, wall=427
2022-03-18 08:18:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:19:14 | INFO | train_inner | epoch 001:   1607 / 7971 loss=0.413, nll_loss=0.007, accuracy=91.3, wps=7646.1, ups=3.65, wpb=2093.2, bsz=37, num_updates=1600, lr=4.30571e-06, gnorm=11.323, loss_scale=64, train_wall=26, wall=454
2022-03-18 08:19:41 | INFO | train_inner | epoch 001:   1707 / 7971 loss=0.431, nll_loss=0.007, accuracy=90.5, wps=7638.1, ups=3.68, wpb=2075, bsz=36.1, num_updates=1700, lr=4.57481e-06, gnorm=11.038, loss_scale=128, train_wall=26, wall=482
2022-03-18 08:20:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:20:08 | INFO | train_inner | epoch 001:   1808 / 7971 loss=0.397, nll_loss=0.007, accuracy=91.4, wps=7614.7, ups=3.67, wpb=2073.2, bsz=36.1, num_updates=1800, lr=4.84392e-06, gnorm=10.594, loss_scale=128, train_wall=26, wall=509
2022-03-18 08:20:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:20:36 | INFO | train_inner | epoch 001:   1909 / 7971 loss=0.397, nll_loss=0.007, accuracy=91.3, wps=7763.9, ups=3.68, wpb=2108.3, bsz=36.9, num_updates=1900, lr=4.99279e-06, gnorm=11.556, loss_scale=64, train_wall=26, wall=536
2022-03-18 08:20:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:21:03 | INFO | train_inner | epoch 001:   2010 / 7971 loss=0.36, nll_loss=0.006, accuracy=92, wps=7599.7, ups=3.68, wpb=2063.5, bsz=36.1, num_updates=2000, lr=4.97561e-06, gnorm=10.432, loss_scale=64, train_wall=26, wall=563
2022-03-18 08:21:30 | INFO | train_inner | epoch 001:   2110 / 7971 loss=0.352, nll_loss=0.006, accuracy=92, wps=7510.4, ups=3.7, wpb=2028.5, bsz=35.6, num_updates=2100, lr=4.95843e-06, gnorm=10.219, loss_scale=64, train_wall=26, wall=590
2022-03-18 08:21:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:21:57 | INFO | train_inner | epoch 001:   2211 / 7971 loss=0.341, nll_loss=0.006, accuracy=92.7, wps=7594.5, ups=3.68, wpb=2065, bsz=36.3, num_updates=2200, lr=4.94126e-06, gnorm=9.606, loss_scale=64, train_wall=26, wall=617
2022-03-18 08:22:24 | INFO | train_inner | epoch 001:   2311 / 7971 loss=0.326, nll_loss=0.006, accuracy=92.9, wps=7771.6, ups=3.71, wpb=2096, bsz=36.5, num_updates=2300, lr=4.92408e-06, gnorm=8.975, loss_scale=128, train_wall=26, wall=644
2022-03-18 08:22:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:22:51 | INFO | train_inner | epoch 001:   2412 / 7971 loss=0.308, nll_loss=0.005, accuracy=92.9, wps=7801.1, ups=3.66, wpb=2133, bsz=37, num_updates=2400, lr=4.9069e-06, gnorm=8.746, loss_scale=128, train_wall=26, wall=672
2022-03-18 08:22:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:23:19 | INFO | train_inner | epoch 001:   2513 / 7971 loss=0.326, nll_loss=0.006, accuracy=93, wps=7543.6, ups=3.66, wpb=2063.3, bsz=35.9, num_updates=2500, lr=4.88973e-06, gnorm=9.999, loss_scale=64, train_wall=26, wall=699
2022-03-18 08:23:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:23:46 | INFO | train_inner | epoch 001:   2614 / 7971 loss=0.29, nll_loss=0.005, accuracy=93.7, wps=7552.4, ups=3.66, wpb=2066.1, bsz=36.2, num_updates=2600, lr=4.87255e-06, gnorm=8.901, loss_scale=64, train_wall=26, wall=726
2022-03-18 08:24:13 | INFO | train_inner | epoch 001:   2714 / 7971 loss=0.309, nll_loss=0.005, accuracy=93.2, wps=7606.9, ups=3.66, wpb=2080.5, bsz=36.7, num_updates=2700, lr=4.85538e-06, gnorm=9.431, loss_scale=128, train_wall=26, wall=754
2022-03-18 08:24:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:24:41 | INFO | train_inner | epoch 001:   2815 / 7971 loss=0.285, nll_loss=0.005, accuracy=93.8, wps=7548.6, ups=3.65, wpb=2066.3, bsz=36.1, num_updates=2800, lr=4.8382e-06, gnorm=9.441, loss_scale=64, train_wall=26, wall=781
2022-03-18 08:25:08 | INFO | train_inner | epoch 001:   2915 / 7971 loss=0.312, nll_loss=0.005, accuracy=93.3, wps=7637.9, ups=3.68, wpb=2074.4, bsz=36.2, num_updates=2900, lr=4.82102e-06, gnorm=8.844, loss_scale=128, train_wall=26, wall=808
2022-03-18 08:25:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:25:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:25:35 | INFO | train_inner | epoch 001:   3017 / 7971 loss=0.29, nll_loss=0.005, accuracy=93.7, wps=7480.7, ups=3.62, wpb=2065.6, bsz=35.9, num_updates=3000, lr=4.80385e-06, gnorm=8.202, loss_scale=64, train_wall=27, wall=836
2022-03-18 08:26:03 | INFO | train_inner | epoch 001:   3117 / 7971 loss=0.277, nll_loss=0.005, accuracy=94.1, wps=7734.1, ups=3.68, wpb=2099.7, bsz=36.6, num_updates=3100, lr=4.78667e-06, gnorm=9.338, loss_scale=64, train_wall=26, wall=863
2022-03-18 08:26:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:26:30 | INFO | train_inner | epoch 001:   3218 / 7971 loss=0.297, nll_loss=0.005, accuracy=93.4, wps=7479.7, ups=3.68, wpb=2033, bsz=35.6, num_updates=3200, lr=4.7695e-06, gnorm=9.157, loss_scale=64, train_wall=26, wall=890
2022-03-18 08:26:57 | INFO | train_inner | epoch 001:   3318 / 7971 loss=0.272, nll_loss=0.005, accuracy=93.9, wps=7635.1, ups=3.71, wpb=2057.2, bsz=36, num_updates=3300, lr=4.75232e-06, gnorm=8.448, loss_scale=128, train_wall=26, wall=917
2022-03-18 08:27:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:27:24 | INFO | train_inner | epoch 001:   3419 / 7971 loss=0.299, nll_loss=0.005, accuracy=93.5, wps=7591.6, ups=3.66, wpb=2074.4, bsz=36.3, num_updates=3400, lr=4.73514e-06, gnorm=8.48, loss_scale=64, train_wall=26, wall=945
2022-03-18 08:27:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:27:51 | INFO | train_inner | epoch 001:   3520 / 7971 loss=0.272, nll_loss=0.005, accuracy=94, wps=7582, ups=3.66, wpb=2072.1, bsz=36.5, num_updates=3500, lr=4.71797e-06, gnorm=7.889, loss_scale=64, train_wall=26, wall=972
2022-03-18 08:28:18 | INFO | train_inner | epoch 001:   3620 / 7971 loss=0.261, nll_loss=0.005, accuracy=94.6, wps=7783.8, ups=3.69, wpb=2108.1, bsz=36.8, num_updates=3600, lr=4.70079e-06, gnorm=7.323, loss_scale=64, train_wall=26, wall=999
2022-03-18 08:28:46 | INFO | train_inner | epoch 001:   3720 / 7971 loss=0.283, nll_loss=0.005, accuracy=93.7, wps=7680.8, ups=3.68, wpb=2085.9, bsz=36.7, num_updates=3700, lr=4.68361e-06, gnorm=8.369, loss_scale=128, train_wall=26, wall=1026
2022-03-18 08:29:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:29:13 | INFO | train_inner | epoch 001:   3821 / 7971 loss=0.264, nll_loss=0.005, accuracy=94.4, wps=7433.3, ups=3.65, wpb=2036.7, bsz=35.4, num_updates=3800, lr=4.66644e-06, gnorm=8.231, loss_scale=128, train_wall=26, wall=1054
2022-03-18 08:29:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:29:41 | INFO | train_inner | epoch 001:   3922 / 7971 loss=0.283, nll_loss=0.005, accuracy=93.9, wps=7456.1, ups=3.58, wpb=2084.2, bsz=36.6, num_updates=3900, lr=4.64926e-06, gnorm=8.092, loss_scale=128, train_wall=27, wall=1082
2022-03-18 08:30:08 | INFO | train_inner | epoch 001:   4022 / 7971 loss=0.243, nll_loss=0.004, accuracy=94.8, wps=7618.8, ups=3.68, wpb=2071, bsz=36.2, num_updates=4000, lr=4.63209e-06, gnorm=7.158, loss_scale=128, train_wall=26, wall=1109
2022-03-18 08:30:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:30:35 | INFO | train_inner | epoch 001:   4123 / 7971 loss=0.233, nll_loss=0.004, accuracy=95, wps=7546, ups=3.67, wpb=2056.3, bsz=36.1, num_updates=4100, lr=4.61491e-06, gnorm=5.88, loss_scale=128, train_wall=26, wall=1136
2022-03-18 08:31:03 | INFO | train_inner | epoch 001:   4223 / 7971 loss=0.231, nll_loss=0.004, accuracy=94.7, wps=7661.8, ups=3.69, wpb=2078.6, bsz=36.3, num_updates=4200, lr=4.59773e-06, gnorm=6.662, loss_scale=256, train_wall=26, wall=1163
2022-03-18 08:31:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:31:30 | INFO | train_inner | epoch 001:   4324 / 7971 loss=0.219, nll_loss=0.004, accuracy=95, wps=7414.5, ups=3.67, wpb=2020.5, bsz=35, num_updates=4300, lr=4.58056e-06, gnorm=6.872, loss_scale=128, train_wall=26, wall=1190
2022-03-18 08:31:57 | INFO | train_inner | epoch 001:   4424 / 7971 loss=0.273, nll_loss=0.005, accuracy=94, wps=7714.1, ups=3.69, wpb=2090.1, bsz=36.5, num_updates=4400, lr=4.56338e-06, gnorm=7.255, loss_scale=128, train_wall=26, wall=1217
2022-03-18 08:32:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:32:24 | INFO | train_inner | epoch 001:   4525 / 7971 loss=0.26, nll_loss=0.005, accuracy=94.3, wps=7718.5, ups=3.65, wpb=2115, bsz=37, num_updates=4500, lr=4.5462e-06, gnorm=6.914, loss_scale=128, train_wall=26, wall=1245
2022-03-18 08:32:51 | INFO | train_inner | epoch 001:   4625 / 7971 loss=0.252, nll_loss=0.004, accuracy=94.5, wps=7670, ups=3.69, wpb=2077.6, bsz=36.1, num_updates=4600, lr=4.52903e-06, gnorm=6.335, loss_scale=256, train_wall=26, wall=1272
2022-03-18 08:33:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:33:19 | INFO | train_inner | epoch 001:   4726 / 7971 loss=0.246, nll_loss=0.004, accuracy=94.5, wps=7668.6, ups=3.66, wpb=2097.8, bsz=36.8, num_updates=4700, lr=4.51185e-06, gnorm=6.123, loss_scale=128, train_wall=26, wall=1299
2022-03-18 08:33:46 | INFO | train_inner | epoch 001:   4826 / 7971 loss=0.256, nll_loss=0.004, accuracy=94.1, wps=7646.4, ups=3.7, wpb=2069.2, bsz=36.3, num_updates=4800, lr=4.49468e-06, gnorm=6.57, loss_scale=256, train_wall=26, wall=1326
2022-03-18 08:33:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:34:13 | INFO | train_inner | epoch 001:   4927 / 7971 loss=0.244, nll_loss=0.004, accuracy=94.8, wps=7445.2, ups=3.62, wpb=2054.9, bsz=35.9, num_updates=4900, lr=4.4775e-06, gnorm=6.27, loss_scale=128, train_wall=27, wall=1354
2022-03-18 08:34:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:34:41 | INFO | train_inner | epoch 001:   5028 / 7971 loss=0.274, nll_loss=0.005, accuracy=93.9, wps=7651, ups=3.64, wpb=2100.6, bsz=36.4, num_updates=5000, lr=4.46032e-06, gnorm=6.552, loss_scale=128, train_wall=27, wall=1381
2022-03-18 08:35:08 | INFO | train_inner | epoch 001:   5128 / 7971 loss=0.233, nll_loss=0.004, accuracy=94.9, wps=7578.1, ups=3.68, wpb=2061.7, bsz=36.1, num_updates=5100, lr=4.44315e-06, gnorm=5.986, loss_scale=128, train_wall=26, wall=1409
2022-03-18 08:35:35 | INFO | train_inner | epoch 001:   5228 / 7971 loss=0.235, nll_loss=0.004, accuracy=95.3, wps=7764.5, ups=3.69, wpb=2106.6, bsz=36.9, num_updates=5200, lr=4.42597e-06, gnorm=5.884, loss_scale=256, train_wall=26, wall=1436
2022-03-18 08:35:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 08:36:03 | INFO | train_inner | epoch 001:   5329 / 7971 loss=0.251, nll_loss=0.004, accuracy=94.2, wps=7558.7, ups=3.65, wpb=2068.3, bsz=36.2, num_updates=5300, lr=4.40879e-06, gnorm=6.091, loss_scale=256, train_wall=26, wall=1463
2022-03-18 08:36:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:36:30 | INFO | train_inner | epoch 001:   5430 / 7971 loss=0.246, nll_loss=0.004, accuracy=94.5, wps=7810.7, ups=3.66, wpb=2135.7, bsz=37.2, num_updates=5400, lr=4.39162e-06, gnorm=5.942, loss_scale=128, train_wall=26, wall=1491
2022-03-18 08:36:57 | INFO | train_inner | epoch 001:   5530 / 7971 loss=0.243, nll_loss=0.004, accuracy=94.9, wps=7622.8, ups=3.68, wpb=2073.2, bsz=36.4, num_updates=5500, lr=4.37444e-06, gnorm=5.878, loss_scale=128, train_wall=26, wall=1518
2022-03-18 08:37:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:37:25 | INFO | train_inner | epoch 001:   5631 / 7971 loss=0.254, nll_loss=0.004, accuracy=94.4, wps=7591.3, ups=3.65, wpb=2080.3, bsz=36.5, num_updates=5600, lr=4.35727e-06, gnorm=6.227, loss_scale=128, train_wall=26, wall=1545
2022-03-18 08:37:52 | INFO | train_inner | epoch 001:   5731 / 7971 loss=0.217, nll_loss=0.004, accuracy=94.9, wps=7734.7, ups=3.7, wpb=2091, bsz=36.6, num_updates=5700, lr=4.34009e-06, gnorm=5.473, loss_scale=256, train_wall=26, wall=1572
2022-03-18 08:38:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 08:38:19 | INFO | train_inner | epoch 001:   5832 / 7971 loss=0.216, nll_loss=0.004, accuracy=95.3, wps=7724.7, ups=3.69, wpb=2093.2, bsz=37, num_updates=5800, lr=4.32291e-06, gnorm=5.434, loss_scale=256, train_wall=26, wall=1599
2022-03-18 08:38:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:38:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:38:46 | INFO | train_inner | epoch 001:   5934 / 7971 loss=0.232, nll_loss=0.004, accuracy=95.2, wps=7516.8, ups=3.65, wpb=2056.6, bsz=35.6, num_updates=5900, lr=4.30574e-06, gnorm=5.776, loss_scale=64, train_wall=26, wall=1627
2022-03-18 08:39:13 | INFO | train_inner | epoch 001:   6034 / 7971 loss=0.221, nll_loss=0.004, accuracy=95.3, wps=7540.3, ups=3.72, wpb=2026.4, bsz=35.6, num_updates=6000, lr=4.28856e-06, gnorm=6.249, loss_scale=128, train_wall=26, wall=1654
2022-03-18 08:39:40 | INFO | train_inner | epoch 001:   6134 / 7971 loss=0.22, nll_loss=0.004, accuracy=95.4, wps=7721.4, ups=3.7, wpb=2086.7, bsz=36.3, num_updates=6100, lr=4.27138e-06, gnorm=5.404, loss_scale=128, train_wall=26, wall=1681
2022-03-18 08:39:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:40:07 | INFO | train_inner | epoch 001:   6235 / 7971 loss=0.234, nll_loss=0.004, accuracy=95.1, wps=7682.2, ups=3.7, wpb=2076.3, bsz=36.5, num_updates=6200, lr=4.25421e-06, gnorm=5.21, loss_scale=128, train_wall=26, wall=1708
2022-03-18 08:40:34 | INFO | train_inner | epoch 001:   6335 / 7971 loss=0.217, nll_loss=0.004, accuracy=94.8, wps=7737.3, ups=3.72, wpb=2079.7, bsz=36.4, num_updates=6300, lr=4.23703e-06, gnorm=5.461, loss_scale=256, train_wall=26, wall=1734
2022-03-18 08:40:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:41:01 | INFO | train_inner | epoch 001:   6436 / 7971 loss=0.246, nll_loss=0.004, accuracy=94.5, wps=7699.4, ups=3.66, wpb=2101.8, bsz=37.5, num_updates=6400, lr=4.21986e-06, gnorm=5.438, loss_scale=128, train_wall=26, wall=1762
2022-03-18 08:41:28 | INFO | train_inner | epoch 001:   6536 / 7971 loss=0.248, nll_loss=0.004, accuracy=94.4, wps=7681.6, ups=3.69, wpb=2082.7, bsz=36.3, num_updates=6500, lr=4.20268e-06, gnorm=5.281, loss_scale=256, train_wall=26, wall=1789
2022-03-18 08:41:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 08:41:55 | INFO | train_inner | epoch 001:   6637 / 7971 loss=0.239, nll_loss=0.004, accuracy=94.9, wps=7645.1, ups=3.69, wpb=2072.3, bsz=36.2, num_updates=6600, lr=4.1855e-06, gnorm=5.495, loss_scale=256, train_wall=26, wall=1816
2022-03-18 08:42:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:42:23 | INFO | train_inner | epoch 001:   6738 / 7971 loss=0.224, nll_loss=0.004, accuracy=95, wps=7556.5, ups=3.68, wpb=2054.5, bsz=35.6, num_updates=6700, lr=4.16833e-06, gnorm=5.703, loss_scale=128, train_wall=26, wall=1843
2022-03-18 08:42:50 | INFO | train_inner | epoch 001:   6838 / 7971 loss=0.197, nll_loss=0.003, accuracy=95.7, wps=7725.6, ups=3.7, wpb=2086.5, bsz=36.5, num_updates=6800, lr=4.15115e-06, gnorm=5.462, loss_scale=256, train_wall=26, wall=1870
2022-03-18 08:43:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:43:17 | INFO | train_inner | epoch 001:   6939 / 7971 loss=0.201, nll_loss=0.004, accuracy=95.6, wps=7557.1, ups=3.67, wpb=2061.7, bsz=36, num_updates=6900, lr=4.13397e-06, gnorm=5.468, loss_scale=128, train_wall=26, wall=1897
2022-03-18 08:43:44 | INFO | train_inner | epoch 001:   7039 / 7971 loss=0.217, nll_loss=0.004, accuracy=95.5, wps=7653.9, ups=3.68, wpb=2079.1, bsz=36.1, num_updates=7000, lr=4.1168e-06, gnorm=5.696, loss_scale=128, train_wall=26, wall=1925
2022-03-18 08:44:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:44:11 | INFO | train_inner | epoch 001:   7140 / 7971 loss=0.223, nll_loss=0.004, accuracy=95.2, wps=7673.1, ups=3.69, wpb=2080.8, bsz=36.5, num_updates=7100, lr=4.09962e-06, gnorm=5.212, loss_scale=128, train_wall=26, wall=1952
2022-03-18 08:44:38 | INFO | train_inner | epoch 001:   7240 / 7971 loss=0.239, nll_loss=0.004, accuracy=94.9, wps=7541.4, ups=3.67, wpb=2052.5, bsz=36, num_updates=7200, lr=4.08245e-06, gnorm=5.551, loss_scale=128, train_wall=26, wall=1979
2022-03-18 08:45:05 | INFO | train_inner | epoch 001:   7340 / 7971 loss=0.223, nll_loss=0.004, accuracy=95.1, wps=7602.9, ups=3.73, wpb=2040.1, bsz=35.6, num_updates=7300, lr=4.06527e-06, gnorm=5.469, loss_scale=256, train_wall=26, wall=2006
2022-03-18 08:45:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:45:32 | INFO | train_inner | epoch 001:   7441 / 7971 loss=0.196, nll_loss=0.003, accuracy=95.5, wps=7714.7, ups=3.68, wpb=2098.9, bsz=36.8, num_updates=7400, lr=4.04809e-06, gnorm=5.395, loss_scale=128, train_wall=26, wall=2033
2022-03-18 08:46:00 | INFO | train_inner | epoch 001:   7541 / 7971 loss=0.242, nll_loss=0.004, accuracy=94.7, wps=7628.7, ups=3.69, wpb=2064.9, bsz=36.2, num_updates=7500, lr=4.03092e-06, gnorm=5.427, loss_scale=256, train_wall=26, wall=2060
2022-03-18 08:46:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 08:46:27 | INFO | train_inner | epoch 001:   7642 / 7971 loss=0.207, nll_loss=0.004, accuracy=95.5, wps=7501, ups=3.69, wpb=2030.5, bsz=35.8, num_updates=7600, lr=4.01374e-06, gnorm=5.2, loss_scale=256, train_wall=26, wall=2087
2022-03-18 08:46:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:46:54 | INFO | train_inner | epoch 001:   7743 / 7971 loss=0.199, nll_loss=0.003, accuracy=95.8, wps=7673.2, ups=3.7, wpb=2074.4, bsz=36.3, num_updates=7700, lr=3.99656e-06, gnorm=5.146, loss_scale=128, train_wall=26, wall=2114
2022-03-18 08:47:21 | INFO | train_inner | epoch 001:   7843 / 7971 loss=0.21, nll_loss=0.004, accuracy=95.8, wps=7592.1, ups=3.71, wpb=2044.8, bsz=35.8, num_updates=7800, lr=3.97939e-06, gnorm=5.255, loss_scale=256, train_wall=26, wall=2141
2022-03-18 08:47:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:47:48 | INFO | train_inner | epoch 001:   7944 / 7971 loss=0.198, nll_loss=0.003, accuracy=95.8, wps=7408.9, ups=3.68, wpb=2014.1, bsz=35.1, num_updates=7900, lr=3.96221e-06, gnorm=4.988, loss_scale=128, train_wall=26, wall=2168
2022-03-18 08:47:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-18 08:48:25 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.2 | nll_loss 0.003 | accuracy 95.8 | wps 29231.4 | wpb 2056.3 | bsz 36 | num_updates 7927
2022-03-18 08:48:25 | INFO | fairseq_cli.train | begin save checkpoint
2022-03-18 08:48:25 | INFO | fairseq.trainer | Preparing to save checkpoint to /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint1.pt after 7927 updates
2022-03-18 08:48:52 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint1.pt
2022-03-18 08:49:27 | INFO | fairseq.checkpoint_utils | saved checkpoint /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint1.pt (epoch 1 @ 7927 updates, score 95.8) (writing took 61.94154789671302 seconds)
2022-03-18 08:49:27 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-18 08:49:27 | INFO | train | epoch 001 | loss 0.542 | nll_loss 0.009 | accuracy 87.7 | wps 7314.9 | ups 3.53 | wpb 2075 | bsz 36.3 | num_updates 7927 | lr 3.95757e-06 | gnorm 9.07 | loss_scale 128 | train_wall 2090 | wall 2267
2022-03-18 08:49:32 | INFO | fairseq.trainer | begin training epoch 2
2022-03-18 08:49:52 | INFO | train_inner | epoch 002:     73 / 7971 loss=0.191, nll_loss=0.003, accuracy=95.8, wps=1654.1, ups=0.8, wpb=2054.8, bsz=35.9, num_updates=8000, lr=3.94504e-06, gnorm=4.899, loss_scale=256, train_wall=26, wall=2293
2022-03-18 08:50:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 08:50:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:50:20 | INFO | train_inner | epoch 002:    175 / 7971 loss=0.143, nll_loss=0.003, accuracy=96.6, wps=7527.5, ups=3.61, wpb=2083.7, bsz=36.5, num_updates=8100, lr=3.92786e-06, gnorm=4.443, loss_scale=128, train_wall=27, wall=2320
2022-03-18 08:50:47 | INFO | train_inner | epoch 002:    275 / 7971 loss=0.182, nll_loss=0.003, accuracy=95.7, wps=7779.2, ups=3.68, wpb=2115, bsz=36.7, num_updates=8200, lr=3.91068e-06, gnorm=4.825, loss_scale=128, train_wall=26, wall=2347
2022-03-18 08:50:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:51:14 | INFO | train_inner | epoch 002:    376 / 7971 loss=0.162, nll_loss=0.003, accuracy=96.7, wps=7721.6, ups=3.68, wpb=2097.3, bsz=36.4, num_updates=8300, lr=3.89351e-06, gnorm=4.687, loss_scale=128, train_wall=26, wall=2375
2022-03-18 08:51:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:51:41 | INFO | train_inner | epoch 002:    477 / 7971 loss=0.172, nll_loss=0.003, accuracy=96.3, wps=7531.1, ups=3.69, wpb=2043.3, bsz=35.7, num_updates=8400, lr=3.87633e-06, gnorm=4.743, loss_scale=128, train_wall=26, wall=2402
2022-03-18 08:52:08 | INFO | train_inner | epoch 002:    577 / 7971 loss=0.169, nll_loss=0.003, accuracy=96.3, wps=7619.7, ups=3.71, wpb=2052.4, bsz=36.1, num_updates=8500, lr=3.85915e-06, gnorm=4.423, loss_scale=128, train_wall=26, wall=2429
2022-03-18 08:52:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:52:35 | INFO | train_inner | epoch 002:    678 / 7971 loss=0.19, nll_loss=0.003, accuracy=96, wps=7644.6, ups=3.69, wpb=2072.6, bsz=36.4, num_updates=8600, lr=3.84198e-06, gnorm=4.911, loss_scale=128, train_wall=26, wall=2456
2022-03-18 08:53:02 | INFO | train_inner | epoch 002:    778 / 7971 loss=0.161, nll_loss=0.003, accuracy=96.6, wps=7651.5, ups=3.7, wpb=2066.6, bsz=36.2, num_updates=8700, lr=3.8248e-06, gnorm=4.599, loss_scale=256, train_wall=26, wall=2483
2022-03-18 08:53:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:53:29 | INFO | train_inner | epoch 002:    879 / 7971 loss=0.187, nll_loss=0.003, accuracy=96, wps=7649.2, ups=3.68, wpb=2078.5, bsz=36.7, num_updates=8800, lr=3.80763e-06, gnorm=4.841, loss_scale=128, train_wall=26, wall=2510
2022-03-18 08:53:56 | INFO | train_inner | epoch 002:    979 / 7971 loss=0.18, nll_loss=0.003, accuracy=95.9, wps=7709.8, ups=3.72, wpb=2075.2, bsz=36.3, num_updates=8900, lr=3.79045e-06, gnorm=5.077, loss_scale=256, train_wall=26, wall=2537
2022-03-18 08:54:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:54:24 | INFO | train_inner | epoch 002:   1080 / 7971 loss=0.172, nll_loss=0.003, accuracy=96.1, wps=7617.3, ups=3.68, wpb=2072.4, bsz=36.3, num_updates=9000, lr=3.77327e-06, gnorm=4.866, loss_scale=128, train_wall=26, wall=2564
2022-03-18 08:54:51 | INFO | train_inner | epoch 002:   1180 / 7971 loss=0.187, nll_loss=0.003, accuracy=96.1, wps=7747.2, ups=3.69, wpb=2098.3, bsz=36.5, num_updates=9100, lr=3.7561e-06, gnorm=4.843, loss_scale=256, train_wall=26, wall=2591
2022-03-18 08:55:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 08:55:18 | INFO | train_inner | epoch 002:   1281 / 7971 loss=0.2, nll_loss=0.004, accuracy=95.6, wps=7592.4, ups=3.65, wpb=2079, bsz=36.6, num_updates=9200, lr=3.73892e-06, gnorm=4.888, loss_scale=256, train_wall=26, wall=2619
2022-03-18 08:55:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:55:45 | INFO | train_inner | epoch 002:   1382 / 7971 loss=0.225, nll_loss=0.004, accuracy=94.8, wps=7520.4, ups=3.67, wpb=2046.4, bsz=36.1, num_updates=9300, lr=3.72175e-06, gnorm=5.184, loss_scale=128, train_wall=26, wall=2646
2022-03-18 08:56:12 | INFO | train_inner | epoch 002:   1482 / 7971 loss=0.154, nll_loss=0.003, accuracy=96.5, wps=7781.3, ups=3.71, wpb=2094.8, bsz=36.7, num_updates=9400, lr=3.70457e-06, gnorm=4.223, loss_scale=256, train_wall=26, wall=2673
2022-03-18 08:56:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:56:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 08:56:40 | INFO | train_inner | epoch 002:   1584 / 7971 loss=0.17, nll_loss=0.003, accuracy=96.1, wps=7520.6, ups=3.65, wpb=2058, bsz=36.1, num_updates=9500, lr=3.68739e-06, gnorm=5.03, loss_scale=64, train_wall=26, wall=2700
2022-03-18 08:57:06 | INFO | train_inner | epoch 002:   1684 / 7971 loss=0.205, nll_loss=0.004, accuracy=95.6, wps=7824.1, ups=3.72, wpb=2102.5, bsz=36.7, num_updates=9600, lr=3.67022e-06, gnorm=5.053, loss_scale=64, train_wall=26, wall=2727
2022-03-18 08:57:33 | INFO | train_inner | epoch 002:   1784 / 7971 loss=0.167, nll_loss=0.003, accuracy=96.5, wps=7920.9, ups=3.71, wpb=2134.1, bsz=37.2, num_updates=9700, lr=3.65304e-06, gnorm=4.454, loss_scale=128, train_wall=26, wall=2754
2022-03-18 08:57:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:58:01 | INFO | train_inner | epoch 002:   1885 / 7971 loss=0.155, nll_loss=0.003, accuracy=96.6, wps=7618.6, ups=3.67, wpb=2076, bsz=36.1, num_updates=9800, lr=3.63586e-06, gnorm=4.488, loss_scale=128, train_wall=26, wall=2781
2022-03-18 08:58:28 | INFO | train_inner | epoch 002:   1985 / 7971 loss=0.164, nll_loss=0.003, accuracy=96.5, wps=7480.4, ups=3.71, wpb=2018.3, bsz=35.4, num_updates=9900, lr=3.61869e-06, gnorm=4.998, loss_scale=256, train_wall=26, wall=2808
2022-03-18 08:58:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:58:55 | INFO | train_inner | epoch 002:   2086 / 7971 loss=0.162, nll_loss=0.003, accuracy=96.4, wps=7613.4, ups=3.68, wpb=2070.3, bsz=36, num_updates=10000, lr=3.60151e-06, gnorm=4.47, loss_scale=128, train_wall=26, wall=2835
2022-03-18 08:59:22 | INFO | train_inner | epoch 002:   2186 / 7971 loss=0.161, nll_loss=0.003, accuracy=96.3, wps=7610.8, ups=3.71, wpb=2052.2, bsz=35.8, num_updates=10100, lr=3.58434e-06, gnorm=4.232, loss_scale=128, train_wall=26, wall=2862
2022-03-18 08:59:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 08:59:49 | INFO | train_inner | epoch 002:   2287 / 7971 loss=0.17, nll_loss=0.003, accuracy=96.6, wps=7730.9, ups=3.69, wpb=2097.3, bsz=37.3, num_updates=10200, lr=3.56716e-06, gnorm=4.566, loss_scale=128, train_wall=26, wall=2889
2022-03-18 09:00:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:00:17 | INFO | train_inner | epoch 002:   2388 / 7971 loss=0.17, nll_loss=0.003, accuracy=96.2, wps=7471.4, ups=3.62, wpb=2062.1, bsz=36.3, num_updates=10300, lr=3.54998e-06, gnorm=4.594, loss_scale=128, train_wall=27, wall=2917
2022-03-18 09:00:44 | INFO | train_inner | epoch 002:   2488 / 7971 loss=0.171, nll_loss=0.003, accuracy=96.3, wps=7647.9, ups=3.68, wpb=2076.8, bsz=35.7, num_updates=10400, lr=3.53281e-06, gnorm=4.75, loss_scale=128, train_wall=26, wall=2944
2022-03-18 09:01:11 | INFO | train_inner | epoch 002:   2588 / 7971 loss=0.168, nll_loss=0.003, accuracy=96.2, wps=7816.9, ups=3.71, wpb=2108.4, bsz=36.7, num_updates=10500, lr=3.51563e-06, gnorm=4.989, loss_scale=256, train_wall=26, wall=2971
2022-03-18 09:01:38 | INFO | train_inner | epoch 002:   2688 / 7971 loss=0.169, nll_loss=0.003, accuracy=96.2, wps=7615.9, ups=3.71, wpb=2055.2, bsz=35.7, num_updates=10600, lr=3.49845e-06, gnorm=4.68, loss_scale=512, train_wall=26, wall=2998
2022-03-18 09:01:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:02:05 | INFO | train_inner | epoch 002:   2789 / 7971 loss=0.174, nll_loss=0.003, accuracy=96.1, wps=7691.1, ups=3.67, wpb=2094.4, bsz=36.2, num_updates=10700, lr=3.48128e-06, gnorm=4.498, loss_scale=256, train_wall=26, wall=3025
2022-03-18 09:02:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:02:32 | INFO | train_inner | epoch 002:   2890 / 7971 loss=0.176, nll_loss=0.003, accuracy=96.2, wps=7572.1, ups=3.66, wpb=2066.6, bsz=36.4, num_updates=10800, lr=3.4641e-06, gnorm=4.576, loss_scale=128, train_wall=26, wall=3053
2022-03-18 09:02:59 | INFO | train_inner | epoch 002:   2990 / 7971 loss=0.157, nll_loss=0.003, accuracy=96.3, wps=7860, ups=3.67, wpb=2139.1, bsz=37.7, num_updates=10900, lr=3.44693e-06, gnorm=4.362, loss_scale=256, train_wall=26, wall=3080
2022-03-18 09:03:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:03:27 | INFO | train_inner | epoch 002:   3091 / 7971 loss=0.162, nll_loss=0.003, accuracy=96.4, wps=7738.3, ups=3.68, wpb=2102.2, bsz=37, num_updates=11000, lr=3.42975e-06, gnorm=4.94, loss_scale=128, train_wall=26, wall=3107
2022-03-18 09:03:53 | INFO | train_inner | epoch 002:   3191 / 7971 loss=0.165, nll_loss=0.003, accuracy=96.6, wps=7874.2, ups=3.73, wpb=2113.6, bsz=37.5, num_updates=11100, lr=3.41257e-06, gnorm=4.755, loss_scale=128, train_wall=26, wall=3134
2022-03-18 09:04:20 | INFO | train_inner | epoch 002:   3291 / 7971 loss=0.16, nll_loss=0.003, accuracy=96.4, wps=7580.8, ups=3.71, wpb=2043.1, bsz=35.5, num_updates=11200, lr=3.3954e-06, gnorm=4.368, loss_scale=256, train_wall=26, wall=3161
2022-03-18 09:04:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:04:48 | INFO | train_inner | epoch 002:   3392 / 7971 loss=0.181, nll_loss=0.003, accuracy=95.8, wps=7667.4, ups=3.67, wpb=2091.2, bsz=37, num_updates=11300, lr=3.37822e-06, gnorm=4.59, loss_scale=256, train_wall=26, wall=3188
2022-03-18 09:05:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:05:15 | INFO | train_inner | epoch 002:   3493 / 7971 loss=0.161, nll_loss=0.003, accuracy=96.7, wps=7483.4, ups=3.61, wpb=2070.3, bsz=35.8, num_updates=11400, lr=3.36104e-06, gnorm=4.751, loss_scale=128, train_wall=27, wall=3216
2022-03-18 09:05:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 09:05:43 | INFO | train_inner | epoch 002:   3594 / 7971 loss=0.185, nll_loss=0.003, accuracy=95.9, wps=7599, ups=3.68, wpb=2066.6, bsz=35.9, num_updates=11500, lr=3.34387e-06, gnorm=5.195, loss_scale=64, train_wall=26, wall=3243
2022-03-18 09:06:10 | INFO | train_inner | epoch 002:   3694 / 7971 loss=0.186, nll_loss=0.003, accuracy=96, wps=7684.4, ups=3.71, wpb=2073.7, bsz=36.2, num_updates=11600, lr=3.32669e-06, gnorm=5.037, loss_scale=128, train_wall=26, wall=3270
2022-03-18 09:06:36 | INFO | train_inner | epoch 002:   3794 / 7971 loss=0.166, nll_loss=0.003, accuracy=96.1, wps=7663.5, ups=3.71, wpb=2067.5, bsz=36.3, num_updates=11700, lr=3.30952e-06, gnorm=4.754, loss_scale=256, train_wall=26, wall=3297
2022-03-18 09:06:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:07:04 | INFO | train_inner | epoch 002:   3895 / 7971 loss=0.19, nll_loss=0.003, accuracy=95.7, wps=7815.1, ups=3.67, wpb=2127.8, bsz=37.3, num_updates=11800, lr=3.29234e-06, gnorm=5.185, loss_scale=128, train_wall=26, wall=3324
2022-03-18 09:07:31 | INFO | train_inner | epoch 002:   3995 / 7971 loss=0.154, nll_loss=0.003, accuracy=97, wps=7784.8, ups=3.69, wpb=2109.6, bsz=37.1, num_updates=11900, lr=3.27516e-06, gnorm=4.086, loss_scale=256, train_wall=26, wall=3351
2022-03-18 09:07:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:07:58 | INFO | train_inner | epoch 002:   4096 / 7971 loss=0.176, nll_loss=0.003, accuracy=96, wps=7717.6, ups=3.69, wpb=2089.7, bsz=36.9, num_updates=12000, lr=3.25799e-06, gnorm=4.813, loss_scale=256, train_wall=26, wall=3378
2022-03-18 09:08:25 | INFO | train_inner | epoch 002:   4196 / 7971 loss=0.159, nll_loss=0.003, accuracy=96.6, wps=7714, ups=3.71, wpb=2081.2, bsz=36.2, num_updates=12100, lr=3.24081e-06, gnorm=4.237, loss_scale=256, train_wall=26, wall=3405
2022-03-18 09:08:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:08:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 09:08:52 | INFO | train_inner | epoch 002:   4298 / 7971 loss=0.185, nll_loss=0.003, accuracy=96.1, wps=7207.3, ups=3.62, wpb=1988.8, bsz=34.9, num_updates=12200, lr=3.22363e-06, gnorm=4.613, loss_scale=64, train_wall=27, wall=3433
2022-03-18 09:09:20 | INFO | train_inner | epoch 002:   4398 / 7971 loss=0.137, nll_loss=0.002, accuracy=96.7, wps=7631, ups=3.66, wpb=2083.4, bsz=36, num_updates=12300, lr=3.20646e-06, gnorm=4.475, loss_scale=64, train_wall=26, wall=3460
2022-03-18 09:09:47 | INFO | train_inner | epoch 002:   4498 / 7971 loss=0.152, nll_loss=0.003, accuracy=96.4, wps=7817, ups=3.73, wpb=2096.5, bsz=36.8, num_updates=12400, lr=3.18928e-06, gnorm=4.917, loss_scale=128, train_wall=26, wall=3487
2022-03-18 09:10:16 | INFO | train_inner | epoch 002:   4598 / 7971 loss=0.155, nll_loss=0.003, accuracy=96.9, wps=6987, ups=3.35, wpb=2086.2, bsz=36.6, num_updates=12500, lr=3.17211e-06, gnorm=4.559, loss_scale=256, train_wall=29, wall=3517
2022-03-18 09:10:45 | INFO | train_inner | epoch 002:   4698 / 7971 loss=0.18, nll_loss=0.003, accuracy=96.1, wps=7537.8, ups=3.57, wpb=2113.5, bsz=36.5, num_updates=12600, lr=3.15493e-06, gnorm=4.762, loss_scale=512, train_wall=27, wall=3545
2022-03-18 09:10:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:11:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:11:12 | INFO | train_inner | epoch 002:   4800 / 7971 loss=0.183, nll_loss=0.003, accuracy=96, wps=7492.5, ups=3.63, wpb=2064.8, bsz=36.2, num_updates=12700, lr=3.13775e-06, gnorm=5.407, loss_scale=128, train_wall=27, wall=3573
2022-03-18 09:11:39 | INFO | train_inner | epoch 002:   4900 / 7971 loss=0.169, nll_loss=0.003, accuracy=96.4, wps=7604.1, ups=3.68, wpb=2066.5, bsz=36.4, num_updates=12800, lr=3.12058e-06, gnorm=4.093, loss_scale=128, train_wall=26, wall=3600
2022-03-18 09:12:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:12:07 | INFO | train_inner | epoch 002:   5001 / 7971 loss=0.164, nll_loss=0.003, accuracy=96.6, wps=7554, ups=3.66, wpb=2061.3, bsz=36, num_updates=12900, lr=3.1034e-06, gnorm=4.231, loss_scale=128, train_wall=26, wall=3627
2022-03-18 09:12:33 | INFO | train_inner | epoch 002:   5101 / 7971 loss=0.185, nll_loss=0.003, accuracy=95.9, wps=7678.6, ups=3.73, wpb=2058.1, bsz=36, num_updates=13000, lr=3.08622e-06, gnorm=5.297, loss_scale=128, train_wall=26, wall=3654
2022-03-18 09:12:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:13:00 | INFO | train_inner | epoch 002:   5202 / 7971 loss=0.178, nll_loss=0.003, accuracy=96.2, wps=7749.4, ups=3.7, wpb=2092.4, bsz=36.9, num_updates=13100, lr=3.06905e-06, gnorm=4.849, loss_scale=128, train_wall=26, wall=3681
2022-03-18 09:13:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:13:27 | INFO | train_inner | epoch 002:   5303 / 7971 loss=0.153, nll_loss=0.003, accuracy=96.5, wps=7668.2, ups=3.7, wpb=2075.2, bsz=36.4, num_updates=13200, lr=3.05187e-06, gnorm=4.018, loss_scale=128, train_wall=26, wall=3708
2022-03-18 09:13:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 09:13:55 | INFO | train_inner | epoch 002:   5404 / 7971 loss=0.151, nll_loss=0.003, accuracy=96.7, wps=7612.4, ups=3.69, wpb=2065.1, bsz=36.6, num_updates=13300, lr=3.0347e-06, gnorm=3.923, loss_scale=64, train_wall=26, wall=3735
2022-03-18 09:14:21 | INFO | train_inner | epoch 002:   5504 / 7971 loss=0.174, nll_loss=0.003, accuracy=96.2, wps=7731, ups=3.72, wpb=2077.2, bsz=36.3, num_updates=13400, lr=3.01752e-06, gnorm=4.815, loss_scale=64, train_wall=26, wall=3762
2022-03-18 09:14:48 | INFO | train_inner | epoch 002:   5604 / 7971 loss=0.171, nll_loss=0.003, accuracy=96.3, wps=7690.9, ups=3.71, wpb=2073.2, bsz=35.7, num_updates=13500, lr=3.00034e-06, gnorm=4.419, loss_scale=128, train_wall=26, wall=3789
2022-03-18 09:15:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:15:16 | INFO | train_inner | epoch 002:   5705 / 7971 loss=0.173, nll_loss=0.003, accuracy=96, wps=7535.9, ups=3.63, wpb=2077.7, bsz=36.2, num_updates=13600, lr=2.98317e-06, gnorm=4.669, loss_scale=128, train_wall=27, wall=3817
2022-03-18 09:15:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 09:15:43 | INFO | train_inner | epoch 002:   5806 / 7971 loss=0.165, nll_loss=0.003, accuracy=96.4, wps=7667.3, ups=3.66, wpb=2092.1, bsz=36.5, num_updates=13700, lr=2.96599e-06, gnorm=4.322, loss_scale=64, train_wall=26, wall=3844
2022-03-18 09:16:10 | INFO | train_inner | epoch 002:   5906 / 7971 loss=0.165, nll_loss=0.003, accuracy=96.3, wps=7693.8, ups=3.71, wpb=2076, bsz=36.1, num_updates=13800, lr=2.94881e-06, gnorm=4.464, loss_scale=128, train_wall=26, wall=3871
2022-03-18 09:16:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 09:16:38 | INFO | train_inner | epoch 002:   6007 / 7971 loss=0.152, nll_loss=0.003, accuracy=96.4, wps=7619.7, ups=3.66, wpb=2079.2, bsz=36.1, num_updates=13900, lr=2.93164e-06, gnorm=4.691, loss_scale=64, train_wall=26, wall=3898
2022-03-18 09:17:05 | INFO | train_inner | epoch 002:   6107 / 7971 loss=0.161, nll_loss=0.003, accuracy=96.4, wps=7293.5, ups=3.59, wpb=2031, bsz=35.4, num_updates=14000, lr=2.91446e-06, gnorm=4.435, loss_scale=128, train_wall=27, wall=3926
2022-03-18 09:17:33 | INFO | train_inner | epoch 002:   6207 / 7971 loss=0.18, nll_loss=0.003, accuracy=96.3, wps=7742, ups=3.68, wpb=2105.3, bsz=36.6, num_updates=14100, lr=2.89729e-06, gnorm=4.391, loss_scale=256, train_wall=26, wall=3953
2022-03-18 09:17:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:18:00 | INFO | train_inner | epoch 002:   6308 / 7971 loss=0.128, nll_loss=0.002, accuracy=96.9, wps=7579.3, ups=3.66, wpb=2071.9, bsz=36.3, num_updates=14200, lr=2.88011e-06, gnorm=3.687, loss_scale=128, train_wall=26, wall=3980
2022-03-18 09:18:27 | INFO | train_inner | epoch 002:   6408 / 7971 loss=0.135, nll_loss=0.002, accuracy=96.9, wps=7710.6, ups=3.71, wpb=2078.1, bsz=36.3, num_updates=14300, lr=2.86293e-06, gnorm=3.82, loss_scale=256, train_wall=26, wall=4007
2022-03-18 09:18:54 | INFO | train_inner | epoch 002:   6508 / 7971 loss=0.168, nll_loss=0.003, accuracy=96.6, wps=7630.8, ups=3.71, wpb=2058.2, bsz=35.9, num_updates=14400, lr=2.84576e-06, gnorm=4.186, loss_scale=512, train_wall=26, wall=4034
2022-03-18 09:19:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:19:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:19:21 | INFO | train_inner | epoch 002:   6610 / 7971 loss=0.171, nll_loss=0.003, accuracy=96.1, wps=7610.5, ups=3.65, wpb=2083.7, bsz=36.9, num_updates=14500, lr=2.82858e-06, gnorm=4.306, loss_scale=128, train_wall=26, wall=4062
2022-03-18 09:19:49 | INFO | train_inner | epoch 002:   6710 / 7971 loss=0.143, nll_loss=0.002, accuracy=96.6, wps=7592.3, ups=3.64, wpb=2086.9, bsz=36.2, num_updates=14600, lr=2.81141e-06, gnorm=4.363, loss_scale=256, train_wall=27, wall=4089
2022-03-18 09:20:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:20:16 | INFO | train_inner | epoch 002:   6811 / 7971 loss=0.156, nll_loss=0.003, accuracy=96.5, wps=7566.8, ups=3.63, wpb=2083.7, bsz=36.5, num_updates=14700, lr=2.79423e-06, gnorm=4.223, loss_scale=128, train_wall=27, wall=4117
2022-03-18 09:20:43 | INFO | train_inner | epoch 002:   6911 / 7971 loss=0.152, nll_loss=0.003, accuracy=96.6, wps=7452.1, ups=3.68, wpb=2024.9, bsz=35.4, num_updates=14800, lr=2.77705e-06, gnorm=4.018, loss_scale=256, train_wall=26, wall=4144
2022-03-18 09:21:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:21:11 | INFO | train_inner | epoch 002:   7012 / 7971 loss=0.17, nll_loss=0.003, accuracy=96.1, wps=7430.6, ups=3.65, wpb=2037.5, bsz=35.8, num_updates=14900, lr=2.75988e-06, gnorm=4.728, loss_scale=128, train_wall=27, wall=4171
2022-03-18 09:21:38 | INFO | train_inner | epoch 002:   7112 / 7971 loss=0.15, nll_loss=0.003, accuracy=96.7, wps=7637.7, ups=3.7, wpb=2065.5, bsz=35.9, num_updates=15000, lr=2.7427e-06, gnorm=4.185, loss_scale=128, train_wall=26, wall=4198
2022-03-18 09:21:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:22:05 | INFO | train_inner | epoch 002:   7213 / 7971 loss=0.159, nll_loss=0.003, accuracy=96.6, wps=7597.6, ups=3.7, wpb=2051.8, bsz=36, num_updates=15100, lr=2.72552e-06, gnorm=4.083, loss_scale=128, train_wall=26, wall=4225
2022-03-18 09:22:32 | INFO | train_inner | epoch 002:   7313 / 7971 loss=0.174, nll_loss=0.003, accuracy=96.2, wps=7554.5, ups=3.73, wpb=2023.8, bsz=35.6, num_updates=15200, lr=2.70835e-06, gnorm=4.587, loss_scale=256, train_wall=26, wall=4252
2022-03-18 09:22:59 | INFO | train_inner | epoch 002:   7413 / 7971 loss=0.164, nll_loss=0.003, accuracy=96.5, wps=7580.3, ups=3.71, wpb=2041.6, bsz=35.5, num_updates=15300, lr=2.69117e-06, gnorm=3.834, loss_scale=256, train_wall=26, wall=4279
2022-03-18 09:23:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:23:26 | INFO | train_inner | epoch 002:   7514 / 7971 loss=0.155, nll_loss=0.003, accuracy=96.3, wps=7530.1, ups=3.65, wpb=2062.4, bsz=35.9, num_updates=15400, lr=2.674e-06, gnorm=4.563, loss_scale=256, train_wall=26, wall=4307
2022-03-18 09:23:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:23:53 | INFO | train_inner | epoch 002:   7615 / 7971 loss=0.164, nll_loss=0.003, accuracy=96.3, wps=7791.5, ups=3.71, wpb=2098, bsz=36.5, num_updates=15500, lr=2.65682e-06, gnorm=4.714, loss_scale=128, train_wall=26, wall=4334
2022-03-18 09:24:20 | INFO | train_inner | epoch 002:   7715 / 7971 loss=0.151, nll_loss=0.003, accuracy=96.8, wps=7744.9, ups=3.75, wpb=2068, bsz=36.4, num_updates=15600, lr=2.63964e-06, gnorm=3.861, loss_scale=256, train_wall=26, wall=4360
2022-03-18 09:24:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:24:47 | INFO | train_inner | epoch 002:   7816 / 7971 loss=0.149, nll_loss=0.003, accuracy=96.6, wps=7797.5, ups=3.72, wpb=2098.8, bsz=36.3, num_updates=15700, lr=2.62247e-06, gnorm=4.547, loss_scale=128, train_wall=26, wall=4387
2022-03-18 09:25:13 | INFO | train_inner | epoch 002:   7916 / 7971 loss=0.156, nll_loss=0.003, accuracy=96.7, wps=7789.6, ups=3.75, wpb=2078.5, bsz=36.4, num_updates=15800, lr=2.60529e-06, gnorm=4.31, loss_scale=256, train_wall=26, wall=4414
2022-03-18 09:25:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-18 09:25:57 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.17 | nll_loss 0.003 | accuracy 96.4 | wps 29674.3 | wpb 2056.3 | bsz 36 | num_updates 15855 | best_accuracy 96.4
2022-03-18 09:25:57 | INFO | fairseq_cli.train | begin save checkpoint
2022-03-18 09:25:57 | INFO | fairseq.trainer | Preparing to save checkpoint to /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint2.pt after 15855 updates
2022-03-18 09:26:33 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint2.pt
2022-03-18 09:27:13 | INFO | fairseq.checkpoint_utils | saved checkpoint /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint2.pt (epoch 2 @ 15855 updates, score 96.4) (writing took 75.79889574833214 seconds)
2022-03-18 09:27:13 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-18 09:27:13 | INFO | train | epoch 002 | loss 0.167 | nll_loss 0.003 | accuracy 96.3 | wps 7260.5 | ups 3.5 | wpb 2075.5 | bsz 36.3 | num_updates 15855 | lr 2.59584e-06 | gnorm 4.567 | loss_scale 256 | train_wall 2083 | wall 4534
2022-03-18 09:27:18 | INFO | fairseq.trainer | begin training epoch 3
2022-03-18 09:27:31 | INFO | train_inner | epoch 003:     45 / 7971 loss=0.15, nll_loss=0.003, accuracy=96.7, wps=1509.7, ups=0.73, wpb=2074.6, bsz=36.4, num_updates=15900, lr=2.58811e-06, gnorm=4.026, loss_scale=256, train_wall=26, wall=4551
2022-03-18 09:27:58 | INFO | train_inner | epoch 003:    145 / 7971 loss=0.135, nll_loss=0.002, accuracy=97.1, wps=7737.1, ups=3.65, wpb=2117.1, bsz=37, num_updates=16000, lr=2.57094e-06, gnorm=3.612, loss_scale=512, train_wall=26, wall=4579
2022-03-18 09:27:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:28:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:28:26 | INFO | train_inner | epoch 003:    247 / 7971 loss=0.118, nll_loss=0.002, accuracy=97.3, wps=7279.2, ups=3.57, wpb=2040.7, bsz=35.7, num_updates=16100, lr=2.55376e-06, gnorm=3.632, loss_scale=128, train_wall=27, wall=4607
2022-03-18 09:28:54 | INFO | train_inner | epoch 003:    347 / 7971 loss=0.122, nll_loss=0.002, accuracy=97.3, wps=7526.1, ups=3.58, wpb=2101, bsz=36.3, num_updates=16200, lr=2.53659e-06, gnorm=3.877, loss_scale=256, train_wall=27, wall=4635
2022-03-18 09:29:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:29:22 | INFO | train_inner | epoch 003:    448 / 7971 loss=0.127, nll_loss=0.002, accuracy=97, wps=7537.4, ups=3.61, wpb=2089.9, bsz=36.5, num_updates=16300, lr=2.51941e-06, gnorm=3.87, loss_scale=128, train_wall=27, wall=4662
2022-03-18 09:29:51 | INFO | train_inner | epoch 003:    548 / 7971 loss=0.128, nll_loss=0.002, accuracy=97.2, wps=7025.2, ups=3.43, wpb=2050.5, bsz=35.8, num_updates=16400, lr=2.50223e-06, gnorm=3.756, loss_scale=256, train_wall=28, wall=4692
2022-03-18 09:30:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:30:24 | INFO | train_inner | epoch 003:    649 / 7971 loss=0.126, nll_loss=0.002, accuracy=97.2, wps=6377.7, ups=3.07, wpb=2078.8, bsz=36.1, num_updates=16500, lr=2.48506e-06, gnorm=3.611, loss_scale=256, train_wall=31, wall=4724
2022-03-18 09:30:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:30:51 | INFO | train_inner | epoch 003:    750 / 7971 loss=0.131, nll_loss=0.002, accuracy=97.2, wps=7502.8, ups=3.65, wpb=2056.8, bsz=36, num_updates=16600, lr=2.46788e-06, gnorm=3.806, loss_scale=128, train_wall=26, wall=4752
2022-03-18 09:31:18 | INFO | train_inner | epoch 003:    850 / 7971 loss=0.132, nll_loss=0.002, accuracy=97.1, wps=7535.5, ups=3.67, wpb=2053.4, bsz=35.4, num_updates=16700, lr=2.4507e-06, gnorm=4.004, loss_scale=256, train_wall=26, wall=4779
2022-03-18 09:31:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:31:46 | INFO | train_inner | epoch 003:    951 / 7971 loss=0.106, nll_loss=0.002, accuracy=97.6, wps=7448.5, ups=3.65, wpb=2043, bsz=35.3, num_updates=16800, lr=2.43353e-06, gnorm=3.545, loss_scale=128, train_wall=26, wall=4806
2022-03-18 09:32:13 | INFO | train_inner | epoch 003:   1051 / 7971 loss=0.122, nll_loss=0.002, accuracy=97.1, wps=7639, ups=3.7, wpb=2064.5, bsz=36.5, num_updates=16900, lr=2.41635e-06, gnorm=3.907, loss_scale=128, train_wall=26, wall=4833
2022-03-18 09:32:39 | INFO | train_inner | epoch 003:   1151 / 7971 loss=0.121, nll_loss=0.002, accuracy=97.1, wps=7974.4, ups=3.74, wpb=2130.7, bsz=37.7, num_updates=17000, lr=2.39918e-06, gnorm=3.695, loss_scale=256, train_wall=26, wall=4860
2022-03-18 09:32:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:33:06 | INFO | train_inner | epoch 003:   1252 / 7971 loss=0.113, nll_loss=0.002, accuracy=97.4, wps=7741.6, ups=3.71, wpb=2087.2, bsz=36.6, num_updates=17100, lr=2.382e-06, gnorm=3.864, loss_scale=256, train_wall=26, wall=4887
2022-03-18 09:33:33 | INFO | train_inner | epoch 003:   1352 / 7971 loss=0.122, nll_loss=0.002, accuracy=97.2, wps=7915.3, ups=3.73, wpb=2122.6, bsz=37.5, num_updates=17200, lr=2.36482e-06, gnorm=3.946, loss_scale=512, train_wall=26, wall=4914
2022-03-18 09:33:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:33:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:34:00 | INFO | train_inner | epoch 003:   1454 / 7971 loss=0.135, nll_loss=0.002, accuracy=97, wps=7725.5, ups=3.69, wpb=2095.7, bsz=37.2, num_updates=17300, lr=2.34765e-06, gnorm=3.768, loss_scale=128, train_wall=26, wall=4941
2022-03-18 09:34:27 | INFO | train_inner | epoch 003:   1554 / 7971 loss=0.113, nll_loss=0.002, accuracy=97.4, wps=7784, ups=3.75, wpb=2075.5, bsz=36.4, num_updates=17400, lr=2.33047e-06, gnorm=3.604, loss_scale=256, train_wall=26, wall=4968
2022-03-18 09:34:54 | INFO | train_inner | epoch 003:   1654 / 7971 loss=0.126, nll_loss=0.002, accuracy=97.1, wps=7801.4, ups=3.71, wpb=2104, bsz=36.7, num_updates=17500, lr=2.31329e-06, gnorm=4.16, loss_scale=256, train_wall=26, wall=4995
2022-03-18 09:34:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:35:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:35:22 | INFO | train_inner | epoch 003:   1756 / 7971 loss=0.112, nll_loss=0.002, accuracy=97.3, wps=7479.9, ups=3.63, wpb=2058, bsz=36.3, num_updates=17600, lr=2.29612e-06, gnorm=3.745, loss_scale=128, train_wall=27, wall=5022
2022-03-18 09:35:49 | INFO | train_inner | epoch 003:   1856 / 7971 loss=0.128, nll_loss=0.002, accuracy=96.8, wps=7537.6, ups=3.7, wpb=2038.5, bsz=35.9, num_updates=17700, lr=2.27894e-06, gnorm=4.327, loss_scale=256, train_wall=26, wall=5049
2022-03-18 09:36:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:36:16 | INFO | train_inner | epoch 003:   1957 / 7971 loss=0.107, nll_loss=0.002, accuracy=97.5, wps=7680.2, ups=3.67, wpb=2092.6, bsz=36.6, num_updates=17800, lr=2.26177e-06, gnorm=3.753, loss_scale=128, train_wall=26, wall=5076
2022-03-18 09:36:44 | INFO | train_inner | epoch 003:   2057 / 7971 loss=0.132, nll_loss=0.002, accuracy=96.8, wps=7563.5, ups=3.61, wpb=2095.2, bsz=36.7, num_updates=17900, lr=2.24459e-06, gnorm=4.039, loss_scale=256, train_wall=27, wall=5104
2022-03-18 09:36:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:37:11 | INFO | train_inner | epoch 003:   2158 / 7971 loss=0.123, nll_loss=0.002, accuracy=97.2, wps=7355.1, ups=3.62, wpb=2029.5, bsz=35.7, num_updates=18000, lr=2.22741e-06, gnorm=3.881, loss_scale=128, train_wall=27, wall=5132
2022-03-18 09:37:38 | INFO | train_inner | epoch 003:   2258 / 7971 loss=0.118, nll_loss=0.002, accuracy=97.3, wps=7847.6, ups=3.73, wpb=2101.7, bsz=36.8, num_updates=18100, lr=2.21024e-06, gnorm=4.006, loss_scale=256, train_wall=26, wall=5158
2022-03-18 09:37:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:38:05 | INFO | train_inner | epoch 003:   2359 / 7971 loss=0.116, nll_loss=0.002, accuracy=97.1, wps=7630.4, ups=3.71, wpb=2058.5, bsz=35.7, num_updates=18200, lr=2.19306e-06, gnorm=4.234, loss_scale=128, train_wall=26, wall=5185
2022-03-18 09:38:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:38:34 | INFO | train_inner | epoch 003:   2460 / 7971 loss=0.13, nll_loss=0.002, accuracy=97.1, wps=7076.3, ups=3.41, wpb=2072.9, bsz=36.5, num_updates=18300, lr=2.17588e-06, gnorm=3.897, loss_scale=128, train_wall=28, wall=5215
2022-03-18 09:39:02 | INFO | train_inner | epoch 003:   2560 / 7971 loss=0.121, nll_loss=0.002, accuracy=97.2, wps=7389.3, ups=3.65, wpb=2023.7, bsz=35.6, num_updates=18400, lr=2.15871e-06, gnorm=4.006, loss_scale=256, train_wall=26, wall=5242
2022-03-18 09:39:28 | INFO | train_inner | epoch 003:   2660 / 7971 loss=0.14, nll_loss=0.002, accuracy=97, wps=7762.6, ups=3.73, wpb=2079, bsz=36.5, num_updates=18500, lr=2.14153e-06, gnorm=4.136, loss_scale=256, train_wall=26, wall=5269
2022-03-18 09:39:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:39:56 | INFO | train_inner | epoch 003:   2761 / 7971 loss=0.113, nll_loss=0.002, accuracy=97.6, wps=7566.3, ups=3.68, wpb=2056.6, bsz=36, num_updates=18600, lr=2.12436e-06, gnorm=3.729, loss_scale=256, train_wall=26, wall=5296
2022-03-18 09:40:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:40:23 | INFO | train_inner | epoch 003:   2862 / 7971 loss=0.113, nll_loss=0.002, accuracy=97.3, wps=7826.6, ups=3.71, wpb=2112.1, bsz=37.1, num_updates=18700, lr=2.10718e-06, gnorm=3.855, loss_scale=256, train_wall=26, wall=5323
2022-03-18 09:40:50 | INFO | train_inner | epoch 003:   2962 / 7971 loss=0.116, nll_loss=0.002, accuracy=97.4, wps=7429.6, ups=3.69, wpb=2011.2, bsz=35.2, num_updates=18800, lr=2.09e-06, gnorm=3.846, loss_scale=512, train_wall=26, wall=5350
2022-03-18 09:41:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:41:17 | INFO | train_inner | epoch 003:   3063 / 7971 loss=0.11, nll_loss=0.002, accuracy=97.7, wps=7608.7, ups=3.67, wpb=2070.4, bsz=36, num_updates=18900, lr=2.07283e-06, gnorm=3.57, loss_scale=256, train_wall=26, wall=5377
2022-03-18 09:41:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:41:44 | INFO | train_inner | epoch 003:   3164 / 7971 loss=0.124, nll_loss=0.002, accuracy=97.4, wps=7316.9, ups=3.66, wpb=1998.7, bsz=35, num_updates=19000, lr=2.05565e-06, gnorm=4.224, loss_scale=128, train_wall=26, wall=5405
2022-03-18 09:42:12 | INFO | train_inner | epoch 003:   3264 / 7971 loss=0.109, nll_loss=0.002, accuracy=97.5, wps=7582.2, ups=3.65, wpb=2075.2, bsz=36.6, num_updates=19100, lr=2.03847e-06, gnorm=3.712, loss_scale=256, train_wall=26, wall=5432
2022-03-18 09:42:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:42:41 | INFO | train_inner | epoch 003:   3365 / 7971 loss=0.11, nll_loss=0.002, accuracy=97.6, wps=6910, ups=3.34, wpb=2066.6, bsz=36.1, num_updates=19200, lr=2.0213e-06, gnorm=3.573, loss_scale=256, train_wall=29, wall=5462
2022-03-18 09:42:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:43:09 | INFO | train_inner | epoch 003:   3466 / 7971 loss=0.112, nll_loss=0.002, accuracy=97.4, wps=7630.1, ups=3.65, wpb=2088.8, bsz=36.5, num_updates=19300, lr=2.00412e-06, gnorm=3.699, loss_scale=128, train_wall=26, wall=5489
2022-03-18 09:43:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 09:43:36 | INFO | train_inner | epoch 003:   3567 / 7971 loss=0.134, nll_loss=0.002, accuracy=97, wps=7567.3, ups=3.66, wpb=2065.8, bsz=36.3, num_updates=19400, lr=1.98695e-06, gnorm=4.584, loss_scale=64, train_wall=26, wall=5517
2022-03-18 09:44:03 | INFO | train_inner | epoch 003:   3667 / 7971 loss=0.135, nll_loss=0.002, accuracy=97.1, wps=7803.2, ups=3.72, wpb=2097.9, bsz=36.6, num_updates=19500, lr=1.96977e-06, gnorm=4.39, loss_scale=128, train_wall=26, wall=5544
2022-03-18 09:44:30 | INFO | train_inner | epoch 003:   3767 / 7971 loss=0.12, nll_loss=0.002, accuracy=97.5, wps=7742.7, ups=3.72, wpb=2081.4, bsz=36.6, num_updates=19600, lr=1.95259e-06, gnorm=3.898, loss_scale=256, train_wall=26, wall=5570
2022-03-18 09:44:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:44:57 | INFO | train_inner | epoch 003:   3868 / 7971 loss=0.103, nll_loss=0.002, accuracy=97.7, wps=7638.5, ups=3.67, wpb=2081.6, bsz=36.4, num_updates=19700, lr=1.93542e-06, gnorm=3.951, loss_scale=128, train_wall=26, wall=5598
2022-03-18 09:45:24 | INFO | train_inner | epoch 003:   3968 / 7971 loss=0.131, nll_loss=0.002, accuracy=97.1, wps=7791.5, ups=3.7, wpb=2103.1, bsz=36.6, num_updates=19800, lr=1.91824e-06, gnorm=4.137, loss_scale=256, train_wall=26, wall=5625
2022-03-18 09:45:51 | INFO | train_inner | epoch 003:   4068 / 7971 loss=0.095, nll_loss=0.002, accuracy=97.6, wps=7692, ups=3.7, wpb=2080, bsz=35.8, num_updates=19900, lr=1.90106e-06, gnorm=3.641, loss_scale=512, train_wall=26, wall=5652
2022-03-18 09:46:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:46:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:46:19 | INFO | train_inner | epoch 003:   4170 / 7971 loss=0.122, nll_loss=0.002, accuracy=97.3, wps=7479.6, ups=3.64, wpb=2056.2, bsz=35.9, num_updates=20000, lr=1.88389e-06, gnorm=4.265, loss_scale=128, train_wall=27, wall=5679
2022-03-18 09:46:46 | INFO | train_inner | epoch 003:   4270 / 7971 loss=0.114, nll_loss=0.002, accuracy=97.5, wps=7713.1, ups=3.7, wpb=2086.3, bsz=36.6, num_updates=20100, lr=1.86671e-06, gnorm=3.821, loss_scale=128, train_wall=26, wall=5706
2022-03-18 09:47:13 | INFO | train_inner | epoch 003:   4370 / 7971 loss=0.139, nll_loss=0.002, accuracy=96.7, wps=7734.3, ups=3.71, wpb=2084.3, bsz=36.5, num_updates=20200, lr=1.84954e-06, gnorm=4.144, loss_scale=256, train_wall=26, wall=5733
2022-03-18 09:47:40 | INFO | train_inner | epoch 003:   4470 / 7971 loss=0.113, nll_loss=0.002, accuracy=97.5, wps=7591.2, ups=3.67, wpb=2069, bsz=35.9, num_updates=20300, lr=1.83236e-06, gnorm=4.299, loss_scale=512, train_wall=26, wall=5760
2022-03-18 09:47:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:47:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:48:07 | INFO | train_inner | epoch 003:   4572 / 7971 loss=0.118, nll_loss=0.002, accuracy=97.5, wps=7386.4, ups=3.63, wpb=2032.7, bsz=35.4, num_updates=20400, lr=1.81518e-06, gnorm=4.444, loss_scale=128, train_wall=27, wall=5788
2022-03-18 09:48:35 | INFO | train_inner | epoch 003:   4672 / 7971 loss=0.13, nll_loss=0.002, accuracy=97.2, wps=7734.4, ups=3.7, wpb=2090.6, bsz=36.4, num_updates=20500, lr=1.79801e-06, gnorm=4.472, loss_scale=256, train_wall=26, wall=5815
2022-03-18 09:48:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:49:02 | INFO | train_inner | epoch 003:   4773 / 7971 loss=0.12, nll_loss=0.002, accuracy=97.4, wps=7579.1, ups=3.67, wpb=2067.8, bsz=36.3, num_updates=20600, lr=1.78083e-06, gnorm=4.371, loss_scale=128, train_wall=26, wall=5842
2022-03-18 09:49:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 09:49:29 | INFO | train_inner | epoch 003:   4874 / 7971 loss=0.106, nll_loss=0.002, accuracy=97.4, wps=7756.7, ups=3.69, wpb=2101.7, bsz=36.5, num_updates=20700, lr=1.76366e-06, gnorm=3.925, loss_scale=64, train_wall=26, wall=5869
2022-03-18 09:49:56 | INFO | train_inner | epoch 003:   4974 / 7971 loss=0.113, nll_loss=0.002, accuracy=97.4, wps=7470.7, ups=3.68, wpb=2031, bsz=35.4, num_updates=20800, lr=1.74648e-06, gnorm=4.045, loss_scale=128, train_wall=26, wall=5897
2022-03-18 09:50:27 | INFO | train_inner | epoch 003:   5074 / 7971 loss=0.119, nll_loss=0.002, accuracy=97.4, wps=6611.4, ups=3.2, wpb=2065.9, bsz=35.8, num_updates=20900, lr=1.7293e-06, gnorm=4.29, loss_scale=256, train_wall=30, wall=5928
2022-03-18 09:50:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:50:56 | INFO | train_inner | epoch 003:   5175 / 7971 loss=0.122, nll_loss=0.002, accuracy=97.4, wps=7247.8, ups=3.48, wpb=2082.2, bsz=36.8, num_updates=21000, lr=1.71213e-06, gnorm=4.172, loss_scale=128, train_wall=28, wall=5957
2022-03-18 09:51:23 | INFO | train_inner | epoch 003:   5275 / 7971 loss=0.116, nll_loss=0.002, accuracy=97.1, wps=7929.2, ups=3.7, wpb=2140.3, bsz=37.4, num_updates=21100, lr=1.69495e-06, gnorm=4.385, loss_scale=256, train_wall=26, wall=5984
2022-03-18 09:51:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:51:50 | INFO | train_inner | epoch 003:   5376 / 7971 loss=0.122, nll_loss=0.002, accuracy=97.3, wps=7686.6, ups=3.68, wpb=2087.7, bsz=36.9, num_updates=21200, lr=1.67777e-06, gnorm=4.166, loss_scale=128, train_wall=26, wall=6011
2022-03-18 09:52:17 | INFO | train_inner | epoch 003:   5476 / 7971 loss=0.125, nll_loss=0.002, accuracy=97.4, wps=7803.2, ups=3.71, wpb=2103.6, bsz=36.6, num_updates=21300, lr=1.6606e-06, gnorm=4.085, loss_scale=256, train_wall=26, wall=6038
2022-03-18 09:52:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:52:45 | INFO | train_inner | epoch 003:   5577 / 7971 loss=0.103, nll_loss=0.002, accuracy=97.7, wps=7596.8, ups=3.63, wpb=2090.6, bsz=36.4, num_updates=21400, lr=1.64342e-06, gnorm=3.796, loss_scale=128, train_wall=27, wall=6065
2022-03-18 09:53:12 | INFO | train_inner | epoch 003:   5677 / 7971 loss=0.123, nll_loss=0.002, accuracy=97.1, wps=7784.5, ups=3.69, wpb=2109.4, bsz=36.4, num_updates=21500, lr=1.62625e-06, gnorm=4.528, loss_scale=256, train_wall=26, wall=6092
2022-03-18 09:53:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:53:39 | INFO | train_inner | epoch 003:   5778 / 7971 loss=0.112, nll_loss=0.002, accuracy=97.7, wps=7662.7, ups=3.66, wpb=2091.8, bsz=36.5, num_updates=21600, lr=1.60907e-06, gnorm=3.664, loss_scale=128, train_wall=26, wall=6120
2022-03-18 09:54:06 | INFO | train_inner | epoch 003:   5878 / 7971 loss=0.129, nll_loss=0.002, accuracy=97, wps=7606.7, ups=3.7, wpb=2058.1, bsz=35.8, num_updates=21700, lr=1.59189e-06, gnorm=4.117, loss_scale=256, train_wall=26, wall=6147
2022-03-18 09:54:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:54:33 | INFO | train_inner | epoch 003:   5979 / 7971 loss=0.111, nll_loss=0.002, accuracy=97.7, wps=7779.8, ups=3.67, wpb=2121.7, bsz=37.2, num_updates=21800, lr=1.57472e-06, gnorm=4.139, loss_scale=128, train_wall=26, wall=6174
2022-03-18 09:54:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:55:01 | INFO | train_inner | epoch 003:   6080 / 7971 loss=0.121, nll_loss=0.002, accuracy=97.2, wps=7690.2, ups=3.67, wpb=2093.4, bsz=36.6, num_updates=21900, lr=1.55754e-06, gnorm=4.063, loss_scale=128, train_wall=26, wall=6201
2022-03-18 09:55:28 | INFO | train_inner | epoch 003:   6180 / 7971 loss=0.113, nll_loss=0.002, accuracy=97.4, wps=7653.8, ups=3.7, wpb=2067.8, bsz=36.4, num_updates=22000, lr=1.54036e-06, gnorm=3.846, loss_scale=256, train_wall=26, wall=6228
2022-03-18 09:55:55 | INFO | train_inner | epoch 003:   6280 / 7971 loss=0.134, nll_loss=0.002, accuracy=97, wps=7670, ups=3.68, wpb=2082.6, bsz=36.8, num_updates=22100, lr=1.52319e-06, gnorm=4.192, loss_scale=256, train_wall=26, wall=6255
2022-03-18 09:55:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:56:22 | INFO | train_inner | epoch 003:   6381 / 7971 loss=0.11, nll_loss=0.002, accuracy=97.2, wps=7616.5, ups=3.65, wpb=2085.2, bsz=36.4, num_updates=22200, lr=1.50601e-06, gnorm=4.186, loss_scale=128, train_wall=26, wall=6283
2022-03-18 09:56:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:56:50 | INFO | train_inner | epoch 003:   6482 / 7971 loss=0.134, nll_loss=0.002, accuracy=96.8, wps=7566.6, ups=3.6, wpb=2100.3, bsz=37.1, num_updates=22300, lr=1.48884e-06, gnorm=4.613, loss_scale=128, train_wall=27, wall=6311
2022-03-18 09:57:19 | INFO | train_inner | epoch 003:   6582 / 7971 loss=0.116, nll_loss=0.002, accuracy=97.3, wps=7027.5, ups=3.41, wpb=2062.8, bsz=36.2, num_updates=22400, lr=1.47166e-06, gnorm=4.404, loss_scale=256, train_wall=28, wall=6340
2022-03-18 09:57:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:57:47 | INFO | train_inner | epoch 003:   6683 / 7971 loss=0.116, nll_loss=0.002, accuracy=97.4, wps=7648.5, ups=3.67, wpb=2086.6, bsz=36.4, num_updates=22500, lr=1.45448e-06, gnorm=4.204, loss_scale=128, train_wall=26, wall=6367
2022-03-18 09:58:14 | INFO | train_inner | epoch 003:   6783 / 7971 loss=0.112, nll_loss=0.002, accuracy=97.7, wps=7462, ups=3.7, wpb=2017.2, bsz=35.2, num_updates=22600, lr=1.43731e-06, gnorm=3.687, loss_scale=256, train_wall=26, wall=6394
2022-03-18 09:58:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 09:58:42 | INFO | train_inner | epoch 003:   6884 / 7971 loss=0.119, nll_loss=0.002, accuracy=97, wps=7315.2, ups=3.51, wpb=2086.6, bsz=36.2, num_updates=22700, lr=1.42013e-06, gnorm=4.353, loss_scale=256, train_wall=28, wall=6423
2022-03-18 09:59:09 | INFO | train_inner | epoch 003:   6984 / 7971 loss=0.12, nll_loss=0.002, accuracy=97.3, wps=7695.2, ups=3.68, wpb=2088.9, bsz=36.6, num_updates=22800, lr=1.40295e-06, gnorm=4.176, loss_scale=256, train_wall=26, wall=6450
2022-03-18 09:59:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 09:59:37 | INFO | train_inner | epoch 003:   7085 / 7971 loss=0.123, nll_loss=0.002, accuracy=97.4, wps=7641.1, ups=3.69, wpb=2073.3, bsz=36, num_updates=22900, lr=1.38578e-06, gnorm=4.224, loss_scale=128, train_wall=26, wall=6477
2022-03-18 10:00:04 | INFO | train_inner | epoch 003:   7185 / 7971 loss=0.113, nll_loss=0.002, accuracy=97.5, wps=7593.3, ups=3.68, wpb=2062.8, bsz=36.2, num_updates=23000, lr=1.3686e-06, gnorm=3.666, loss_scale=256, train_wall=26, wall=6504
2022-03-18 10:00:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:00:31 | INFO | train_inner | epoch 003:   7286 / 7971 loss=0.142, nll_loss=0.002, accuracy=97, wps=7650.5, ups=3.65, wpb=2096, bsz=36.6, num_updates=23100, lr=1.35143e-06, gnorm=4.29, loss_scale=256, train_wall=26, wall=6532
2022-03-18 10:00:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:00:58 | INFO | train_inner | epoch 003:   7387 / 7971 loss=0.135, nll_loss=0.002, accuracy=96.8, wps=7608.7, ups=3.66, wpb=2077.7, bsz=36.2, num_updates=23200, lr=1.33425e-06, gnorm=4.373, loss_scale=128, train_wall=26, wall=6559
2022-03-18 10:01:25 | INFO | train_inner | epoch 003:   7487 / 7971 loss=0.116, nll_loss=0.002, accuracy=97.3, wps=7610.3, ups=3.7, wpb=2058.5, bsz=35.8, num_updates=23300, lr=1.31707e-06, gnorm=3.838, loss_scale=256, train_wall=26, wall=6586
2022-03-18 10:01:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:01:53 | INFO | train_inner | epoch 003:   7588 / 7971 loss=0.126, nll_loss=0.002, accuracy=97, wps=7563.9, ups=3.65, wpb=2074.3, bsz=35.9, num_updates=23400, lr=1.2999e-06, gnorm=3.735, loss_scale=256, train_wall=26, wall=6613
2022-03-18 10:02:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:02:20 | INFO | train_inner | epoch 003:   7689 / 7971 loss=0.109, nll_loss=0.002, accuracy=97.5, wps=7438.1, ups=3.64, wpb=2043.8, bsz=35.6, num_updates=23500, lr=1.28272e-06, gnorm=3.503, loss_scale=128, train_wall=27, wall=6641
2022-03-18 10:02:47 | INFO | train_inner | epoch 003:   7789 / 7971 loss=0.118, nll_loss=0.002, accuracy=97.5, wps=7643.4, ups=3.71, wpb=2059.4, bsz=36.1, num_updates=23600, lr=1.26554e-06, gnorm=4.036, loss_scale=256, train_wall=26, wall=6668
2022-03-18 10:02:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:03:17 | INFO | train_inner | epoch 003:   7890 / 7971 loss=0.108, nll_loss=0.002, accuracy=97.4, wps=7062.6, ups=3.42, wpb=2064.2, bsz=35.9, num_updates=23700, lr=1.24837e-06, gnorm=3.932, loss_scale=128, train_wall=28, wall=6697
2022-03-18 10:03:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-18 10:04:09 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.155 | nll_loss 0.003 | accuracy 96.8 | wps 29128.1 | wpb 2056.3 | bsz 36 | num_updates 23781 | best_accuracy 96.8
2022-03-18 10:04:09 | INFO | fairseq_cli.train | begin save checkpoint
2022-03-18 10:04:09 | INFO | fairseq.trainer | Preparing to save checkpoint to /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint3.pt after 23781 updates
2022-03-18 10:04:37 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint3.pt
2022-03-18 10:05:19 | INFO | fairseq.checkpoint_utils | saved checkpoint /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint3.pt (epoch 3 @ 23781 updates, score 96.8) (writing took 70.06819773837924 seconds)
2022-03-18 10:05:19 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-18 10:05:19 | INFO | train | epoch 003 | loss 0.12 | nll_loss 0.002 | accuracy 97.3 | wps 7193.5 | ups 3.47 | wpb 2074.8 | bsz 36.3 | num_updates 23781 | lr 1.23446e-06 | gnorm 3.996 | loss_scale 256 | train_wall 2105 | wall 6820
2022-03-18 10:05:24 | INFO | fairseq.trainer | begin training epoch 4
2022-03-18 10:05:30 | INFO | train_inner | epoch 004:     19 / 7971 loss=0.116, nll_loss=0.002, accuracy=97.7, wps=1509.4, ups=0.75, wpb=2010.2, bsz=35.4, num_updates=23800, lr=1.23119e-06, gnorm=3.918, loss_scale=256, train_wall=27, wall=6830
2022-03-18 10:05:57 | INFO | train_inner | epoch 004:    119 / 7971 loss=0.099, nll_loss=0.002, accuracy=97.8, wps=7651.6, ups=3.72, wpb=2055, bsz=35.7, num_updates=23900, lr=1.21402e-06, gnorm=3.12, loss_scale=512, train_wall=26, wall=6857
2022-03-18 10:06:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:06:24 | INFO | train_inner | epoch 004:    220 / 7971 loss=0.097, nll_loss=0.002, accuracy=97.8, wps=7696.2, ups=3.69, wpb=2083.6, bsz=36.5, num_updates=24000, lr=1.19684e-06, gnorm=3.419, loss_scale=256, train_wall=26, wall=6884
2022-03-18 10:06:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:06:52 | INFO | train_inner | epoch 004:    321 / 7971 loss=0.082, nll_loss=0.001, accuracy=98.2, wps=7298.1, ups=3.48, wpb=2095.9, bsz=37, num_updates=24100, lr=1.17966e-06, gnorm=3.048, loss_scale=256, train_wall=28, wall=6913
2022-03-18 10:07:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:07:21 | INFO | train_inner | epoch 004:    422 / 7971 loss=0.095, nll_loss=0.002, accuracy=97.8, wps=7353.4, ups=3.54, wpb=2076.4, bsz=36, num_updates=24200, lr=1.16249e-06, gnorm=3.305, loss_scale=128, train_wall=27, wall=6941
2022-03-18 10:07:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:07:48 | INFO | train_inner | epoch 004:    523 / 7971 loss=0.102, nll_loss=0.002, accuracy=97.7, wps=7463.3, ups=3.63, wpb=2054.4, bsz=36.1, num_updates=24300, lr=1.14531e-06, gnorm=3.465, loss_scale=128, train_wall=27, wall=6969
2022-03-18 10:08:15 | INFO | train_inner | epoch 004:    623 / 7971 loss=0.095, nll_loss=0.002, accuracy=97.7, wps=7664.7, ups=3.71, wpb=2065.6, bsz=36.4, num_updates=24400, lr=1.12813e-06, gnorm=3.676, loss_scale=128, train_wall=26, wall=6996
2022-03-18 10:08:42 | INFO | train_inner | epoch 004:    723 / 7971 loss=0.106, nll_loss=0.002, accuracy=97.6, wps=7693.4, ups=3.71, wpb=2072.5, bsz=36.2, num_updates=24500, lr=1.11096e-06, gnorm=3.509, loss_scale=256, train_wall=26, wall=7023
2022-03-18 10:09:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:09:09 | INFO | train_inner | epoch 004:    824 / 7971 loss=0.088, nll_loss=0.002, accuracy=98, wps=7692.1, ups=3.69, wpb=2083, bsz=36.6, num_updates=24600, lr=1.09378e-06, gnorm=3.515, loss_scale=256, train_wall=26, wall=7050
2022-03-18 10:09:36 | INFO | train_inner | epoch 004:    924 / 7971 loss=0.086, nll_loss=0.002, accuracy=98.1, wps=7624.4, ups=3.69, wpb=2064, bsz=36.3, num_updates=24700, lr=1.07661e-06, gnorm=3.419, loss_scale=512, train_wall=26, wall=7077
2022-03-18 10:09:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:10:03 | INFO | train_inner | epoch 004:   1025 / 7971 loss=0.103, nll_loss=0.002, accuracy=97.9, wps=7591.8, ups=3.67, wpb=2070.5, bsz=36.2, num_updates=24800, lr=1.05943e-06, gnorm=3.51, loss_scale=256, train_wall=26, wall=7104
2022-03-18 10:10:30 | INFO | train_inner | epoch 004:   1125 / 7971 loss=0.092, nll_loss=0.002, accuracy=98.2, wps=7683.7, ups=3.72, wpb=2065.4, bsz=36, num_updates=24900, lr=1.04225e-06, gnorm=3.47, loss_scale=512, train_wall=26, wall=7131
2022-03-18 10:10:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:10:58 | INFO | train_inner | epoch 004:   1226 / 7971 loss=0.087, nll_loss=0.002, accuracy=98.1, wps=7491.4, ups=3.65, wpb=2055.1, bsz=35.7, num_updates=25000, lr=1.02508e-06, gnorm=3.566, loss_scale=256, train_wall=26, wall=7158
2022-03-18 10:11:28 | INFO | train_inner | epoch 004:   1326 / 7971 loss=0.103, nll_loss=0.002, accuracy=97.8, wps=6886, ups=3.32, wpb=2071.8, bsz=36.1, num_updates=25100, lr=1.0079e-06, gnorm=3.659, loss_scale=512, train_wall=29, wall=7188
2022-03-18 10:11:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:11:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:11:56 | INFO | train_inner | epoch 004:   1428 / 7971 loss=0.095, nll_loss=0.002, accuracy=97.9, wps=7483.1, ups=3.62, wpb=2067.9, bsz=36.3, num_updates=25200, lr=9.90725e-07, gnorm=3.806, loss_scale=128, train_wall=27, wall=7216
2022-03-18 10:12:22 | INFO | train_inner | epoch 004:   1528 / 7971 loss=0.103, nll_loss=0.002, accuracy=97.8, wps=7916.4, ups=3.73, wpb=2124.8, bsz=36.9, num_updates=25300, lr=9.73549e-07, gnorm=3.793, loss_scale=128, train_wall=26, wall=7243
2022-03-18 10:12:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:12:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 10:12:50 | INFO | train_inner | epoch 004:   1630 / 7971 loss=0.076, nll_loss=0.001, accuracy=98.3, wps=7732.2, ups=3.66, wpb=2110.6, bsz=36.9, num_updates=25400, lr=9.56372e-07, gnorm=3.451, loss_scale=64, train_wall=26, wall=7270
2022-03-18 10:13:16 | INFO | train_inner | epoch 004:   1730 / 7971 loss=0.091, nll_loss=0.002, accuracy=97.9, wps=7890.7, ups=3.73, wpb=2114.9, bsz=37, num_updates=25500, lr=9.39196e-07, gnorm=3.466, loss_scale=128, train_wall=26, wall=7297
2022-03-18 10:13:43 | INFO | train_inner | epoch 004:   1830 / 7971 loss=0.111, nll_loss=0.002, accuracy=97.6, wps=7633.4, ups=3.71, wpb=2057.8, bsz=35.9, num_updates=25600, lr=9.2202e-07, gnorm=4.018, loss_scale=128, train_wall=26, wall=7324
2022-03-18 10:14:11 | INFO | train_inner | epoch 004:   1930 / 7971 loss=0.088, nll_loss=0.002, accuracy=97.8, wps=7734.2, ups=3.69, wpb=2096.8, bsz=36.9, num_updates=25700, lr=9.04844e-07, gnorm=3.503, loss_scale=256, train_wall=26, wall=7351
2022-03-18 10:14:38 | INFO | train_inner | epoch 004:   2030 / 7971 loss=0.096, nll_loss=0.002, accuracy=98, wps=7512.8, ups=3.69, wpb=2033.9, bsz=35.4, num_updates=25800, lr=8.87667e-07, gnorm=3.096, loss_scale=512, train_wall=26, wall=7378
2022-03-18 10:14:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:15:05 | INFO | train_inner | epoch 004:   2131 / 7971 loss=0.089, nll_loss=0.002, accuracy=97.9, wps=7544.1, ups=3.66, wpb=2059.5, bsz=36.1, num_updates=25900, lr=8.70491e-07, gnorm=3.684, loss_scale=256, train_wall=26, wall=7405
2022-03-18 10:15:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:15:33 | INFO | train_inner | epoch 004:   2232 / 7971 loss=0.083, nll_loss=0.001, accuracy=98.1, wps=7411.6, ups=3.62, wpb=2048.4, bsz=35.5, num_updates=26000, lr=8.53315e-07, gnorm=3.335, loss_scale=256, train_wall=27, wall=7433
2022-03-18 10:16:00 | INFO | train_inner | epoch 004:   2332 / 7971 loss=0.092, nll_loss=0.002, accuracy=97.9, wps=7751.1, ups=3.68, wpb=2103.8, bsz=36.3, num_updates=26100, lr=8.36139e-07, gnorm=3.881, loss_scale=512, train_wall=26, wall=7460
2022-03-18 10:16:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:16:27 | INFO | train_inner | epoch 004:   2433 / 7971 loss=0.086, nll_loss=0.002, accuracy=98.1, wps=7372.5, ups=3.63, wpb=2028.9, bsz=35.4, num_updates=26200, lr=8.18963e-07, gnorm=3.392, loss_scale=256, train_wall=27, wall=7488
2022-03-18 10:16:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:16:56 | INFO | train_inner | epoch 004:   2534 / 7971 loss=0.095, nll_loss=0.002, accuracy=98.2, wps=7358.9, ups=3.51, wpb=2096.7, bsz=36.5, num_updates=26300, lr=8.01786e-07, gnorm=3.477, loss_scale=128, train_wall=27, wall=7516
2022-03-18 10:17:24 | INFO | train_inner | epoch 004:   2634 / 7971 loss=0.085, nll_loss=0.001, accuracy=98.2, wps=7394.7, ups=3.51, wpb=2108.4, bsz=36.5, num_updates=26400, lr=7.8461e-07, gnorm=3.158, loss_scale=256, train_wall=27, wall=7545
2022-03-18 10:17:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:17:51 | INFO | train_inner | epoch 004:   2735 / 7971 loss=0.084, nll_loss=0.001, accuracy=98, wps=7906.9, ups=3.67, wpb=2151.8, bsz=37.4, num_updates=26500, lr=7.67434e-07, gnorm=3.089, loss_scale=128, train_wall=26, wall=7572
2022-03-18 10:18:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:18:19 | INFO | train_inner | epoch 004:   2836 / 7971 loss=0.107, nll_loss=0.002, accuracy=97.6, wps=7433.5, ups=3.64, wpb=2041.7, bsz=35.8, num_updates=26600, lr=7.50258e-07, gnorm=4.088, loss_scale=128, train_wall=27, wall=7599
2022-03-18 10:18:46 | INFO | train_inner | epoch 004:   2936 / 7971 loss=0.106, nll_loss=0.002, accuracy=97.6, wps=7748.6, ups=3.71, wpb=2086.6, bsz=36.5, num_updates=26700, lr=7.33081e-07, gnorm=4.263, loss_scale=256, train_wall=26, wall=7626
2022-03-18 10:19:13 | INFO | train_inner | epoch 004:   3036 / 7971 loss=0.097, nll_loss=0.002, accuracy=97.9, wps=7536.3, ups=3.72, wpb=2026.4, bsz=35.4, num_updates=26800, lr=7.15905e-07, gnorm=3.731, loss_scale=256, train_wall=26, wall=7653
2022-03-18 10:19:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:19:40 | INFO | train_inner | epoch 004:   3137 / 7971 loss=0.101, nll_loss=0.002, accuracy=97.9, wps=7580.4, ups=3.66, wpb=2072, bsz=36.4, num_updates=26900, lr=6.98729e-07, gnorm=3.677, loss_scale=256, train_wall=26, wall=7681
2022-03-18 10:20:07 | INFO | train_inner | epoch 004:   3237 / 7971 loss=0.097, nll_loss=0.002, accuracy=97.7, wps=7772, ups=3.69, wpb=2104.7, bsz=36.8, num_updates=27000, lr=6.81553e-07, gnorm=3.837, loss_scale=512, train_wall=26, wall=7708
2022-03-18 10:20:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:20:34 | INFO | train_inner | epoch 004:   3338 / 7971 loss=0.105, nll_loss=0.002, accuracy=97.9, wps=7817.7, ups=3.72, wpb=2100.4, bsz=36.4, num_updates=27100, lr=6.64377e-07, gnorm=3.455, loss_scale=256, train_wall=26, wall=7735
2022-03-18 10:21:01 | INFO | train_inner | epoch 004:   3438 / 7971 loss=0.087, nll_loss=0.002, accuracy=98.2, wps=7759.8, ups=3.75, wpb=2070.7, bsz=36.5, num_updates=27200, lr=6.472e-07, gnorm=3.518, loss_scale=512, train_wall=26, wall=7761
2022-03-18 10:21:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:21:28 | INFO | train_inner | epoch 004:   3539 / 7971 loss=0.092, nll_loss=0.002, accuracy=97.9, wps=7595.5, ups=3.71, wpb=2044.8, bsz=35.5, num_updates=27300, lr=6.30024e-07, gnorm=3.794, loss_scale=256, train_wall=26, wall=7788
2022-03-18 10:21:54 | INFO | train_inner | epoch 004:   3639 / 7971 loss=0.089, nll_loss=0.002, accuracy=97.9, wps=7844.3, ups=3.74, wpb=2099.2, bsz=36.8, num_updates=27400, lr=6.12848e-07, gnorm=3.343, loss_scale=512, train_wall=26, wall=7815
2022-03-18 10:22:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:22:22 | INFO | train_inner | epoch 004:   3740 / 7971 loss=0.082, nll_loss=0.001, accuracy=98.2, wps=7544.9, ups=3.69, wpb=2043.2, bsz=35.5, num_updates=27500, lr=5.95672e-07, gnorm=3.405, loss_scale=256, train_wall=26, wall=7842
2022-03-18 10:22:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:22:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:22:49 | INFO | train_inner | epoch 004:   3842 / 7971 loss=0.083, nll_loss=0.001, accuracy=98.1, wps=7634, ups=3.63, wpb=2100.7, bsz=36.9, num_updates=27600, lr=5.78495e-07, gnorm=3.448, loss_scale=128, train_wall=27, wall=7870
2022-03-18 10:23:16 | INFO | train_inner | epoch 004:   3942 / 7971 loss=0.083, nll_loss=0.001, accuracy=97.9, wps=7770.8, ups=3.74, wpb=2076.3, bsz=36.6, num_updates=27700, lr=5.61319e-07, gnorm=3.932, loss_scale=128, train_wall=26, wall=7896
2022-03-18 10:23:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:23:43 | INFO | train_inner | epoch 004:   4043 / 7971 loss=0.076, nll_loss=0.001, accuracy=98.3, wps=7646.7, ups=3.67, wpb=2081.4, bsz=36.2, num_updates=27800, lr=5.44143e-07, gnorm=3.273, loss_scale=128, train_wall=26, wall=7924
2022-03-18 10:24:10 | INFO | train_inner | epoch 004:   4143 / 7971 loss=0.093, nll_loss=0.002, accuracy=97.9, wps=7522.6, ups=3.68, wpb=2046.1, bsz=35.6, num_updates=27900, lr=5.26967e-07, gnorm=4.2, loss_scale=256, train_wall=26, wall=7951
2022-03-18 10:24:37 | INFO | train_inner | epoch 004:   4243 / 7971 loss=0.092, nll_loss=0.002, accuracy=97.8, wps=7685.6, ups=3.7, wpb=2079.6, bsz=36.6, num_updates=28000, lr=5.0979e-07, gnorm=3.89, loss_scale=256, train_wall=26, wall=7978
2022-03-18 10:24:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:25:04 | INFO | train_inner | epoch 004:   4344 / 7971 loss=0.09, nll_loss=0.002, accuracy=98, wps=7634.9, ups=3.68, wpb=2072.7, bsz=36.5, num_updates=28100, lr=4.92614e-07, gnorm=3.25, loss_scale=256, train_wall=26, wall=8005
2022-03-18 10:25:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:25:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:25:36 | INFO | train_inner | epoch 004:   4446 / 7971 loss=0.083, nll_loss=0.001, accuracy=98.2, wps=6309.8, ups=3.14, wpb=2012.6, bsz=35.4, num_updates=28200, lr=4.75438e-07, gnorm=2.954, loss_scale=128, train_wall=30, wall=8037
2022-03-18 10:26:03 | INFO | train_inner | epoch 004:   4546 / 7971 loss=0.083, nll_loss=0.001, accuracy=98, wps=7729.8, ups=3.71, wpb=2085, bsz=36.1, num_updates=28300, lr=4.58262e-07, gnorm=3.27, loss_scale=256, train_wall=26, wall=8064
2022-03-18 10:26:31 | INFO | train_inner | epoch 004:   4646 / 7971 loss=0.086, nll_loss=0.002, accuracy=98, wps=7441, ups=3.58, wpb=2077.8, bsz=36.4, num_updates=28400, lr=4.41086e-07, gnorm=3.692, loss_scale=256, train_wall=27, wall=8092
2022-03-18 10:26:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:26:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:26:59 | INFO | train_inner | epoch 004:   4748 / 7971 loss=0.088, nll_loss=0.002, accuracy=97.9, wps=7682.2, ups=3.62, wpb=2120.9, bsz=37.3, num_updates=28500, lr=4.23909e-07, gnorm=3.995, loss_scale=128, train_wall=27, wall=8119
2022-03-18 10:27:26 | INFO | train_inner | epoch 004:   4848 / 7971 loss=0.089, nll_loss=0.002, accuracy=98.1, wps=7659.3, ups=3.68, wpb=2079, bsz=36.1, num_updates=28600, lr=4.06733e-07, gnorm=3.165, loss_scale=256, train_wall=26, wall=8147
2022-03-18 10:27:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:27:53 | INFO | train_inner | epoch 004:   4949 / 7971 loss=0.087, nll_loss=0.002, accuracy=97.9, wps=7690.7, ups=3.64, wpb=2112.9, bsz=36.7, num_updates=28700, lr=3.89557e-07, gnorm=3.359, loss_scale=128, train_wall=27, wall=8174
2022-03-18 10:28:21 | INFO | train_inner | epoch 004:   5049 / 7971 loss=0.091, nll_loss=0.002, accuracy=98.1, wps=7810.9, ups=3.69, wpb=2117.3, bsz=37.2, num_updates=28800, lr=3.72381e-07, gnorm=3.643, loss_scale=128, train_wall=26, wall=8201
2022-03-18 10:28:48 | INFO | train_inner | epoch 004:   5149 / 7971 loss=0.108, nll_loss=0.002, accuracy=97.6, wps=7625.1, ups=3.69, wpb=2068.9, bsz=36.1, num_updates=28900, lr=3.55204e-07, gnorm=3.816, loss_scale=256, train_wall=26, wall=8228
2022-03-18 10:28:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:29:15 | INFO | train_inner | epoch 004:   5250 / 7971 loss=0.116, nll_loss=0.002, accuracy=97.6, wps=7591.9, ups=3.68, wpb=2062.7, bsz=36.2, num_updates=29000, lr=3.38028e-07, gnorm=4.052, loss_scale=128, train_wall=26, wall=8255
2022-03-18 10:29:42 | INFO | train_inner | epoch 004:   5350 / 7971 loss=0.083, nll_loss=0.001, accuracy=98.2, wps=7879, ups=3.75, wpb=2103.1, bsz=36.5, num_updates=29100, lr=3.20852e-07, gnorm=3.39, loss_scale=256, train_wall=26, wall=8282
2022-03-18 10:30:08 | INFO | train_inner | epoch 004:   5450 / 7971 loss=0.087, nll_loss=0.002, accuracy=98, wps=7696.1, ups=3.72, wpb=2067.5, bsz=36, num_updates=29200, lr=3.03676e-07, gnorm=3.401, loss_scale=512, train_wall=26, wall=8309
2022-03-18 10:30:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:30:36 | INFO | train_inner | epoch 004:   5551 / 7971 loss=0.103, nll_loss=0.002, accuracy=97.9, wps=7591.3, ups=3.67, wpb=2068.9, bsz=36.2, num_updates=29300, lr=2.86499e-07, gnorm=3.717, loss_scale=256, train_wall=26, wall=8336
2022-03-18 10:30:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:31:03 | INFO | train_inner | epoch 004:   5652 / 7971 loss=0.083, nll_loss=0.001, accuracy=98.2, wps=7674.9, ups=3.66, wpb=2097.1, bsz=36.8, num_updates=29400, lr=2.69323e-07, gnorm=3.675, loss_scale=256, train_wall=26, wall=8364
2022-03-18 10:31:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:31:31 | INFO | train_inner | epoch 004:   5753 / 7971 loss=0.102, nll_loss=0.002, accuracy=97.8, wps=7427.8, ups=3.62, wpb=2050, bsz=36, num_updates=29500, lr=2.52147e-07, gnorm=3.946, loss_scale=128, train_wall=27, wall=8391
2022-03-18 10:31:58 | INFO | train_inner | epoch 004:   5853 / 7971 loss=0.099, nll_loss=0.002, accuracy=97.6, wps=7634.4, ups=3.69, wpb=2067.6, bsz=36.3, num_updates=29600, lr=2.34971e-07, gnorm=3.984, loss_scale=256, train_wall=26, wall=8418
2022-03-18 10:32:25 | INFO | train_inner | epoch 004:   5953 / 7971 loss=0.105, nll_loss=0.002, accuracy=97.6, wps=7600, ups=3.7, wpb=2056, bsz=36, num_updates=29700, lr=2.17795e-07, gnorm=3.701, loss_scale=512, train_wall=26, wall=8445
2022-03-18 10:32:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:32:53 | INFO | train_inner | epoch 004:   6054 / 7971 loss=0.094, nll_loss=0.002, accuracy=98, wps=7430.9, ups=3.5, wpb=2120.7, bsz=36.9, num_updates=29800, lr=2.00618e-07, gnorm=3.473, loss_scale=256, train_wall=28, wall=8474
2022-03-18 10:33:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:33:21 | INFO | train_inner | epoch 004:   6155 / 7971 loss=0.103, nll_loss=0.002, accuracy=97.9, wps=7713.1, ups=3.63, wpb=2122.7, bsz=37.5, num_updates=29900, lr=1.83442e-07, gnorm=3.721, loss_scale=128, train_wall=27, wall=8501
2022-03-18 10:33:48 | INFO | train_inner | epoch 004:   6255 / 7971 loss=0.104, nll_loss=0.002, accuracy=97.8, wps=7680.6, ups=3.7, wpb=2075.5, bsz=36.2, num_updates=30000, lr=1.66266e-07, gnorm=3.752, loss_scale=256, train_wall=26, wall=8528
2022-03-18 10:34:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:34:15 | INFO | train_inner | epoch 004:   6356 / 7971 loss=0.085, nll_loss=0.001, accuracy=98, wps=7524.1, ups=3.68, wpb=2042.6, bsz=35.1, num_updates=30100, lr=1.4909e-07, gnorm=3.493, loss_scale=128, train_wall=26, wall=8556
2022-03-18 10:34:42 | INFO | train_inner | epoch 004:   6456 / 7971 loss=0.086, nll_loss=0.002, accuracy=98.1, wps=7736.2, ups=3.72, wpb=2081.3, bsz=36.6, num_updates=30200, lr=1.31913e-07, gnorm=3.613, loss_scale=128, train_wall=26, wall=8582
2022-03-18 10:35:09 | INFO | train_inner | epoch 004:   6556 / 7971 loss=0.093, nll_loss=0.002, accuracy=97.9, wps=7664.3, ups=3.72, wpb=2059.5, bsz=36.5, num_updates=30300, lr=1.14737e-07, gnorm=3.358, loss_scale=256, train_wall=26, wall=8609
2022-03-18 10:35:36 | INFO | train_inner | epoch 004:   6656 / 7971 loss=0.079, nll_loss=0.001, accuracy=98.3, wps=7586, ups=3.72, wpb=2038.2, bsz=36.2, num_updates=30400, lr=9.7561e-08, gnorm=3.228, loss_scale=512, train_wall=26, wall=8636
2022-03-18 10:35:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:36:03 | INFO | train_inner | epoch 004:   6757 / 7971 loss=0.095, nll_loss=0.002, accuracy=97.8, wps=7658.1, ups=3.69, wpb=2073.4, bsz=36.2, num_updates=30500, lr=8.03847e-08, gnorm=3.544, loss_scale=256, train_wall=26, wall=8663
2022-03-18 10:36:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:36:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:36:30 | INFO | train_inner | epoch 004:   6859 / 7971 loss=0.083, nll_loss=0.001, accuracy=98.2, wps=7636.4, ups=3.65, wpb=2092.2, bsz=36.2, num_updates=30600, lr=6.32085e-08, gnorm=3.209, loss_scale=128, train_wall=26, wall=8691
2022-03-18 10:36:57 | INFO | train_inner | epoch 004:   6959 / 7971 loss=0.077, nll_loss=0.001, accuracy=98.4, wps=7669, ups=3.7, wpb=2074.9, bsz=36.3, num_updates=30700, lr=4.60323e-08, gnorm=3.337, loss_scale=128, train_wall=26, wall=8718
2022-03-18 10:37:24 | INFO | train_inner | epoch 004:   7059 / 7971 loss=0.094, nll_loss=0.002, accuracy=97.8, wps=7771.2, ups=3.72, wpb=2088.9, bsz=36.6, num_updates=30800, lr=2.88561e-08, gnorm=3.448, loss_scale=256, train_wall=26, wall=8745
2022-03-18 10:37:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:37:51 | INFO | train_inner | epoch 004:   7160 / 7971 loss=0.096, nll_loss=0.002, accuracy=98, wps=7588, ups=3.68, wpb=2064.7, bsz=36.2, num_updates=30900, lr=1.16798e-08, gnorm=3.899, loss_scale=256, train_wall=26, wall=8772
2022-03-18 10:38:18 | INFO | train_inner | epoch 004:   7260 / 7971 loss=0.091, nll_loss=0.002, accuracy=97.9, wps=7583, ups=3.71, wpb=2041.5, bsz=35.7, num_updates=31000, lr=0, gnorm=3.428, loss_scale=512, train_wall=26, wall=8799
2022-03-18 10:38:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:38:45 | INFO | train_inner | epoch 004:   7361 / 7971 loss=0.084, nll_loss=0.001, accuracy=98.1, wps=7574.1, ups=3.69, wpb=2051, bsz=35.7, num_updates=31100, lr=0, gnorm=3.184, loss_scale=256, train_wall=26, wall=8826
2022-03-18 10:39:12 | INFO | train_inner | epoch 004:   7461 / 7971 loss=0.086, nll_loss=0.002, accuracy=98.1, wps=7624.5, ups=3.7, wpb=2061.6, bsz=35.9, num_updates=31200, lr=0, gnorm=3.542, loss_scale=512, train_wall=26, wall=8853
2022-03-18 10:39:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:39:40 | INFO | train_inner | epoch 004:   7562 / 7971 loss=0.086, nll_loss=0.002, accuracy=98.1, wps=7554.6, ups=3.68, wpb=2050.9, bsz=36.5, num_updates=31300, lr=0, gnorm=3.424, loss_scale=256, train_wall=26, wall=8880
2022-03-18 10:40:06 | INFO | train_inner | epoch 004:   7662 / 7971 loss=0.096, nll_loss=0.002, accuracy=97.8, wps=7889.6, ups=3.74, wpb=2110.7, bsz=36.8, num_updates=31400, lr=0, gnorm=3.727, loss_scale=512, train_wall=26, wall=8907
2022-03-18 10:40:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:40:33 | INFO | train_inner | epoch 004:   7763 / 7971 loss=0.09, nll_loss=0.002, accuracy=98, wps=7654.2, ups=3.7, wpb=2069.2, bsz=36.4, num_updates=31500, lr=0, gnorm=3.545, loss_scale=256, train_wall=26, wall=8934
2022-03-18 10:41:00 | INFO | train_inner | epoch 004:   7863 / 7971 loss=0.088, nll_loss=0.002, accuracy=98, wps=7789.3, ups=3.73, wpb=2090.6, bsz=36.9, num_updates=31600, lr=0, gnorm=3.584, loss_scale=512, train_wall=26, wall=8961
2022-03-18 10:41:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:41:27 | INFO | train_inner | epoch 004:   7964 / 7971 loss=0.095, nll_loss=0.002, accuracy=97.9, wps=7440.6, ups=3.68, wpb=2021.9, bsz=35, num_updates=31700, lr=0, gnorm=3.537, loss_scale=256, train_wall=26, wall=8988
2022-03-18 10:41:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-18 10:42:01 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.156 | nll_loss 0.003 | accuracy 96.9 | wps 27290.1 | wpb 2056.3 | bsz 36 | num_updates 31707 | best_accuracy 96.9
2022-03-18 10:42:01 | INFO | fairseq_cli.train | begin save checkpoint
2022-03-18 10:42:01 | INFO | fairseq.trainer | Preparing to save checkpoint to /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint4.pt after 31707 updates
2022-03-18 10:42:32 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint4.pt
2022-03-18 10:43:06 | INFO | fairseq.checkpoint_utils | saved checkpoint /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint4.pt (epoch 4 @ 31707 updates, score 96.9) (writing took 64.85916558653116 seconds)
2022-03-18 10:43:06 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-18 10:43:06 | INFO | train | epoch 004 | loss 0.092 | nll_loss 0.002 | accuracy 97.9 | wps 7254.4 | ups 3.5 | wpb 2075 | bsz 36.3 | num_updates 31707 | lr 0 | gnorm 3.557 | loss_scale 256 | train_wall 2091 | wall 9087
2022-03-18 10:43:12 | INFO | fairseq.trainer | begin training epoch 5
2022-03-18 10:43:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:43:37 | INFO | train_inner | epoch 005:     94 / 7971 loss=0.093, nll_loss=0.002, accuracy=97.9, wps=1587, ups=0.77, wpb=2062.2, bsz=36.5, num_updates=31800, lr=0, gnorm=3.721, loss_scale=128, train_wall=27, wall=9118
2022-03-18 10:44:04 | INFO | train_inner | epoch 005:    194 / 7971 loss=0.079, nll_loss=0.001, accuracy=98.4, wps=7452.1, ups=3.68, wpb=2022.5, bsz=35.5, num_updates=31900, lr=0, gnorm=3.211, loss_scale=256, train_wall=26, wall=9145
2022-03-18 10:44:32 | INFO | train_inner | epoch 005:    294 / 7971 loss=0.089, nll_loss=0.002, accuracy=98.1, wps=7799.9, ups=3.69, wpb=2115.6, bsz=37.1, num_updates=32000, lr=0, gnorm=3.238, loss_scale=512, train_wall=26, wall=9172
2022-03-18 10:44:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:44:59 | INFO | train_inner | epoch 005:    395 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.1, wps=7372.7, ups=3.65, wpb=2019.5, bsz=35.2, num_updates=32100, lr=0, gnorm=3.174, loss_scale=256, train_wall=26, wall=9199
2022-03-18 10:45:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:45:26 | INFO | train_inner | epoch 005:    496 / 7971 loss=0.097, nll_loss=0.002, accuracy=98.2, wps=7618.3, ups=3.69, wpb=2066, bsz=36.2, num_updates=32200, lr=0, gnorm=3.724, loss_scale=256, train_wall=26, wall=9227
2022-03-18 10:45:53 | INFO | train_inner | epoch 005:    596 / 7971 loss=0.098, nll_loss=0.002, accuracy=97.8, wps=7721.3, ups=3.66, wpb=2107.1, bsz=37.2, num_updates=32300, lr=0, gnorm=3.615, loss_scale=512, train_wall=26, wall=9254
2022-03-18 10:46:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:46:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:46:21 | INFO | train_inner | epoch 005:    698 / 7971 loss=0.072, nll_loss=0.001, accuracy=98.4, wps=7555.1, ups=3.62, wpb=2088.1, bsz=36.2, num_updates=32400, lr=0, gnorm=2.836, loss_scale=128, train_wall=27, wall=9282
2022-03-18 10:46:48 | INFO | train_inner | epoch 005:    798 / 7971 loss=0.075, nll_loss=0.001, accuracy=98.1, wps=7530.7, ups=3.69, wpb=2042.3, bsz=35.5, num_updates=32500, lr=0, gnorm=3.607, loss_scale=128, train_wall=26, wall=9309
2022-03-18 10:47:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:47:16 | INFO | train_inner | epoch 005:    899 / 7971 loss=0.099, nll_loss=0.002, accuracy=97.9, wps=7594.6, ups=3.65, wpb=2079.9, bsz=36.2, num_updates=32600, lr=0, gnorm=3.734, loss_scale=128, train_wall=26, wall=9336
2022-03-18 10:47:43 | INFO | train_inner | epoch 005:    999 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.2, wps=7697.8, ups=3.69, wpb=2084.1, bsz=36.1, num_updates=32700, lr=0, gnorm=3.091, loss_scale=128, train_wall=26, wall=9363
2022-03-18 10:48:10 | INFO | train_inner | epoch 005:   1099 / 7971 loss=0.077, nll_loss=0.001, accuracy=98.4, wps=7656.2, ups=3.69, wpb=2077.2, bsz=36, num_updates=32800, lr=0, gnorm=3.265, loss_scale=256, train_wall=26, wall=9390
2022-03-18 10:48:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:48:37 | INFO | train_inner | epoch 005:   1200 / 7971 loss=0.086, nll_loss=0.001, accuracy=98.4, wps=7703.6, ups=3.66, wpb=2106, bsz=36.6, num_updates=32900, lr=0, gnorm=3.112, loss_scale=128, train_wall=26, wall=9418
2022-03-18 10:49:04 | INFO | train_inner | epoch 005:   1300 / 7971 loss=0.087, nll_loss=0.002, accuracy=98, wps=7810.8, ups=3.68, wpb=2120.2, bsz=36.9, num_updates=33000, lr=0, gnorm=3.456, loss_scale=256, train_wall=26, wall=9445
2022-03-18 10:49:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:49:32 | INFO | train_inner | epoch 005:   1401 / 7971 loss=0.092, nll_loss=0.002, accuracy=97.9, wps=7448.6, ups=3.66, wpb=2036.8, bsz=35.5, num_updates=33100, lr=0, gnorm=3.55, loss_scale=256, train_wall=26, wall=9472
2022-03-18 10:49:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:49:59 | INFO | train_inner | epoch 005:   1502 / 7971 loss=0.074, nll_loss=0.001, accuracy=98.4, wps=7610.1, ups=3.66, wpb=2081.6, bsz=36, num_updates=33200, lr=0, gnorm=3.447, loss_scale=128, train_wall=26, wall=9499
2022-03-18 10:50:26 | INFO | train_inner | epoch 005:   1602 / 7971 loss=0.091, nll_loss=0.002, accuracy=98.1, wps=7755.6, ups=3.69, wpb=2102.8, bsz=36.7, num_updates=33300, lr=0, gnorm=3.624, loss_scale=128, train_wall=26, wall=9527
2022-03-18 10:50:53 | INFO | train_inner | epoch 005:   1702 / 7971 loss=0.086, nll_loss=0.002, accuracy=98.1, wps=7433.8, ups=3.68, wpb=2019.1, bsz=35.6, num_updates=33400, lr=0, gnorm=3.162, loss_scale=256, train_wall=26, wall=9554
2022-03-18 10:50:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:51:21 | INFO | train_inner | epoch 005:   1803 / 7971 loss=0.075, nll_loss=0.001, accuracy=98.4, wps=7302.6, ups=3.65, wpb=2002, bsz=35.6, num_updates=33500, lr=0, gnorm=3.139, loss_scale=128, train_wall=27, wall=9581
2022-03-18 10:51:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:51:48 | INFO | train_inner | epoch 005:   1904 / 7971 loss=0.086, nll_loss=0.002, accuracy=98.1, wps=7590.7, ups=3.66, wpb=2074.9, bsz=36, num_updates=33600, lr=0, gnorm=3.139, loss_scale=128, train_wall=26, wall=9609
2022-03-18 10:52:15 | INFO | train_inner | epoch 005:   2004 / 7971 loss=0.076, nll_loss=0.001, accuracy=98.3, wps=7653.9, ups=3.68, wpb=2077.9, bsz=36.1, num_updates=33700, lr=0, gnorm=2.904, loss_scale=256, train_wall=26, wall=9636
2022-03-18 10:52:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:52:43 | INFO | train_inner | epoch 005:   2105 / 7971 loss=0.075, nll_loss=0.001, accuracy=98.3, wps=7688.1, ups=3.65, wpb=2106.1, bsz=37.1, num_updates=33800, lr=0, gnorm=3.327, loss_scale=128, train_wall=26, wall=9663
2022-03-18 10:53:10 | INFO | train_inner | epoch 005:   2205 / 7971 loss=0.093, nll_loss=0.002, accuracy=98, wps=7540.4, ups=3.67, wpb=2054.6, bsz=35.9, num_updates=33900, lr=0, gnorm=3.309, loss_scale=256, train_wall=26, wall=9690
2022-03-18 10:53:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:53:37 | INFO | train_inner | epoch 005:   2306 / 7971 loss=0.093, nll_loss=0.002, accuracy=97.9, wps=7646.3, ups=3.71, wpb=2060.8, bsz=36.5, num_updates=34000, lr=0, gnorm=3.368, loss_scale=256, train_wall=26, wall=9717
2022-03-18 10:53:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:54:04 | INFO | train_inner | epoch 005:   2407 / 7971 loss=0.086, nll_loss=0.001, accuracy=98.1, wps=7569.6, ups=3.7, wpb=2044, bsz=35.4, num_updates=34100, lr=0, gnorm=3.266, loss_scale=128, train_wall=26, wall=9744
2022-03-18 10:54:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 10:54:31 | INFO | train_inner | epoch 005:   2508 / 7971 loss=0.082, nll_loss=0.001, accuracy=98.2, wps=7553.4, ups=3.69, wpb=2049.5, bsz=36.3, num_updates=34200, lr=0, gnorm=3.647, loss_scale=64, train_wall=26, wall=9771
2022-03-18 10:54:58 | INFO | train_inner | epoch 005:   2608 / 7971 loss=0.086, nll_loss=0.001, accuracy=98.2, wps=7543.6, ups=3.7, wpb=2039.3, bsz=35.5, num_updates=34300, lr=0, gnorm=3.542, loss_scale=128, train_wall=26, wall=9798
2022-03-18 10:55:25 | INFO | train_inner | epoch 005:   2708 / 7971 loss=0.097, nll_loss=0.002, accuracy=98, wps=7733.5, ups=3.7, wpb=2088.7, bsz=36.6, num_updates=34400, lr=0, gnorm=3.301, loss_scale=256, train_wall=26, wall=9825
2022-03-18 10:55:52 | INFO | train_inner | epoch 005:   2808 / 7971 loss=0.066, nll_loss=0.001, accuracy=98.5, wps=7735, ups=3.7, wpb=2089.7, bsz=36.5, num_updates=34500, lr=0, gnorm=2.974, loss_scale=256, train_wall=26, wall=9852
2022-03-18 10:56:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:56:19 | INFO | train_inner | epoch 005:   2909 / 7971 loss=0.076, nll_loss=0.001, accuracy=98.2, wps=7731.8, ups=3.67, wpb=2104.1, bsz=37.2, num_updates=34600, lr=0, gnorm=2.904, loss_scale=256, train_wall=26, wall=9880
2022-03-18 10:56:46 | INFO | train_inner | epoch 005:   3009 / 7971 loss=0.082, nll_loss=0.001, accuracy=98, wps=7729.5, ups=3.69, wpb=2093, bsz=36.6, num_updates=34700, lr=0, gnorm=3.05, loss_scale=512, train_wall=26, wall=9907
2022-03-18 10:57:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:57:14 | INFO | train_inner | epoch 005:   3110 / 7971 loss=0.079, nll_loss=0.001, accuracy=98.3, wps=7619.5, ups=3.66, wpb=2080.6, bsz=35.9, num_updates=34800, lr=0, gnorm=3.354, loss_scale=256, train_wall=26, wall=9934
2022-03-18 10:57:40 | INFO | train_inner | epoch 005:   3210 / 7971 loss=0.085, nll_loss=0.002, accuracy=98, wps=7698.2, ups=3.72, wpb=2069.1, bsz=36.3, num_updates=34900, lr=0, gnorm=3.322, loss_scale=256, train_wall=26, wall=9961
2022-03-18 10:57:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:58:07 | INFO | train_inner | epoch 005:   3311 / 7971 loss=0.075, nll_loss=0.001, accuracy=98.5, wps=7801.9, ups=3.72, wpb=2099.7, bsz=37.1, num_updates=35000, lr=0, gnorm=2.962, loss_scale=256, train_wall=26, wall=9988
2022-03-18 10:58:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 10:58:34 | INFO | train_inner | epoch 005:   3412 / 7971 loss=0.091, nll_loss=0.002, accuracy=98.1, wps=7633.9, ups=3.71, wpb=2056, bsz=36, num_updates=35100, lr=0, gnorm=3.332, loss_scale=128, train_wall=26, wall=10015
2022-03-18 10:59:01 | INFO | train_inner | epoch 005:   3512 / 7971 loss=0.086, nll_loss=0.002, accuracy=98.2, wps=7774.2, ups=3.74, wpb=2076.3, bsz=36.2, num_updates=35200, lr=0, gnorm=3.483, loss_scale=256, train_wall=26, wall=10042
2022-03-18 10:59:28 | INFO | train_inner | epoch 005:   3612 / 7971 loss=0.08, nll_loss=0.001, accuracy=98.1, wps=7852.1, ups=3.74, wpb=2097.4, bsz=37, num_updates=35300, lr=0, gnorm=3.156, loss_scale=256, train_wall=26, wall=10068
2022-03-18 10:59:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 10:59:55 | INFO | train_inner | epoch 005:   3713 / 7971 loss=0.081, nll_loss=0.001, accuracy=98.1, wps=7694.7, ups=3.72, wpb=2068.8, bsz=36.7, num_updates=35400, lr=0, gnorm=3.288, loss_scale=256, train_wall=26, wall=10095
2022-03-18 11:00:21 | INFO | train_inner | epoch 005:   3813 / 7971 loss=0.074, nll_loss=0.001, accuracy=98.5, wps=7745.9, ups=3.74, wpb=2073.2, bsz=35.9, num_updates=35500, lr=0, gnorm=3.364, loss_scale=512, train_wall=26, wall=10122
2022-03-18 11:00:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:00:49 | INFO | train_inner | epoch 005:   3914 / 7971 loss=0.093, nll_loss=0.002, accuracy=97.7, wps=7616.6, ups=3.66, wpb=2082.6, bsz=36.4, num_updates=35600, lr=0, gnorm=3.746, loss_scale=256, train_wall=26, wall=10149
2022-03-18 11:00:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:01:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:01:16 | INFO | train_inner | epoch 005:   4016 / 7971 loss=0.086, nll_loss=0.002, accuracy=98, wps=7637.4, ups=3.62, wpb=2108, bsz=37.1, num_updates=35700, lr=0, gnorm=3.218, loss_scale=128, train_wall=27, wall=10177
2022-03-18 11:01:43 | INFO | train_inner | epoch 005:   4116 / 7971 loss=0.088, nll_loss=0.002, accuracy=98, wps=7651.5, ups=3.69, wpb=2075, bsz=36.8, num_updates=35800, lr=0, gnorm=3.487, loss_scale=128, train_wall=26, wall=10204
2022-03-18 11:02:11 | INFO | train_inner | epoch 005:   4216 / 7971 loss=0.081, nll_loss=0.001, accuracy=98, wps=7664.1, ups=3.69, wpb=2076.7, bsz=35.9, num_updates=35900, lr=0, gnorm=3.527, loss_scale=256, train_wall=26, wall=10231
2022-03-18 11:02:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:02:38 | INFO | train_inner | epoch 005:   4317 / 7971 loss=0.077, nll_loss=0.001, accuracy=98.3, wps=7483, ups=3.65, wpb=2048, bsz=35.4, num_updates=36000, lr=0, gnorm=3.118, loss_scale=256, train_wall=26, wall=10258
2022-03-18 11:03:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:03:05 | INFO | train_inner | epoch 005:   4418 / 7971 loss=0.076, nll_loss=0.001, accuracy=98.2, wps=7468.9, ups=3.66, wpb=2042, bsz=36, num_updates=36100, lr=0, gnorm=3.324, loss_scale=256, train_wall=26, wall=10286
2022-03-18 11:03:32 | INFO | train_inner | epoch 005:   4518 / 7971 loss=0.088, nll_loss=0.002, accuracy=98.1, wps=7636.1, ups=3.69, wpb=2070.3, bsz=36.1, num_updates=36200, lr=0, gnorm=3.4, loss_scale=256, train_wall=26, wall=10313
2022-03-18 11:03:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:04:00 | INFO | train_inner | epoch 005:   4619 / 7971 loss=0.079, nll_loss=0.001, accuracy=98.4, wps=7507, ups=3.66, wpb=2052.2, bsz=35.7, num_updates=36300, lr=0, gnorm=3.238, loss_scale=256, train_wall=26, wall=10340
2022-03-18 11:04:27 | INFO | train_inner | epoch 005:   4719 / 7971 loss=0.094, nll_loss=0.002, accuracy=98, wps=7682.1, ups=3.69, wpb=2083.7, bsz=36.4, num_updates=36400, lr=0, gnorm=3.434, loss_scale=512, train_wall=26, wall=10367
2022-03-18 11:04:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:04:54 | INFO | train_inner | epoch 005:   4820 / 7971 loss=0.073, nll_loss=0.001, accuracy=98.2, wps=7334.6, ups=3.65, wpb=2008, bsz=35.1, num_updates=36500, lr=0, gnorm=3.139, loss_scale=256, train_wall=26, wall=10395
2022-03-18 11:05:21 | INFO | train_inner | epoch 005:   4920 / 7971 loss=0.077, nll_loss=0.001, accuracy=98.3, wps=7761.7, ups=3.69, wpb=2105.8, bsz=36.8, num_updates=36600, lr=0, gnorm=3.466, loss_scale=512, train_wall=26, wall=10422
2022-03-18 11:05:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:05:49 | INFO | train_inner | epoch 005:   5021 / 7971 loss=0.098, nll_loss=0.002, accuracy=97.8, wps=7590.7, ups=3.66, wpb=2076, bsz=36.1, num_updates=36700, lr=0, gnorm=3.699, loss_scale=256, train_wall=26, wall=10449
2022-03-18 11:06:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:06:16 | INFO | train_inner | epoch 005:   5122 / 7971 loss=0.079, nll_loss=0.001, accuracy=98.5, wps=7628.5, ups=3.66, wpb=2083.7, bsz=36.6, num_updates=36800, lr=0, gnorm=2.923, loss_scale=256, train_wall=26, wall=10477
2022-03-18 11:06:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:06:43 | INFO | train_inner | epoch 005:   5223 / 7971 loss=0.099, nll_loss=0.002, accuracy=97.8, wps=7495.3, ups=3.66, wpb=2050.1, bsz=36.2, num_updates=36900, lr=0, gnorm=3.931, loss_scale=256, train_wall=26, wall=10504
2022-03-18 11:07:11 | INFO | train_inner | epoch 005:   5323 / 7971 loss=0.068, nll_loss=0.001, accuracy=98.6, wps=7670.3, ups=3.69, wpb=2081.2, bsz=35.9, num_updates=37000, lr=0, gnorm=3.454, loss_scale=256, train_wall=26, wall=10531
2022-03-18 11:07:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:07:38 | INFO | train_inner | epoch 005:   5424 / 7971 loss=0.088, nll_loss=0.002, accuracy=98.1, wps=7812.8, ups=3.66, wpb=2135.6, bsz=37.5, num_updates=37100, lr=0, gnorm=3.175, loss_scale=256, train_wall=26, wall=10558
2022-03-18 11:08:05 | INFO | train_inner | epoch 005:   5524 / 7971 loss=0.096, nll_loss=0.002, accuracy=98, wps=7615.1, ups=3.67, wpb=2074.4, bsz=35.9, num_updates=37200, lr=0, gnorm=3.775, loss_scale=512, train_wall=26, wall=10586
2022-03-18 11:08:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:08:32 | INFO | train_inner | epoch 005:   5625 / 7971 loss=0.085, nll_loss=0.001, accuracy=98.2, wps=7594.1, ups=3.66, wpb=2075.5, bsz=36.3, num_updates=37300, lr=0, gnorm=3.606, loss_scale=256, train_wall=26, wall=10613
2022-03-18 11:08:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:09:00 | INFO | train_inner | epoch 005:   5726 / 7971 loss=0.067, nll_loss=0.001, accuracy=98.3, wps=7669.9, ups=3.66, wpb=2096.1, bsz=36.2, num_updates=37400, lr=0, gnorm=2.913, loss_scale=128, train_wall=26, wall=10640
2022-03-18 11:09:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:09:27 | INFO | train_inner | epoch 005:   5827 / 7971 loss=0.079, nll_loss=0.001, accuracy=98.2, wps=7599.6, ups=3.66, wpb=2077.8, bsz=36.1, num_updates=37500, lr=0, gnorm=3.002, loss_scale=128, train_wall=26, wall=10668
2022-03-18 11:09:54 | INFO | train_inner | epoch 005:   5927 / 7971 loss=0.077, nll_loss=0.001, accuracy=98.3, wps=7628.7, ups=3.7, wpb=2064.3, bsz=36.2, num_updates=37600, lr=0, gnorm=3.112, loss_scale=128, train_wall=26, wall=10695
2022-03-18 11:10:21 | INFO | train_inner | epoch 005:   6027 / 7971 loss=0.08, nll_loss=0.001, accuracy=98.2, wps=7835.3, ups=3.69, wpb=2123.3, bsz=37.1, num_updates=37700, lr=0, gnorm=3.005, loss_scale=256, train_wall=26, wall=10722
2022-03-18 11:10:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:10:49 | INFO | train_inner | epoch 005:   6128 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.3, wps=7517.3, ups=3.66, wpb=2053.4, bsz=36.2, num_updates=37800, lr=0, gnorm=3.185, loss_scale=256, train_wall=26, wall=10749
2022-03-18 11:11:16 | INFO | train_inner | epoch 005:   6228 / 7971 loss=0.076, nll_loss=0.001, accuracy=98.2, wps=7633, ups=3.69, wpb=2070.4, bsz=36.2, num_updates=37900, lr=0, gnorm=3.115, loss_scale=256, train_wall=26, wall=10776
2022-03-18 11:11:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:11:43 | INFO | train_inner | epoch 005:   6329 / 7971 loss=0.089, nll_loss=0.002, accuracy=98.2, wps=7537.2, ups=3.66, wpb=2058.4, bsz=36, num_updates=38000, lr=0, gnorm=3.458, loss_scale=256, train_wall=26, wall=10804
2022-03-18 11:12:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:12:10 | INFO | train_inner | epoch 005:   6430 / 7971 loss=0.091, nll_loss=0.002, accuracy=98, wps=7441.1, ups=3.66, wpb=2034.3, bsz=35.9, num_updates=38100, lr=0, gnorm=3.375, loss_scale=256, train_wall=26, wall=10831
2022-03-18 11:12:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:12:38 | INFO | train_inner | epoch 005:   6531 / 7971 loss=0.081, nll_loss=0.001, accuracy=98.4, wps=7655.8, ups=3.66, wpb=2091.5, bsz=37, num_updates=38200, lr=0, gnorm=3.143, loss_scale=128, train_wall=26, wall=10858
2022-03-18 11:13:05 | INFO | train_inner | epoch 005:   6631 / 7971 loss=0.105, nll_loss=0.002, accuracy=97.7, wps=7690.9, ups=3.69, wpb=2084.4, bsz=36.5, num_updates=38300, lr=0, gnorm=3.855, loss_scale=256, train_wall=26, wall=10885
2022-03-18 11:13:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:13:32 | INFO | train_inner | epoch 005:   6732 / 7971 loss=0.087, nll_loss=0.002, accuracy=98.1, wps=7659.3, ups=3.66, wpb=2093.2, bsz=36.5, num_updates=38400, lr=0, gnorm=3.626, loss_scale=128, train_wall=26, wall=10913
2022-03-18 11:13:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:13:59 | INFO | train_inner | epoch 005:   6833 / 7971 loss=0.076, nll_loss=0.001, accuracy=98.7, wps=7535.6, ups=3.66, wpb=2056.7, bsz=36.4, num_updates=38500, lr=0, gnorm=2.881, loss_scale=128, train_wall=26, wall=10940
2022-03-18 11:14:27 | INFO | train_inner | epoch 005:   6933 / 7971 loss=0.085, nll_loss=0.001, accuracy=98.3, wps=7541.7, ups=3.69, wpb=2044, bsz=35.5, num_updates=38600, lr=0, gnorm=3.688, loss_scale=128, train_wall=26, wall=10967
2022-03-18 11:14:54 | INFO | train_inner | epoch 005:   7033 / 7971 loss=0.076, nll_loss=0.001, accuracy=98.1, wps=7733.7, ups=3.69, wpb=2093.6, bsz=36.6, num_updates=38700, lr=0, gnorm=3.142, loss_scale=256, train_wall=26, wall=10994
2022-03-18 11:15:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:15:21 | INFO | train_inner | epoch 005:   7134 / 7971 loss=0.085, nll_loss=0.001, accuracy=98.2, wps=7681.9, ups=3.66, wpb=2097.5, bsz=36.1, num_updates=38800, lr=0, gnorm=3.524, loss_scale=256, train_wall=26, wall=11021
2022-03-18 11:15:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:15:48 | INFO | train_inner | epoch 005:   7235 / 7971 loss=0.095, nll_loss=0.002, accuracy=97.8, wps=7491.5, ups=3.64, wpb=2055.5, bsz=35.5, num_updates=38900, lr=0, gnorm=4.035, loss_scale=128, train_wall=27, wall=11049
2022-03-18 11:16:15 | INFO | train_inner | epoch 005:   7335 / 7971 loss=0.086, nll_loss=0.001, accuracy=98.1, wps=7688, ups=3.7, wpb=2078.4, bsz=36.2, num_updates=39000, lr=0, gnorm=3.262, loss_scale=256, train_wall=26, wall=11076
2022-03-18 11:16:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:16:42 | INFO | train_inner | epoch 005:   7436 / 7971 loss=0.066, nll_loss=0.001, accuracy=98.5, wps=7702.6, ups=3.7, wpb=2082.2, bsz=36.4, num_updates=39100, lr=0, gnorm=2.79, loss_scale=128, train_wall=26, wall=11103
2022-03-18 11:17:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:17:10 | INFO | train_inner | epoch 005:   7537 / 7971 loss=0.071, nll_loss=0.001, accuracy=98.4, wps=7615.4, ups=3.67, wpb=2077.3, bsz=36.4, num_updates=39200, lr=0, gnorm=2.955, loss_scale=128, train_wall=26, wall=11130
2022-03-18 11:17:37 | INFO | train_inner | epoch 005:   7637 / 7971 loss=0.074, nll_loss=0.001, accuracy=98.4, wps=7861.4, ups=3.69, wpb=2131, bsz=37.4, num_updates=39300, lr=0, gnorm=3.262, loss_scale=128, train_wall=26, wall=11157
2022-03-18 11:18:04 | INFO | train_inner | epoch 005:   7737 / 7971 loss=0.1, nll_loss=0.002, accuracy=97.8, wps=7851.2, ups=3.7, wpb=2120.2, bsz=37.2, num_updates=39400, lr=0, gnorm=3.789, loss_scale=256, train_wall=26, wall=11184
2022-03-18 11:18:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:18:31 | INFO | train_inner | epoch 005:   7838 / 7971 loss=0.114, nll_loss=0.002, accuracy=97.5, wps=7575.8, ups=3.69, wpb=2052.5, bsz=36, num_updates=39500, lr=0, gnorm=4.285, loss_scale=128, train_wall=26, wall=11211
2022-03-18 11:18:58 | INFO | train_inner | epoch 005:   7938 / 7971 loss=0.077, nll_loss=0.001, accuracy=98.2, wps=7826.5, ups=3.73, wpb=2098.6, bsz=36.7, num_updates=39600, lr=0, gnorm=3.363, loss_scale=256, train_wall=26, wall=11238
2022-03-18 11:19:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-18 11:19:36 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.156 | nll_loss 0.003 | accuracy 96.9 | wps 29547.8 | wpb 2056.3 | bsz 36 | num_updates 39633 | best_accuracy 96.9
2022-03-18 11:19:36 | INFO | fairseq_cli.train | begin save checkpoint
2022-03-18 11:19:36 | INFO | fairseq.trainer | Preparing to save checkpoint to /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint5.pt after 39633 updates
2022-03-18 11:20:04 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint5.pt
2022-03-18 11:20:43 | INFO | fairseq.checkpoint_utils | saved checkpoint /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint5.pt (epoch 5 @ 39633 updates, score 96.9) (writing took 66.58390177227557 seconds)
2022-03-18 11:20:43 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-18 11:20:43 | INFO | train | epoch 005 | loss 0.084 | nll_loss 0.001 | accuracy 98.2 | wps 7288.5 | ups 3.51 | wpb 2074.8 | bsz 36.3 | num_updates 39633 | lr 0 | gnorm 3.338 | loss_scale 256 | train_wall 2082 | wall 11343
2022-03-18 11:20:48 | INFO | fairseq.trainer | begin training epoch 6
2022-03-18 11:21:06 | INFO | train_inner | epoch 006:     67 / 7971 loss=0.071, nll_loss=0.001, accuracy=98.4, wps=1646.7, ups=0.78, wpb=2114.4, bsz=36.8, num_updates=39700, lr=0, gnorm=3.222, loss_scale=512, train_wall=26, wall=11367
2022-03-18 11:21:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:21:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:21:33 | INFO | train_inner | epoch 006:    169 / 7971 loss=0.074, nll_loss=0.001, accuracy=98.3, wps=7628.4, ups=3.67, wpb=2078.8, bsz=36.2, num_updates=39800, lr=0, gnorm=3.005, loss_scale=128, train_wall=26, wall=11394
2022-03-18 11:22:00 | INFO | train_inner | epoch 006:    269 / 7971 loss=0.085, nll_loss=0.001, accuracy=98.2, wps=7747.8, ups=3.72, wpb=2080.3, bsz=35.9, num_updates=39900, lr=0, gnorm=3.163, loss_scale=128, train_wall=26, wall=11421
2022-03-18 11:22:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:22:28 | INFO | train_inner | epoch 006:    370 / 7971 loss=0.079, nll_loss=0.001, accuracy=98.1, wps=7576.1, ups=3.67, wpb=2066.4, bsz=36.3, num_updates=40000, lr=0, gnorm=3.338, loss_scale=128, train_wall=26, wall=11448
2022-03-18 11:22:55 | INFO | train_inner | epoch 006:    470 / 7971 loss=0.082, nll_loss=0.001, accuracy=98.2, wps=7589.4, ups=3.7, wpb=2052.4, bsz=35.9, num_updates=40100, lr=0, gnorm=3.513, loss_scale=128, train_wall=26, wall=11475
2022-03-18 11:23:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:23:22 | INFO | train_inner | epoch 006:    571 / 7971 loss=0.075, nll_loss=0.001, accuracy=98.5, wps=7663.4, ups=3.69, wpb=2077.6, bsz=36, num_updates=40200, lr=0, gnorm=2.987, loss_scale=128, train_wall=26, wall=11502
2022-03-18 11:23:49 | INFO | train_inner | epoch 006:    671 / 7971 loss=0.084, nll_loss=0.001, accuracy=98.2, wps=7529.1, ups=3.71, wpb=2029.5, bsz=35.3, num_updates=40300, lr=0, gnorm=3.229, loss_scale=256, train_wall=26, wall=11529
2022-03-18 11:24:16 | INFO | train_inner | epoch 006:    771 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.1, wps=7882.2, ups=3.7, wpb=2128.8, bsz=37.4, num_updates=40400, lr=0, gnorm=2.996, loss_scale=256, train_wall=26, wall=11556
2022-03-18 11:24:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:24:43 | INFO | train_inner | epoch 006:    872 / 7971 loss=0.088, nll_loss=0.002, accuracy=98.2, wps=7714.6, ups=3.68, wpb=2094.1, bsz=36.9, num_updates=40500, lr=0, gnorm=3.509, loss_scale=256, train_wall=26, wall=11583
2022-03-18 11:25:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:25:10 | INFO | train_inner | epoch 006:    973 / 7971 loss=0.076, nll_loss=0.001, accuracy=98.4, wps=7721.5, ups=3.71, wpb=2079.2, bsz=36.5, num_updates=40600, lr=0, gnorm=3.127, loss_scale=256, train_wall=26, wall=11610
2022-03-18 11:25:37 | INFO | train_inner | epoch 006:   1073 / 7971 loss=0.096, nll_loss=0.002, accuracy=97.9, wps=7757.5, ups=3.74, wpb=2073.8, bsz=36.2, num_updates=40700, lr=0, gnorm=3.863, loss_scale=256, train_wall=26, wall=11637
2022-03-18 11:26:03 | INFO | train_inner | epoch 006:   1173 / 7971 loss=0.071, nll_loss=0.001, accuracy=98.5, wps=7818.7, ups=3.71, wpb=2109, bsz=36.7, num_updates=40800, lr=0, gnorm=3.035, loss_scale=512, train_wall=26, wall=11664
2022-03-18 11:26:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:26:31 | INFO | train_inner | epoch 006:   1274 / 7971 loss=0.089, nll_loss=0.002, accuracy=98.2, wps=7761.6, ups=3.7, wpb=2098.6, bsz=36.5, num_updates=40900, lr=0, gnorm=3.017, loss_scale=256, train_wall=26, wall=11691
2022-03-18 11:26:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:26:58 | INFO | train_inner | epoch 006:   1375 / 7971 loss=0.09, nll_loss=0.002, accuracy=98.1, wps=7679.7, ups=3.7, wpb=2075.2, bsz=36.1, num_updates=41000, lr=0, gnorm=3.334, loss_scale=256, train_wall=26, wall=11718
2022-03-18 11:27:24 | INFO | train_inner | epoch 006:   1475 / 7971 loss=0.08, nll_loss=0.001, accuracy=98.2, wps=7905.9, ups=3.75, wpb=2109.6, bsz=37.3, num_updates=41100, lr=0, gnorm=3.397, loss_scale=256, train_wall=26, wall=11745
2022-03-18 11:27:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:27:51 | INFO | train_inner | epoch 006:   1576 / 7971 loss=0.067, nll_loss=0.001, accuracy=98.6, wps=7591, ups=3.69, wpb=2055.3, bsz=36.3, num_updates=41200, lr=0, gnorm=2.951, loss_scale=256, train_wall=26, wall=11772
2022-03-18 11:28:18 | INFO | train_inner | epoch 006:   1676 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.5, wps=7525.8, ups=3.68, wpb=2044, bsz=35.8, num_updates=41300, lr=0, gnorm=2.767, loss_scale=512, train_wall=26, wall=11799
2022-03-18 11:28:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-03-18 11:28:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:28:46 | INFO | train_inner | epoch 006:   1778 / 7971 loss=0.092, nll_loss=0.002, accuracy=98.2, wps=7589.1, ups=3.66, wpb=2074.4, bsz=36.4, num_updates=41400, lr=0, gnorm=3.34, loss_scale=256, train_wall=26, wall=11826
2022-03-18 11:29:13 | INFO | train_inner | epoch 006:   1878 / 7971 loss=0.083, nll_loss=0.001, accuracy=98.1, wps=7599, ups=3.72, wpb=2044.4, bsz=35.6, num_updates=41500, lr=0, gnorm=3.282, loss_scale=256, train_wall=26, wall=11853
2022-03-18 11:29:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:29:40 | INFO | train_inner | epoch 006:   1979 / 7971 loss=0.084, nll_loss=0.001, accuracy=98.1, wps=7642, ups=3.68, wpb=2074.1, bsz=36.6, num_updates=41600, lr=0, gnorm=3.082, loss_scale=256, train_wall=26, wall=11880
2022-03-18 11:29:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:30:07 | INFO | train_inner | epoch 006:   2080 / 7971 loss=0.077, nll_loss=0.001, accuracy=98.5, wps=7682, ups=3.67, wpb=2095.4, bsz=36.8, num_updates=41700, lr=0, gnorm=3.407, loss_scale=128, train_wall=26, wall=11908
2022-03-18 11:30:34 | INFO | train_inner | epoch 006:   2180 / 7971 loss=0.085, nll_loss=0.001, accuracy=98.1, wps=7759.7, ups=3.7, wpb=2097.4, bsz=36.8, num_updates=41800, lr=0, gnorm=3.09, loss_scale=256, train_wall=26, wall=11935
2022-03-18 11:30:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:31:02 | INFO | train_inner | epoch 006:   2281 / 7971 loss=0.085, nll_loss=0.001, accuracy=97.9, wps=7556.4, ups=3.64, wpb=2073.5, bsz=36.4, num_updates=41900, lr=0, gnorm=3.733, loss_scale=256, train_wall=27, wall=11962
2022-03-18 11:31:29 | INFO | train_inner | epoch 006:   2381 / 7971 loss=0.089, nll_loss=0.002, accuracy=97.8, wps=7692.3, ups=3.72, wpb=2069.7, bsz=36.4, num_updates=42000, lr=0, gnorm=3.551, loss_scale=256, train_wall=26, wall=11989
2022-03-18 11:31:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:31:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:31:56 | INFO | train_inner | epoch 006:   2483 / 7971 loss=0.097, nll_loss=0.002, accuracy=97.7, wps=7559.2, ups=3.66, wpb=2064.5, bsz=36.1, num_updates=42100, lr=0, gnorm=3.695, loss_scale=128, train_wall=26, wall=12016
2022-03-18 11:32:23 | INFO | train_inner | epoch 006:   2583 / 7971 loss=0.081, nll_loss=0.001, accuracy=98.2, wps=7763.9, ups=3.7, wpb=2095.9, bsz=36.5, num_updates=42200, lr=0, gnorm=3.612, loss_scale=256, train_wall=26, wall=12043
2022-03-18 11:32:50 | INFO | train_inner | epoch 006:   2683 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.3, wps=7783.3, ups=3.71, wpb=2096.7, bsz=36.4, num_updates=42300, lr=0, gnorm=3.154, loss_scale=256, train_wall=26, wall=12070
2022-03-18 11:32:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:33:17 | INFO | train_inner | epoch 006:   2784 / 7971 loss=0.079, nll_loss=0.001, accuracy=98, wps=7556.7, ups=3.69, wpb=2047.9, bsz=35.8, num_updates=42400, lr=0, gnorm=3.658, loss_scale=128, train_wall=26, wall=12097
2022-03-18 11:33:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:33:44 | INFO | train_inner | epoch 006:   2885 / 7971 loss=0.083, nll_loss=0.001, accuracy=98.3, wps=7774.6, ups=3.72, wpb=2088.3, bsz=36.1, num_updates=42500, lr=0, gnorm=3.195, loss_scale=128, train_wall=26, wall=12124
2022-03-18 11:34:11 | INFO | train_inner | epoch 006:   2985 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.5, wps=7630.2, ups=3.72, wpb=2052.9, bsz=35.6, num_updates=42600, lr=0, gnorm=2.969, loss_scale=256, train_wall=26, wall=12151
2022-03-18 11:34:38 | INFO | train_inner | epoch 006:   3085 / 7971 loss=0.073, nll_loss=0.001, accuracy=98.2, wps=7638.9, ups=3.7, wpb=2063, bsz=35.9, num_updates=42700, lr=0, gnorm=3.189, loss_scale=512, train_wall=26, wall=12178
2022-03-18 11:34:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:35:05 | INFO | train_inner | epoch 006:   3186 / 7971 loss=0.089, nll_loss=0.002, accuracy=98.2, wps=7363.8, ups=3.67, wpb=2008.3, bsz=35.1, num_updates=42800, lr=0, gnorm=3.592, loss_scale=256, train_wall=26, wall=12206
2022-03-18 11:35:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:35:32 | INFO | train_inner | epoch 006:   3287 / 7971 loss=0.089, nll_loss=0.002, accuracy=98.2, wps=7749.1, ups=3.68, wpb=2105.3, bsz=37.1, num_updates=42900, lr=0, gnorm=3.422, loss_scale=256, train_wall=26, wall=12233
2022-03-18 11:35:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:35:59 | INFO | train_inner | epoch 006:   3388 / 7971 loss=0.112, nll_loss=0.002, accuracy=97.4, wps=7658.8, ups=3.71, wpb=2062.2, bsz=36.2, num_updates=43000, lr=0, gnorm=4.107, loss_scale=256, train_wall=26, wall=12260
2022-03-18 11:36:26 | INFO | train_inner | epoch 006:   3488 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.1, wps=7779.7, ups=3.74, wpb=2079, bsz=36.6, num_updates=43100, lr=0, gnorm=3.258, loss_scale=256, train_wall=26, wall=12286
2022-03-18 11:36:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:36:53 | INFO | train_inner | epoch 006:   3589 / 7971 loss=0.086, nll_loss=0.002, accuracy=97.9, wps=7675.5, ups=3.71, wpb=2069.7, bsz=36.4, num_updates=43200, lr=0, gnorm=3.823, loss_scale=128, train_wall=26, wall=12313
2022-03-18 11:37:19 | INFO | train_inner | epoch 006:   3689 / 7971 loss=0.088, nll_loss=0.002, accuracy=98, wps=7715.2, ups=3.74, wpb=2062, bsz=36.6, num_updates=43300, lr=0, gnorm=3.884, loss_scale=256, train_wall=26, wall=12340
2022-03-18 11:37:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:37:47 | INFO | train_inner | epoch 006:   3790 / 7971 loss=0.093, nll_loss=0.002, accuracy=98, wps=7566.1, ups=3.69, wpb=2049.5, bsz=35.7, num_updates=43400, lr=0, gnorm=3.425, loss_scale=128, train_wall=26, wall=12367
2022-03-18 11:38:13 | INFO | train_inner | epoch 006:   3890 / 7971 loss=0.083, nll_loss=0.001, accuracy=98.3, wps=7692, ups=3.72, wpb=2065.3, bsz=36.1, num_updates=43500, lr=0, gnorm=3.274, loss_scale=256, train_wall=26, wall=12394
2022-03-18 11:38:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:38:41 | INFO | train_inner | epoch 006:   3991 / 7971 loss=0.076, nll_loss=0.001, accuracy=98.3, wps=7829.1, ups=3.68, wpb=2126.4, bsz=37.1, num_updates=43600, lr=0, gnorm=3.206, loss_scale=256, train_wall=26, wall=12421
2022-03-18 11:39:07 | INFO | train_inner | epoch 006:   4091 / 7971 loss=0.071, nll_loss=0.001, accuracy=98.5, wps=7743.1, ups=3.72, wpb=2081.1, bsz=36.3, num_updates=43700, lr=0, gnorm=2.701, loss_scale=256, train_wall=26, wall=12448
2022-03-18 11:39:34 | INFO | train_inner | epoch 006:   4191 / 7971 loss=0.097, nll_loss=0.002, accuracy=97.6, wps=7723.7, ups=3.73, wpb=2071.2, bsz=35.8, num_updates=43800, lr=0, gnorm=3.641, loss_scale=512, train_wall=26, wall=12475
2022-03-18 11:39:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:40:01 | INFO | train_inner | epoch 006:   4292 / 7971 loss=0.08, nll_loss=0.001, accuracy=98.2, wps=7729.7, ups=3.7, wpb=2091.8, bsz=36.4, num_updates=43900, lr=0, gnorm=3.001, loss_scale=256, train_wall=26, wall=12502
2022-03-18 11:40:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:40:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:40:29 | INFO | train_inner | epoch 006:   4394 / 7971 loss=0.085, nll_loss=0.001, accuracy=98, wps=7543.2, ups=3.64, wpb=2071.2, bsz=36.4, num_updates=44000, lr=0, gnorm=3.454, loss_scale=128, train_wall=26, wall=12529
2022-03-18 11:40:56 | INFO | train_inner | epoch 006:   4494 / 7971 loss=0.077, nll_loss=0.001, accuracy=98.4, wps=7829.9, ups=3.72, wpb=2106.4, bsz=37, num_updates=44100, lr=0, gnorm=3.244, loss_scale=256, train_wall=26, wall=12556
2022-03-18 11:41:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:41:23 | INFO | train_inner | epoch 006:   4595 / 7971 loss=0.093, nll_loss=0.002, accuracy=97.9, wps=7724.6, ups=3.69, wpb=2094, bsz=36.5, num_updates=44200, lr=0, gnorm=3.765, loss_scale=128, train_wall=26, wall=12583
2022-03-18 11:41:50 | INFO | train_inner | epoch 006:   4695 / 7971 loss=0.079, nll_loss=0.001, accuracy=98.6, wps=7618.1, ups=3.7, wpb=2056.8, bsz=35.9, num_updates=44300, lr=0, gnorm=2.807, loss_scale=128, train_wall=26, wall=12610
2022-03-18 11:42:17 | INFO | train_inner | epoch 006:   4795 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.1, wps=7763.5, ups=3.73, wpb=2080.9, bsz=36.2, num_updates=44400, lr=0, gnorm=3.067, loss_scale=256, train_wall=26, wall=12637
2022-03-18 11:42:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:42:44 | INFO | train_inner | epoch 006:   4896 / 7971 loss=0.064, nll_loss=0.001, accuracy=98.5, wps=7752.4, ups=3.71, wpb=2088.8, bsz=36.7, num_updates=44500, lr=0, gnorm=2.729, loss_scale=256, train_wall=26, wall=12664
2022-03-18 11:42:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:43:11 | INFO | train_inner | epoch 006:   4997 / 7971 loss=0.073, nll_loss=0.001, accuracy=98.6, wps=7594.2, ups=3.69, wpb=2057.2, bsz=36.2, num_updates=44600, lr=0, gnorm=2.845, loss_scale=128, train_wall=26, wall=12691
2022-03-18 11:43:38 | INFO | train_inner | epoch 006:   5097 / 7971 loss=0.082, nll_loss=0.001, accuracy=98.3, wps=7556.4, ups=3.7, wpb=2044.1, bsz=35.8, num_updates=44700, lr=0, gnorm=3.18, loss_scale=256, train_wall=26, wall=12718
2022-03-18 11:44:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:44:05 | INFO | train_inner | epoch 006:   5198 / 7971 loss=0.079, nll_loss=0.001, accuracy=98.3, wps=7542.9, ups=3.67, wpb=2056.2, bsz=35.9, num_updates=44800, lr=0, gnorm=3.092, loss_scale=256, train_wall=26, wall=12746
2022-03-18 11:44:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:44:32 | INFO | train_inner | epoch 006:   5299 / 7971 loss=0.071, nll_loss=0.001, accuracy=98.5, wps=7428.1, ups=3.67, wpb=2025.4, bsz=35.4, num_updates=44900, lr=0, gnorm=3.005, loss_scale=128, train_wall=26, wall=12773
2022-03-18 11:44:59 | INFO | train_inner | epoch 006:   5399 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.4, wps=7783.9, ups=3.71, wpb=2100.6, bsz=36.6, num_updates=45000, lr=0, gnorm=2.885, loss_scale=256, train_wall=26, wall=12800
2022-03-18 11:45:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:45:26 | INFO | train_inner | epoch 006:   5500 / 7971 loss=0.097, nll_loss=0.002, accuracy=97.9, wps=7739.4, ups=3.7, wpb=2092.1, bsz=36.2, num_updates=45100, lr=0, gnorm=3.69, loss_scale=128, train_wall=26, wall=12827
2022-03-18 11:45:53 | INFO | train_inner | epoch 006:   5600 / 7971 loss=0.071, nll_loss=0.001, accuracy=98.6, wps=7513.6, ups=3.7, wpb=2029.8, bsz=35.8, num_updates=45200, lr=0, gnorm=2.952, loss_scale=256, train_wall=26, wall=12854
2022-03-18 11:46:20 | INFO | train_inner | epoch 006:   5700 / 7971 loss=0.081, nll_loss=0.001, accuracy=98.2, wps=7818.2, ups=3.74, wpb=2089, bsz=36.6, num_updates=45300, lr=0, gnorm=3.462, loss_scale=256, train_wall=26, wall=12881
2022-03-18 11:46:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:46:47 | INFO | train_inner | epoch 006:   5801 / 7971 loss=0.063, nll_loss=0.001, accuracy=98.8, wps=7600.7, ups=3.71, wpb=2049.5, bsz=36.4, num_updates=45400, lr=0, gnorm=2.824, loss_scale=256, train_wall=26, wall=12908
2022-03-18 11:47:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:47:14 | INFO | train_inner | epoch 006:   5902 / 7971 loss=0.09, nll_loss=0.002, accuracy=97.8, wps=7575.2, ups=3.7, wpb=2047.8, bsz=36, num_updates=45500, lr=0, gnorm=3.514, loss_scale=256, train_wall=26, wall=12935
2022-03-18 11:47:41 | INFO | train_inner | epoch 006:   6002 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.5, wps=7834.5, ups=3.72, wpb=2104.1, bsz=36.8, num_updates=45600, lr=0, gnorm=3.357, loss_scale=512, train_wall=26, wall=12961
2022-03-18 11:47:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:48:08 | INFO | train_inner | epoch 006:   6103 / 7971 loss=0.08, nll_loss=0.001, accuracy=98.2, wps=7765.6, ups=3.69, wpb=2106.8, bsz=36.8, num_updates=45700, lr=0, gnorm=3.166, loss_scale=256, train_wall=26, wall=12989
2022-03-18 11:48:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:48:35 | INFO | train_inner | epoch 006:   6204 / 7971 loss=0.08, nll_loss=0.001, accuracy=98.3, wps=7524.1, ups=3.69, wpb=2036.5, bsz=35.5, num_updates=45800, lr=0, gnorm=3.475, loss_scale=128, train_wall=26, wall=13016
2022-03-18 11:48:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:49:02 | INFO | train_inner | epoch 006:   6305 / 7971 loss=0.073, nll_loss=0.001, accuracy=98.4, wps=7785.9, ups=3.68, wpb=2114.5, bsz=36.8, num_updates=45900, lr=0, gnorm=3.302, loss_scale=128, train_wall=26, wall=13043
2022-03-18 11:49:29 | INFO | train_inner | epoch 006:   6405 / 7971 loss=0.082, nll_loss=0.001, accuracy=98.4, wps=7717.6, ups=3.69, wpb=2089.9, bsz=36.7, num_updates=46000, lr=0, gnorm=3.007, loss_scale=256, train_wall=26, wall=13070
2022-03-18 11:49:57 | INFO | train_inner | epoch 006:   6505 / 7971 loss=0.081, nll_loss=0.001, accuracy=97.9, wps=7687.5, ups=3.68, wpb=2086.2, bsz=36.6, num_updates=46100, lr=0, gnorm=3.229, loss_scale=256, train_wall=26, wall=13097
2022-03-18 11:50:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:50:24 | INFO | train_inner | epoch 006:   6606 / 7971 loss=0.095, nll_loss=0.002, accuracy=98.1, wps=7349.1, ups=3.65, wpb=2013.6, bsz=34.9, num_updates=46200, lr=0, gnorm=3.376, loss_scale=256, train_wall=26, wall=13124
2022-03-18 11:50:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:50:51 | INFO | train_inner | epoch 006:   6707 / 7971 loss=0.081, nll_loss=0.001, accuracy=98.3, wps=7615.7, ups=3.67, wpb=2077.2, bsz=36.4, num_updates=46300, lr=0, gnorm=3.158, loss_scale=128, train_wall=26, wall=13152
2022-03-18 11:51:18 | INFO | train_inner | epoch 006:   6807 / 7971 loss=0.081, nll_loss=0.001, accuracy=98.2, wps=7821.8, ups=3.71, wpb=2105.8, bsz=37.3, num_updates=46400, lr=0, gnorm=3.01, loss_scale=256, train_wall=26, wall=13179
2022-03-18 11:51:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:51:45 | INFO | train_inner | epoch 006:   6908 / 7971 loss=0.095, nll_loss=0.002, accuracy=98.1, wps=7708.5, ups=3.72, wpb=2072.9, bsz=36.5, num_updates=46500, lr=0, gnorm=3.387, loss_scale=128, train_wall=26, wall=13206
2022-03-18 11:52:12 | INFO | train_inner | epoch 006:   7008 / 7971 loss=0.088, nll_loss=0.002, accuracy=98, wps=7753.4, ups=3.73, wpb=2076.9, bsz=36, num_updates=46600, lr=0, gnorm=3.703, loss_scale=256, train_wall=26, wall=13232
2022-03-18 11:52:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:52:39 | INFO | train_inner | epoch 006:   7109 / 7971 loss=0.09, nll_loss=0.002, accuracy=97.9, wps=7612.7, ups=3.71, wpb=2053, bsz=35.7, num_updates=46700, lr=0, gnorm=3.462, loss_scale=128, train_wall=26, wall=13259
2022-03-18 11:53:05 | INFO | train_inner | epoch 006:   7209 / 7971 loss=0.091, nll_loss=0.002, accuracy=97.7, wps=7898.1, ups=3.75, wpb=2108.3, bsz=36.8, num_updates=46800, lr=0, gnorm=4.051, loss_scale=128, train_wall=26, wall=13286
2022-03-18 11:53:32 | INFO | train_inner | epoch 006:   7309 / 7971 loss=0.097, nll_loss=0.002, accuracy=97.8, wps=7750.2, ups=3.72, wpb=2084.8, bsz=37, num_updates=46900, lr=0, gnorm=4.038, loss_scale=256, train_wall=26, wall=13313
2022-03-18 11:53:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:53:59 | INFO | train_inner | epoch 006:   7410 / 7971 loss=0.086, nll_loss=0.001, accuracy=98, wps=7755.7, ups=3.7, wpb=2098.3, bsz=36.2, num_updates=47000, lr=0, gnorm=3.266, loss_scale=256, train_wall=26, wall=13340
2022-03-18 11:54:26 | INFO | train_inner | epoch 006:   7510 / 7971 loss=0.085, nll_loss=0.001, accuracy=98.2, wps=7853.5, ups=3.71, wpb=2114.4, bsz=36.5, num_updates=47100, lr=0, gnorm=3.224, loss_scale=512, train_wall=26, wall=13367
2022-03-18 11:54:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:54:53 | INFO | train_inner | epoch 006:   7611 / 7971 loss=0.081, nll_loss=0.001, accuracy=98.1, wps=7532.9, ups=3.72, wpb=2026, bsz=34.9, num_updates=47200, lr=0, gnorm=3.613, loss_scale=256, train_wall=26, wall=13394
2022-03-18 11:54:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:55:20 | INFO | train_inner | epoch 006:   7712 / 7971 loss=0.085, nll_loss=0.001, accuracy=98.3, wps=7620.8, ups=3.7, wpb=2060.4, bsz=36.1, num_updates=47300, lr=0, gnorm=3.122, loss_scale=128, train_wall=26, wall=13421
2022-03-18 11:55:47 | INFO | train_inner | epoch 006:   7812 / 7971 loss=0.086, nll_loss=0.001, accuracy=98.2, wps=7713.3, ups=3.71, wpb=2078.2, bsz=36.2, num_updates=47400, lr=0, gnorm=3.443, loss_scale=256, train_wall=26, wall=13448
2022-03-18 11:56:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:56:14 | INFO | train_inner | epoch 006:   7913 / 7971 loss=0.082, nll_loss=0.001, accuracy=98.3, wps=7723.2, ups=3.68, wpb=2098.9, bsz=37.2, num_updates=47500, lr=0, gnorm=3.156, loss_scale=256, train_wall=26, wall=13475
2022-03-18 11:56:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-18 11:56:59 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 0.156 | nll_loss 0.003 | accuracy 96.9 | wps 29664.9 | wpb 2056.3 | bsz 36 | num_updates 47558 | best_accuracy 96.9
2022-03-18 11:56:59 | INFO | fairseq_cli.train | begin save checkpoint
2022-03-18 11:56:59 | INFO | fairseq.trainer | Preparing to save checkpoint to /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint6.pt after 47558 updates
2022-03-18 11:57:27 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint6.pt
2022-03-18 11:58:01 | INFO | fairseq.checkpoint_utils | saved checkpoint /workspace/data/users/zanchangtong1/3_XIE/checkpoints/prompt_2/checkpoint6.pt (epoch 6 @ 47558 updates, score 96.9) (writing took 61.8066407199949 seconds)
2022-03-18 11:58:01 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-18 11:58:01 | INFO | train | epoch 006 | loss 0.083 | nll_loss 0.001 | accuracy 98.2 | wps 7345.8 | ups 3.54 | wpb 2074.9 | bsz 36.3 | num_updates 47558 | lr 0 | gnorm 3.302 | loss_scale 256 | train_wall 2069 | wall 13582
2022-03-18 11:58:06 | INFO | fairseq.trainer | begin training epoch 7
2022-03-18 11:58:18 | INFO | train_inner | epoch 007:     42 / 7971 loss=0.07, nll_loss=0.001, accuracy=98.4, wps=1620.3, ups=0.81, wpb=1999.4, bsz=34.6, num_updates=47600, lr=0, gnorm=2.821, loss_scale=256, train_wall=26, wall=13598
2022-03-18 11:58:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 11:58:45 | INFO | train_inner | epoch 007:    143 / 7971 loss=0.098, nll_loss=0.002, accuracy=98, wps=7640.5, ups=3.68, wpb=2076.2, bsz=36.6, num_updates=47700, lr=0, gnorm=3.303, loss_scale=256, train_wall=26, wall=13626
2022-03-18 11:58:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 11:59:12 | INFO | train_inner | epoch 007:    244 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.5, wps=7658.2, ups=3.7, wpb=2071, bsz=36, num_updates=47800, lr=0, gnorm=2.882, loss_scale=128, train_wall=26, wall=13653
2022-03-18 11:59:39 | INFO | train_inner | epoch 007:    344 / 7971 loss=0.088, nll_loss=0.002, accuracy=98, wps=7477.5, ups=3.69, wpb=2026.3, bsz=35.8, num_updates=47900, lr=0, gnorm=3.603, loss_scale=256, train_wall=26, wall=13680
2022-03-18 11:59:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 12:00:06 | INFO | train_inner | epoch 007:    445 / 7971 loss=0.087, nll_loss=0.002, accuracy=98, wps=7654.3, ups=3.71, wpb=2065.7, bsz=36.1, num_updates=48000, lr=0, gnorm=3.286, loss_scale=128, train_wall=26, wall=13707
2022-03-18 12:00:33 | INFO | train_inner | epoch 007:    545 / 7971 loss=0.093, nll_loss=0.002, accuracy=98, wps=7736.3, ups=3.72, wpb=2077.8, bsz=36.3, num_updates=48100, lr=0, gnorm=3.257, loss_scale=256, train_wall=26, wall=13734
2022-03-18 12:00:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 12:01:00 | INFO | train_inner | epoch 007:    646 / 7971 loss=0.088, nll_loss=0.002, accuracy=98.3, wps=7783, ups=3.67, wpb=2123.3, bsz=36.7, num_updates=48200, lr=0, gnorm=3.446, loss_scale=128, train_wall=26, wall=13761
2022-03-18 12:01:27 | INFO | train_inner | epoch 007:    746 / 7971 loss=0.089, nll_loss=0.002, accuracy=98, wps=7840.2, ups=3.73, wpb=2102.3, bsz=37, num_updates=48300, lr=0, gnorm=3.5, loss_scale=256, train_wall=26, wall=13788
2022-03-18 12:01:54 | INFO | train_inner | epoch 007:    846 / 7971 loss=0.077, nll_loss=0.001, accuracy=98.2, wps=7551.6, ups=3.72, wpb=2030.5, bsz=35.9, num_updates=48400, lr=0, gnorm=3.122, loss_scale=256, train_wall=26, wall=13815
2022-03-18 12:02:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:02:21 | INFO | train_inner | epoch 007:    947 / 7971 loss=0.074, nll_loss=0.001, accuracy=98.4, wps=7487.3, ups=3.69, wpb=2027.9, bsz=35.3, num_updates=48500, lr=0, gnorm=3.059, loss_scale=256, train_wall=26, wall=13842
2022-03-18 12:02:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:02:48 | INFO | train_inner | epoch 007:   1048 / 7971 loss=0.074, nll_loss=0.001, accuracy=98.1, wps=7486.1, ups=3.67, wpb=2040.8, bsz=35.5, num_updates=48600, lr=0, gnorm=3.379, loss_scale=256, train_wall=26, wall=13869
2022-03-18 12:02:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 12:03:16 | INFO | train_inner | epoch 007:   1149 / 7971 loss=0.088, nll_loss=0.002, accuracy=98.2, wps=7542.8, ups=3.67, wpb=2056.1, bsz=35.8, num_updates=48700, lr=0, gnorm=3.144, loss_scale=128, train_wall=26, wall=13896
2022-03-18 12:03:43 | INFO | train_inner | epoch 007:   1249 / 7971 loss=0.083, nll_loss=0.001, accuracy=98.3, wps=7708.8, ups=3.7, wpb=2081.9, bsz=37, num_updates=48800, lr=0, gnorm=3.274, loss_scale=256, train_wall=26, wall=13923
2022-03-18 12:04:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:04:10 | INFO | train_inner | epoch 007:   1350 / 7971 loss=0.102, nll_loss=0.002, accuracy=97.9, wps=7616.5, ups=3.67, wpb=2073.6, bsz=36.6, num_updates=48900, lr=0, gnorm=3.844, loss_scale=256, train_wall=26, wall=13950
2022-03-18 12:04:37 | INFO | train_inner | epoch 007:   1450 / 7971 loss=0.076, nll_loss=0.001, accuracy=98.3, wps=7540.4, ups=3.73, wpb=2022.8, bsz=35.6, num_updates=49000, lr=0, gnorm=3.152, loss_scale=256, train_wall=26, wall=13977
2022-03-18 12:04:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:05:04 | INFO | train_inner | epoch 007:   1551 / 7971 loss=0.088, nll_loss=0.002, accuracy=98, wps=7654.3, ups=3.68, wpb=2079.6, bsz=36.1, num_updates=49100, lr=0, gnorm=3.188, loss_scale=256, train_wall=26, wall=14004
2022-03-18 12:05:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:05:31 | INFO | train_inner | epoch 007:   1652 / 7971 loss=0.089, nll_loss=0.002, accuracy=98, wps=7635, ups=3.67, wpb=2078.2, bsz=36.3, num_updates=49200, lr=0, gnorm=3.503, loss_scale=256, train_wall=26, wall=14032
2022-03-18 12:05:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 12:05:58 | INFO | train_inner | epoch 007:   1753 / 7971 loss=0.083, nll_loss=0.001, accuracy=98.3, wps=7584.7, ups=3.69, wpb=2055, bsz=35.7, num_updates=49300, lr=0, gnorm=3.275, loss_scale=128, train_wall=26, wall=14059
2022-03-18 12:06:25 | INFO | train_inner | epoch 007:   1853 / 7971 loss=0.066, nll_loss=0.001, accuracy=98.6, wps=7764.7, ups=3.73, wpb=2082.1, bsz=36.1, num_updates=49400, lr=0, gnorm=2.812, loss_scale=128, train_wall=26, wall=14086
2022-03-18 12:06:52 | INFO | train_inner | epoch 007:   1953 / 7971 loss=0.076, nll_loss=0.001, accuracy=98.3, wps=7682.4, ups=3.71, wpb=2072.6, bsz=36.5, num_updates=49500, lr=0, gnorm=3.173, loss_scale=256, train_wall=26, wall=14113
2022-03-18 12:06:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 12:07:19 | INFO | train_inner | epoch 007:   2054 / 7971 loss=0.096, nll_loss=0.002, accuracy=97.8, wps=7700.6, ups=3.68, wpb=2092.9, bsz=36.6, num_updates=49600, lr=0, gnorm=3.654, loss_scale=128, train_wall=26, wall=14140
2022-03-18 12:07:46 | INFO | train_inner | epoch 007:   2154 / 7971 loss=0.082, nll_loss=0.001, accuracy=98.2, wps=7719.5, ups=3.73, wpb=2067.9, bsz=36.4, num_updates=49700, lr=0, gnorm=3.369, loss_scale=256, train_wall=26, wall=14167
2022-03-18 12:08:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:08:13 | INFO | train_inner | epoch 007:   2255 / 7971 loss=0.085, nll_loss=0.001, accuracy=98.3, wps=7727.6, ups=3.69, wpb=2096.3, bsz=36.6, num_updates=49800, lr=0, gnorm=3.376, loss_scale=256, train_wall=26, wall=14194
2022-03-18 12:08:40 | INFO | train_inner | epoch 007:   2355 / 7971 loss=0.083, nll_loss=0.001, accuracy=98, wps=7644.3, ups=3.69, wpb=2074.2, bsz=36.2, num_updates=49900, lr=0, gnorm=3.355, loss_scale=256, train_wall=26, wall=14221
2022-03-18 12:09:07 | INFO | train_inner | epoch 007:   2455 / 7971 loss=0.076, nll_loss=0.001, accuracy=98.2, wps=7429.8, ups=3.68, wpb=2017.1, bsz=35.4, num_updates=50000, lr=0, gnorm=3.237, loss_scale=512, train_wall=26, wall=14248
2022-03-18 12:09:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:09:35 | INFO | train_inner | epoch 007:   2556 / 7971 loss=0.08, nll_loss=0.001, accuracy=98.4, wps=7521.6, ups=3.68, wpb=2042.7, bsz=35.9, num_updates=50100, lr=0, gnorm=3.163, loss_scale=256, train_wall=26, wall=14275
2022-03-18 12:09:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:10:02 | INFO | train_inner | epoch 007:   2657 / 7971 loss=0.085, nll_loss=0.001, accuracy=98.2, wps=7699.6, ups=3.7, wpb=2080.4, bsz=36.4, num_updates=50200, lr=0, gnorm=3.286, loss_scale=256, train_wall=26, wall=14302
2022-03-18 12:10:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 12:10:29 | INFO | train_inner | epoch 007:   2758 / 7971 loss=0.079, nll_loss=0.001, accuracy=98.5, wps=7741.9, ups=3.68, wpb=2104.3, bsz=36.8, num_updates=50300, lr=0, gnorm=3.401, loss_scale=128, train_wall=26, wall=14329
2022-03-18 12:10:56 | INFO | train_inner | epoch 007:   2858 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.2, wps=7757.4, ups=3.7, wpb=2098.6, bsz=36.2, num_updates=50400, lr=0, gnorm=3.242, loss_scale=256, train_wall=26, wall=14356
2022-03-18 12:11:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 12:11:23 | INFO | train_inner | epoch 007:   2959 / 7971 loss=0.091, nll_loss=0.002, accuracy=98, wps=7540.3, ups=3.67, wpb=2053.7, bsz=36, num_updates=50500, lr=0, gnorm=3.76, loss_scale=128, train_wall=26, wall=14384
2022-03-18 12:11:50 | INFO | train_inner | epoch 007:   3059 / 7971 loss=0.08, nll_loss=0.001, accuracy=98.3, wps=7763.2, ups=3.7, wpb=2098.7, bsz=36.7, num_updates=50600, lr=0, gnorm=3.319, loss_scale=256, train_wall=26, wall=14411
2022-03-18 12:12:17 | INFO | train_inner | epoch 007:   3159 / 7971 loss=0.08, nll_loss=0.001, accuracy=98.3, wps=7729.3, ups=3.69, wpb=2093.3, bsz=36.4, num_updates=50700, lr=0, gnorm=3.099, loss_scale=512, train_wall=26, wall=14438
2022-03-18 12:12:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:12:44 | INFO | train_inner | epoch 007:   3260 / 7971 loss=0.09, nll_loss=0.002, accuracy=98.3, wps=7675.5, ups=3.67, wpb=2093.7, bsz=36.7, num_updates=50800, lr=0, gnorm=3.191, loss_scale=256, train_wall=26, wall=14465
2022-03-18 12:13:12 | INFO | train_inner | epoch 007:   3360 / 7971 loss=0.077, nll_loss=0.001, accuracy=98.3, wps=7614.4, ups=3.7, wpb=2060.3, bsz=35.7, num_updates=50900, lr=0, gnorm=3.434, loss_scale=256, train_wall=26, wall=14492
2022-03-18 12:13:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:13:39 | INFO | train_inner | epoch 007:   3461 / 7971 loss=0.087, nll_loss=0.002, accuracy=98.2, wps=7688.1, ups=3.69, wpb=2085, bsz=36.3, num_updates=51000, lr=0, gnorm=3.046, loss_scale=256, train_wall=26, wall=14519
2022-03-18 12:14:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:14:06 | INFO | train_inner | epoch 007:   3562 / 7971 loss=0.094, nll_loss=0.002, accuracy=97.7, wps=7802.7, ups=3.69, wpb=2113.4, bsz=36.9, num_updates=51100, lr=0, gnorm=3.615, loss_scale=256, train_wall=26, wall=14546
2022-03-18 12:14:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 12:14:33 | INFO | train_inner | epoch 007:   3663 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.6, wps=7579.3, ups=3.68, wpb=2058.4, bsz=35.7, num_updates=51200, lr=0, gnorm=3.061, loss_scale=128, train_wall=26, wall=14573
2022-03-18 12:15:00 | INFO | train_inner | epoch 007:   3763 / 7971 loss=0.091, nll_loss=0.002, accuracy=98, wps=7562.3, ups=3.69, wpb=2048.3, bsz=36.2, num_updates=51300, lr=0, gnorm=3.699, loss_scale=128, train_wall=26, wall=14601
2022-03-18 12:15:27 | INFO | train_inner | epoch 007:   3863 / 7971 loss=0.072, nll_loss=0.001, accuracy=98.5, wps=7743.6, ups=3.7, wpb=2090.7, bsz=36.3, num_updates=51400, lr=0, gnorm=2.895, loss_scale=256, train_wall=26, wall=14628
2022-03-18 12:15:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 12:15:54 | INFO | train_inner | epoch 007:   3964 / 7971 loss=0.077, nll_loss=0.001, accuracy=98.3, wps=7652, ups=3.69, wpb=2075.3, bsz=36.4, num_updates=51500, lr=0, gnorm=3.01, loss_scale=128, train_wall=26, wall=14655
2022-03-18 12:16:21 | INFO | train_inner | epoch 007:   4064 / 7971 loss=0.077, nll_loss=0.001, accuracy=98.3, wps=7498.6, ups=3.66, wpb=2049.2, bsz=35.6, num_updates=51600, lr=0, gnorm=3.103, loss_scale=256, train_wall=26, wall=14682
2022-03-18 12:16:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 12:16:49 | INFO | train_inner | epoch 007:   4165 / 7971 loss=0.077, nll_loss=0.001, accuracy=98.4, wps=7693.7, ups=3.66, wpb=2100.3, bsz=36.6, num_updates=51700, lr=0, gnorm=2.887, loss_scale=128, train_wall=26, wall=14709
2022-03-18 12:17:16 | INFO | train_inner | epoch 007:   4265 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.2, wps=7620.5, ups=3.69, wpb=2066, bsz=36.2, num_updates=51800, lr=0, gnorm=3.417, loss_scale=256, train_wall=26, wall=14736
2022-03-18 12:17:43 | INFO | train_inner | epoch 007:   4365 / 7971 loss=0.091, nll_loss=0.002, accuracy=98, wps=7740.8, ups=3.69, wpb=2098.8, bsz=36.4, num_updates=51900, lr=0, gnorm=3.231, loss_scale=512, train_wall=26, wall=14764
2022-03-18 12:17:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:18:10 | INFO | train_inner | epoch 007:   4466 / 7971 loss=0.082, nll_loss=0.001, accuracy=98.2, wps=7564.5, ups=3.68, wpb=2058.1, bsz=35.7, num_updates=52000, lr=0, gnorm=3.413, loss_scale=256, train_wall=26, wall=14791
2022-03-18 12:18:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:18:37 | INFO | train_inner | epoch 007:   4567 / 7971 loss=0.097, nll_loss=0.002, accuracy=97.6, wps=7728.6, ups=3.72, wpb=2078.2, bsz=36.6, num_updates=52100, lr=0, gnorm=3.952, loss_scale=256, train_wall=26, wall=14818
2022-03-18 12:19:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:19:04 | INFO | train_inner | epoch 007:   4668 / 7971 loss=0.094, nll_loss=0.002, accuracy=97.7, wps=7737.9, ups=3.7, wpb=2093.4, bsz=36.5, num_updates=52200, lr=0, gnorm=3.413, loss_scale=256, train_wall=26, wall=14845
2022-03-18 12:19:31 | INFO | train_inner | epoch 007:   4768 / 7971 loss=0.088, nll_loss=0.002, accuracy=98.3, wps=7654.5, ups=3.72, wpb=2056.8, bsz=35.9, num_updates=52300, lr=0, gnorm=3.276, loss_scale=256, train_wall=26, wall=14872
2022-03-18 12:19:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:19:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 12:19:58 | INFO | train_inner | epoch 007:   4870 / 7971 loss=0.096, nll_loss=0.002, accuracy=97.9, wps=7632.1, ups=3.66, wpb=2084.8, bsz=36, num_updates=52400, lr=0, gnorm=3.44, loss_scale=128, train_wall=26, wall=14899
2022-03-18 12:20:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-03-18 12:20:26 | INFO | train_inner | epoch 007:   4971 / 7971 loss=0.091, nll_loss=0.002, accuracy=98.1, wps=7310.1, ups=3.66, wpb=1998.8, bsz=35, num_updates=52500, lr=0, gnorm=3.576, loss_scale=64, train_wall=26, wall=14926
2022-03-18 12:20:56 | INFO | train_inner | epoch 007:   5071 / 7971 loss=0.069, nll_loss=0.001, accuracy=98.5, wps=6666.8, ups=3.28, wpb=2030, bsz=35.1, num_updates=52600, lr=0, gnorm=2.759, loss_scale=128, train_wall=29, wall=14957
2022-03-18 12:21:24 | INFO | train_inner | epoch 007:   5171 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.4, wps=7510.5, ups=3.65, wpb=2056, bsz=35.8, num_updates=52700, lr=0, gnorm=3.428, loss_scale=256, train_wall=26, wall=14984
2022-03-18 12:21:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 12:21:51 | INFO | train_inner | epoch 007:   5272 / 7971 loss=0.088, nll_loss=0.002, accuracy=97.9, wps=7498.2, ups=3.68, wpb=2040, bsz=35.8, num_updates=52800, lr=0, gnorm=3.352, loss_scale=128, train_wall=26, wall=15011
2022-03-18 12:22:18 | INFO | train_inner | epoch 007:   5372 / 7971 loss=0.085, nll_loss=0.001, accuracy=98.1, wps=7621, ups=3.7, wpb=2058, bsz=35.9, num_updates=52900, lr=0, gnorm=3.384, loss_scale=128, train_wall=26, wall=15038
2022-03-18 12:22:45 | INFO | train_inner | epoch 007:   5472 / 7971 loss=0.103, nll_loss=0.002, accuracy=97.7, wps=7714.8, ups=3.7, wpb=2083.9, bsz=36.9, num_updates=53000, lr=0, gnorm=3.6, loss_scale=256, train_wall=26, wall=15065
2022-03-18 12:22:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 12:23:12 | INFO | train_inner | epoch 007:   5573 / 7971 loss=0.087, nll_loss=0.002, accuracy=98.1, wps=7666.3, ups=3.67, wpb=2089.6, bsz=36.8, num_updates=53100, lr=0, gnorm=3.282, loss_scale=128, train_wall=26, wall=15093
2022-03-18 12:23:39 | INFO | train_inner | epoch 007:   5673 / 7971 loss=0.081, nll_loss=0.001, accuracy=98.3, wps=7740.9, ups=3.71, wpb=2089.2, bsz=36.3, num_updates=53200, lr=0, gnorm=3.301, loss_scale=256, train_wall=26, wall=15120
2022-03-18 12:24:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:24:06 | INFO | train_inner | epoch 007:   5774 / 7971 loss=0.087, nll_loss=0.002, accuracy=98.2, wps=7817.5, ups=3.71, wpb=2107.3, bsz=36.8, num_updates=53300, lr=0, gnorm=3.231, loss_scale=256, train_wall=26, wall=15147
2022-03-18 12:24:33 | INFO | train_inner | epoch 007:   5874 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.4, wps=7667.5, ups=3.73, wpb=2054.2, bsz=36.2, num_updates=53400, lr=0, gnorm=3.196, loss_scale=256, train_wall=26, wall=15173
2022-03-18 12:24:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:25:00 | INFO | train_inner | epoch 007:   5975 / 7971 loss=0.086, nll_loss=0.001, accuracy=97.9, wps=7787.9, ups=3.69, wpb=2108, bsz=36.5, num_updates=53500, lr=0, gnorm=3.55, loss_scale=256, train_wall=26, wall=15200
2022-03-18 12:25:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:25:27 | INFO | train_inner | epoch 007:   6076 / 7971 loss=0.076, nll_loss=0.001, accuracy=98.2, wps=7767.5, ups=3.69, wpb=2103.2, bsz=37.1, num_updates=53600, lr=0, gnorm=3.278, loss_scale=256, train_wall=26, wall=15227
2022-03-18 12:25:54 | INFO | train_inner | epoch 007:   6176 / 7971 loss=0.078, nll_loss=0.001, accuracy=98.3, wps=7819.3, ups=3.73, wpb=2097.6, bsz=36.6, num_updates=53700, lr=0, gnorm=3.261, loss_scale=512, train_wall=26, wall=15254
2022-03-18 12:25:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:26:21 | INFO | train_inner | epoch 007:   6277 / 7971 loss=0.088, nll_loss=0.002, accuracy=98.2, wps=7667.5, ups=3.69, wpb=2078, bsz=36.4, num_updates=53800, lr=0, gnorm=3.205, loss_scale=256, train_wall=26, wall=15281
2022-03-18 12:26:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-03-18 12:26:48 | INFO | train_inner | epoch 007:   6378 / 7971 loss=0.102, nll_loss=0.002, accuracy=97.9, wps=7732.7, ups=3.69, wpb=2098, bsz=37.1, num_updates=53900, lr=0, gnorm=3.575, loss_scale=256, train_wall=26, wall=15309
2022-03-18 12:26:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-03-18 12:27:15 | INFO | train_inner | epoch 007:   6479 / 7971 loss=0.07, nll_loss=0.001, accuracy=98.4, wps=7513.3, ups=3.67, wpb=2049.7, bsz=35.8, num_updates=54000, lr=0, gnorm=3.496, loss_scale=128, train_wall=26, wall=15336
